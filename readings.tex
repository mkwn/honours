%% LyX 2.0.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt,english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm,headheight=3cm,headsep=3cm,footskip=1.5cm}
\setlength{\parskip}{\bigskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\setstretch{1.5}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\AtBeginDocument{
%\addtolength\abovedisplayskip{-10pt}
%\addtolength\belowdisplayskip{-5pt}
%}

\usepackage{letltxmacro}
%\LetLtxMacro{\polishL}{\L{}}

\usepackage[bookmarks]{hyperref}

\renewcommand\theenumi{\arabic{enumi}}
\renewcommand\labelenumi{(\theenumi)}

\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

\usepackage{tocloft}
\setlength\cftparskip{-2pt}
\setlength\cftbeforesecskip{1pt}
\setlength\cftaftertoctitleskip{2pt}

\LetLtxMacro\polishL\L
\DeclareRobustCommand{\L}{\ifmmode{\mathcal{L}}\else\polishL\fi}

\makeatother

\usepackage{babel}
\begin{document}

\title{Readings}


\author{Matthew Kwan}

\maketitle
\global\long\def\floor#1{\left\lfloor #1\right\rfloor }


\global\long\def\ceil#1{\left\lceil #1\right\rceil }


\global\long\def\NN{\mathbb{N}}


\global\long\def\ZZ{\mathbb{Z}}


\global\long\def\RR{\mathbb{R}}


\global\long\def\CC{\mathbb{C}}


\global\long\def\E{\mathbb{E}}


\global\long\def\Pr{\mathbb{P}}


\global\long\def\L{\mathcal{L}}


\DeclareRobustCommand{\L}{\ifmmode{\mathcal{L}}\else\polishL\fi}

\global\long\def\GBin#1#2{\mathbb{G}\left(#1,#2\right)}


\global\long\def\I{\boldsymbol{1}}


\global\long\def\d{\operatorname d}


\global\long\def\Be{\operatorname{Ber}}


\global\long\def\Po{\operatorname{Po}}


\tableofcontents{}


\section{Combinatorial estimates by the switching method \cite{HM10}}


\subsection{Summary}

Consider a finite set $\Omega$ of objects. A switching is a (nondeterministic)
operation that transforms one object into another (or more generally,
a switching is a relation $R\subseteq\Omega\times\Omega$). We can
partition $\Omega$ into subsets $\left\{ C\left(v\right)\right\} _{v\in V}$
and put a directed graph structure (possibly with loops) on the index
set $V$: if an element in $C\left(v\right)$ can switch to an element
in $C\left(w\right)$ then there is an arc between $v$ and $w$.

Hopefully, for each $v$ we can find a good lower bound $a\left(v\right)$
on the number of ways an $\omega\in C\left(v\right)$ can be switched,
and a good upper bound $b\left(v\right)$ on the number of switchings
an $\omega\in C\left(v\right)$ can be produced by. If we imagine
that all switchings are performed at once, $a$ and $b$ give bounds
on the inflow and outflow at each vertex in terms of the size of each
$C\left(v\right)$. (So, $a\left(v\right)$ and $b\left(v\right)$
can more generally be bounds on the \textit{average} number of switchings
per element in $C\left(v\right)$).

This gives some information about the relative sizes of the classes
$C\left(v\right)$. Given a set of vertices $X$, let $N\left(X\right)$
be the total amount of elements in all $C\left(x\right)$, where $x\in X$.
The objective of the paper is to bound $N\left(Y\right)/N\left(X\right)$
for given vertex sets $X$ and $Y$.

In order to obtain bounds on $N\left(Y\right)/N\left(X\right)$, we
write the constraints as a system of linear inequalities. Let $s'\left(v,w\right)$
be the (unknown) amount of switchings going from $C\left(v\right)$
to $C\left(w\right)$. For instance, for any vertex $v$, we have
$\sum s'\left(vw\right)\ge a\left(v\right)N\left(v\right)$. A nonzero
assignment of $s$ and $N$ satisfying the full set of inequalities
is called a \textit{feasible solution}, and a feasible solution which
maximizes $N\left(Y\right)/N\left(X\right)$ is called an \textit{optimal
solution}.

The key result of the paper is to show that optimal solutions always
take one of six standard forms. This is proved by a reduction of the
inequalities to a linear program, and the fact that an optimal solution
of a linear program always occurs at a vertex of the corresponding
convex polytope. The analysis is amenable to a general bound $\alpha$
on the arcs of the digraph instead of the functions $a,b$ (in that
case we have $\alpha\left(v,w\right)=b\left(w\right)/a\left(v\right)$).
Although$\alpha$ doesn't have a clear combinatorial interpretation,
the paper suggests that different $\alpha$ may arise from richer
information bounding the behaviour of the flow of switchings.

For a number of commonly-satisfied assumptions, the paper proves some
alternative bounds for $N\left(Y\right)/N\left(X\right)$ that are
slightly looser but easier to apply. In particular, a common use case
is that we have some statistic on the objects in $\Omega$ and we
believe that objects with a high value of that statistic make up a
negligible fraction of all the objects in $\Omega$. We would then
partition the objects according to our statistic, and design a switching
that tends to decrease the statistic, but by no more than some fixed
amount. We would choose $X$ to be the set of all vertices (partititions)
and $Y$ would be the set of all vertices (partitions) with a statistic
value higher than some $M$.


\subsection{Remarks}
\begin{itemize}
\item I'm not sure why $i_{k-1}=\max\left\{ M-\left(k-1\right)K,N+1\right\} $
is required for cases (a) and (b) of Lemma 3, instead of just $M-\left(k-1\right)K$
as in case (c).
\item In Corollary 1, it isn't spelled out what assumptions $X$ should
satisfy, but it would seem A1 still has to hold.
\item In Corollary 1, the somewhat inscrutable expression $k=\ceil{\frac{M+\min\left\{ 0,K-\rho-1\right\} }{K}}$
can be more easily seen to be $k=\ceil{\frac{M-\floor{\rho}}{K}}$.
\end{itemize}

\section{Asymptotic enumeration of sparse multigraphs with given degrees \cite{GM13} }


\subsection{Summary}

The paper introduces a generalized version of the switching method,
where there are multiple different ``colours'' of switchings that
operate on the same set of objects. By slightly loosening the bound,
this generalized model can be reduced to an instance of the single-switching
model.

The purpose of the generalized method is to make it tractable to generalize
an earlier paper \cite{MW91} which counted the number of graphs with
a given degree sequence. The result was generalized to multigraphs.


\subsection{Remarks}
\begin{itemize}
\item The switchings are described in terms of \textit{oriented} edges.
That makes sense because $M$ is the sum of the degrees and therefore
twice the number of edges.
\item When giving a lower bound for the number of switchings, there are
formulas like $\left[\ell_{1}\right]_{3}-O\left(k_{\mathrm{max}}\ell_{1}^{2}\right)$.
The reasoning for the $O\left(k_{\mathrm{max}}\ell_{1}^{2}\right)$
term is that it bounds the number of triples where two vertices are
loops and the second of those is adjacent to the final vertex. This
is an upper bound for the number of triples of loops which contain
an edge.
\end{itemize}

\section{Random Graphs \cite{JLR00}}


\subsection{Remarks}


\subsubsection{Proposition 1.15}

The ``subsubsequence principle'' means that taking $n$ along any
subsequence of $\NN$, we can find a subsubsequence so that either
$M$ is bounded, or $N-M$ is bounded, or $M\left(N-M\right)/N\to\infty$.
For, suppose none of these are true. Take a subsubsequence where $M/N$
and $\left(N-M\right)/N$ both converge. They cannot both converge
to zero; if for example $M/N\to L>0$ then $M\left(N-M\right)/N=\Theta\left(N-M\right)$,
contradiction.

In fact we can assume that if $M\left(N-M\right)/N\not\to\infty$
then $M$ or $N-M$ is constant, which simplifies the analysis significantly.
If $M$ is constant, prove that $\Pr\left(\left|\Gamma_{M/N}\right|<M\right)\to p_{M}\left(M\right)/e^{M}$,
where $p_{M}$ is the first $M$ terms of the taylor expansion of
$e^{z}$. Also, I think the result of the central limit theorem is
not enough for the estimate $\Pr\left(\left|\Gamma_{M/N}\right|<M\right)\to1/2$,
one needs to dig into the proof by convergence of characteristic function.


\subsubsection{Theorem 5.4}

The process can be viewed as either exploring a random graph or randomly
generating a graph in an exotic edge order. Both are equivalent, but
different viewpoints help understand different parts of the argument.
The ``$k$th step'' means the $k$th saturation, not the $k$th
generation. If the process is dead before step $k$, then $X_{k}$
is not defined. The last paragraph on p109/first paragraph on p110
is confusing and contains some typos, here's a little reprashing of
bits and pieces:

The event that after the first $k$ steps (saturations) there are
fewer than $\left(c-1\right)k/2$ unsaturated vertices remaining and
the process is alive is 
\[
H=\left\{ \mbox{process is alive for step \ensuremath{k}and}\sum_{i=1}^{k}X_{i}-k<\frac{\left(c-1\right)k}{2}\right\} .
\]
Now, at step $i$, if $m$ vertices have been touched there are $n-m$
vertices that can be touched, each with probability $p$, so $X_{i}\sim\mathrm{Bi}\left(n-m,p\right)$.
For event $H$, no more than $\left(c+1\right)k_{+}/2$ vertices can
have been touched up to step $k$, so $X_{i}\ge X_{i}^{-}$ where
$X_{i}^{-}\sim\mathrm{Bi}\left(n-k_{+}\left(c+1\right)/2,p\right)$.
Then $\sum_{i=1}^{k}X_{i}^{-}\sim\mathrm{Bi}\left(k\left(n-n^{2/3}\left(c+1\right)\right)/2,p\right)$
and $\E\left[\sum_{i=1}^{k}X_{i}^{-}\right]>k-1$ for sufficiently
large $n$.

The bound for there to be two giant components uses $e^{Q}\ge1+Q$
for $Q=-c/n$. I think formal justification for the for the $o\left(1\right)$
term in $\rho_{-}+o\left(1\right)$ is quite complicated. Let $Y_{n}\sim\mathrm{Bi}\left(n,c/n\right)$
and $Y\sim\mathrm{Po}\left(c\right)$. Since $Y_{n}\xrightarrow{d}Y$,
for all $\varepsilon$ there is $N$ so that $Y_{n}\le\left(1+\varepsilon\right)Y$
for $n>N$. The pgf of $Y$ is $G:x\mapsto\exp\left(c\left(x^{1+\varepsilon}-1\right)\right)$
so by the implicit function theorem, the solution for $x$ of $G\left(x;\varepsilon\right)=x$
is continuous in $\varepsilon$ and 
\begin{align*}
 & \Pr\left(\mbox{branching process with }X_{i}\sim\mathrm{Bi}\left(n,c/n\right)\mbox{ dies after more than }k_{-}\mbox{ steps}\right)\\
 & \le\Pr\left(\mbox{branching process with }X_{i}\sim\mathrm{Po}\left(c\right)\mbox{ dies after more than }k_{-}\mbox{ steps}\right)+o\left(1\right)
\end{align*}
because $\Pr\left(\mbox{process dies at all}\right)$ is (in the limit)
the same for both cases. But since $k_{-}\to\infty$ the above probability
is $o\left(1\right)$.

To choose an ordered pair of small vertices, first choose a small
vertex arbitrarily (there are $n\rho\left(n,p\right)$ ways to do
this on average), then either choose a small vertex in the same component
(there are at most $k_{-}-1$ ways to do this), or choose a small
vertex in a different component (there are independently $n\rho\left(n-O\left(k_{-}\right),p\right)$
ways of doing this on average). Using a similar bounding argument
as for $\rho\left(n,p\right)$ we have $\rho\left(n-O\left(k_{-}\right),p\right)\sim\rho\left(n,p\right)$.


\subsubsection{Theorem 5.5}

The reasoning behind the estimate $k!k^{2}$ is that each bad subgraph
can be constructed by choosing a path then choosing two points on
that path to glue the endpoints to. The first line of the displayed
inequality is due to Markov's inequality, the second can be derived
with $\left(\frac{2M}{n}\right)^{k+1}>\frac{n^{k+1}\left[M\right]_{k+1}}{\left[n^{2}/2-1\right]_{k+1}}>\frac{\left[n\right]_{k}\left[M\right]_{k+1}}{\left[{n \choose 2}\right]_{k+1}}$
for $2\le k<n/2$.


\subsubsection{Proof of Theorem 9.23}

For the trace identity, prove by induction that $\left(A^{k}\right)_{ij}$
is the sum of all products $a_{i\alpha_{1}}a_{\alpha_{1}\alpha_{2}}\dots a_{\alpha_{k-2}\alpha_{k-1}}a_{\alpha_{k-1}j}$.


\section{Fundamentals of Stein's Method \cite{Ros11}}


\subsection{Summary}


\subsubsection{Overview}

Stein's method is a technique for bounding the ``distance'' between
distributions in some metric of the form $d\left(X,Z\right)=\sup_{h\in\mathcal{H}}\left|\mathbb{E}h\left(X\right)-\mathbb{E}h\left(Z\right)\right|$,
where $\mathcal{H}$ is a family of functions. If $\mathcal{H}=\left\{ 1_{\left(-\infty,x\right]}:x\in\mathbb{R}\right\} $,
then this measures the maximum distance between distribution functions
(Kolmogorov metric). It's often possible to transfer bounds between
different metrics, so it's worthwhile to define some auxilary metrics
for which it's easier to apply Stein's method.

I think the motivations behind Stein's method are twofold. First,
it gives a concrete bound of the kind not usually provided by asymptotic
methods. Second, it can be used to give uniform convergence of probabilities,
which is sometimes a necessary ingredient to asymptotic results where
we allow things to vary (for example, proving something about $\mathbb{G}\left(n,p\right)$)

The idea of Stein's method is that there is some characterizing operator
$\mathcal{A}$ of a random variable $Z$ so that $\mathbb{E}\mathcal{A}f\left(X\right)=0$
for all $f$ in some manageable family $\mathcal{F}$, precisely when
$X$ has the distribution of $Z$. For example, if $Z\sim\mathcal{N}\left(0,1\right)$
then we can choose $\mathcal{A}$ as $f\mapsto\left(x\mapsto f'\left(x\right)-xf\left(x\right)\right)$
and $\mathcal{F}$ as the set of bounded, continuously differentiable
functions. If $Z\sim\mbox{Po}\left(\lambda\right)$ then we can choose
$\mathcal{A}$ as $f\mapsto\left(k\mapsto\lambda f\left(k+1\right)-kf\left(k\right)\right)$
and $\mathcal{F}$ as the set of all bounded functions with bounded
differences.

For all $h\in\mathcal{H}$, we can try to choose $f_{h}$ to solve
(or approximately solve) the functional equation $\mathcal{A}f_{h}=h-\mathbb{E}hZ$
(and adjust/refine $\mathcal{F}$ to include all such $f_{h}$). Then,
$d\left(X,Z\right)\le\sup_{f\in\mathcal{F}}\left|\mathbb{E}\mathcal{A}f\left(X\right)\right|$.
By the choice of $\mathcal{A}$, we hope that this bound is relatively
sharp. Also, we also hope that bounding $\left|\mathbb{E}\mathcal{A}f\left(X\right)\right|$
is easier than directly bounding $\left|\mathbb{E}h\left(X\right)-\mathbb{E}h\left(Z\right)\right|$.


\subsubsection{Size-Bias Coupling}

For a nonnegative integrable random variable $X$, we say $X^{s}$
has the size-bias distribution with respect to $X$ if for all $f$
so that $Xf\left(X\right)$ is integrable, we have $\E\left[Xf\left(X\right)\right]=\E X\E f\left(X^{s}\right)$.
For example, if $X$ is a Bernoulli random variable then we can choose
$X^{s}=1$. The size-bias distribution always exists, with $\d\left(\L X^{s}\right)\left(x\right)=\frac{x}{\E X}\d\left(\L X\right)\left(x\right)$;
that is $\left(\L X^{s}\right)\left(A\right)=\E\left[X\I_{A}\circ X\right]/\E X$.
The size-bias distribution can be seen as weighting (biasing) the
probability of $X$ taking some value $x$ by the size of $x$.

The idea is that we can perturb a nonnegative random variable $X$
to obtain $X^{s}$ in such a way that $X-X^{s}$ has low variance.
If $X$ has mean $\mu$ and variance $\sigma^{2}$, then $\E\left[X^{s}-X\right]=\frac{1}{\mu}\E\left[X^{2}\right]-\mu=\frac{\sigma^{2}}{\mu}$.
If $X$ is approximately Poisson, then this is equal to 1, so if $X-X^{s}$
has low variance then $\E\left[Xf\left(X\right)-\lambda f\left(X+1\right)\right]=\lambda\E\left[f\left(X^{s}\right)-f\left(X+1\right)\right]\approx0$

For the normal case, define $W=\frac{X-\mu}{\sigma}$. If $X^{s}-X$
has low variance then 
\begin{align*}
\E\left[Wf\left(W\right)\right] & =\frac{\mu}{\sigma}\E\left[f\left(\frac{X^{s}-\mu}{\sigma}\right)-f\left(\frac{X-\mu}{\sigma}\right)\right]\\
 & \approx\frac{\mu}{\sigma}\E\left[f'\left(X\right)\left(\frac{X^{s}-X}{\sigma}\right)\right]\\
 & \approx\frac{\mu}{\sigma^{2}}\E\left[f'\left(X\right)\right]\E\left[X^{s}-X\right]\\
 & =\E\left[f'\left(X\right)\right].
\end{align*}
Here's an often-useful description of the size-bias distribution of
a sum of random variables: If $\mathbf{X}=\left(X_{i}\right)_{i=1}^{n}$
is a sequence of random variables, define $\mathbf{X}^{\left(i\right)}$
by $\d\left(\L\mathbf{X}^{\left(i\right)}\right)\left(\mathbf{x}\right)=\frac{x_{i}}{\E X_{i}}\d\left(\L\mathbf{X}\right)\left(\mathbf{x}\right)$,
so that for any $f$, we have $\E\left[f\left(\mathbf{X}^{\left(i\right)}\right)\right]=\E\left[X_{i}f\left(\mathbf{X}\right)\right]/\E\left[X_{i}\right]$.
Let $X=\sum_{i=1}^{n}X_{i}$ and let $I$ satisfy $\Pr\left(I=i\right)=\E\left[X_{i}\right]/\E\left[X\right]$.
Then $\L X^{s}=\L\sum_{i=1}^{n}X_{i}^{\left(I\right)}$.

An interpretation of $\d\left(\L\mathbf{X}^{\left(i\right)}\right)\left(\mathbf{x}\right)=\frac{x_{i}}{\E X_{i}}\d\left(\L\mathbf{X}\right)\left(\mathbf{x}\right)$
for discrete random variables is that the $i$th component of $X^{\left(i\right)}$
has the size-bias distribution of $X_{i}$ and given $X_{i}^{\left(i\right)}=x$,
the other components of $\mathbf{X}^{\left(i\right)}$ have the distribution
of the corresponding components of $\mathbf{X}$, conditioned on $X_{i}=x$.

In practice, we can often represent $X$ as a sum of indicator variables
(which have size-bias distribution 1), each representing some local
property of an underlying space of combinatorial objects. For example,
$X$ might be the number of isolated vertices in a random graph $G\in\GBin np$,
so we would have an indicator for each vertex. For each $G$ we choose
a location at random and force the local property to hold there; this
creates a new object $G'$. Provided we have made the local adjustment
in the right way, we will have $\L X\left(\cdot'\right)=\L X^{s}$
in such a way that $X$ is close to $X^{s}$. For the isolated vertex
example, we can create $G'$ by deleting all the edges incident to
a randomly chosen vertex.


\subsubsection{Exchangeable Pairs for Poisson Approximation}

A pair of random variables $\left(X,X'\right)$ is exchangable if
it has the same distribution as $\left(X',X\right)$. If $\left(X,X'\right)$
are exchangeable, we have $\mathbb{E}\left[1\left\{ X=X'+1\right\} f\left(X\right)\right]=\mathbb{E}\left[1\left\{ X'=X+1\right\} f\left(X'\right)\right]$
so $\mathbb{E}\left[\Pr\left(X'=X-1|X\right)f\left(X\right)\right]=\mathbb{E}\left[\Pr\left(X'=X+1|X\right)f\left(X+1\right)\right]$.

So, if we have $\Pr\left(X'=X-1|X\right)\approx cX$ and $\Pr\left(X'=X+1|X\right)\approx c\lambda$
for some $c$ then $\mathbb{E}\left[\lambda f\left(X\right)-Xf\left(X\right)\right]\approx0$
and $X$ is approximately Poisson.

To choose appropriate $X'$, we want to perturb $X$ in some ``reversible''
way, so that $X$ can also be symmetrically viewed as a perturbation
of $X'$. Sometimes $X$ can be viewed as some statistic on a space
$\Omega$ of combinatorial objects with some distribution $\pi$,
so we can obtain $X'$ with a reversible Markov chain on $\Omega$
that has stationary distribution $\pi$.

For example, if $X$ is a sum of indicator variables $X_{1},.\dots,X_{n}$,
we could view $X$ as the cardinality of a random subset of $\left\{ 1,\dots,n\right\} $.
For the transitions of a Markov chain, we could randomly select some
$i\in\left\{ 1,\dots,n\right\} $ and independently re-determine its
membership. This sort of construction often naturally gives a suitable
exchangeable pair. The event $\left\{ X'=X-1\right\} $ can be interpreted
as a ``death'' and the event $\left\{ X'=X+1\right\} $ can be interpreted
as an ``immigration'' in the Markov chain. The probability of a
death is roughly proportional to the number alive, and the probability
of an immigration is roughly proportional to the sum of the probabilities
for each element to be alive in the stationary distribution, which
is $\mathbb{E}\left[X\right]\approx\lambda$.


\subsection{Remarks}
\begin{itemize}
\item For the proof of proposition 1, the $C\varepsilon/2$ term comes from
a $C\times\varepsilon$ triangular region of integration. The $d_{W}\left(W,Z\right)/\varepsilon$
term comes from the fact that $h_{x,\varepsilon}\varepsilon$ is 1-Lipschitz.
\item For the proof of Lemma 3.4, as well as Cauchy-Schwarz H\"older's
inequality $\mathbb{E}\left|X\right|\le\left(\mathbb{E}X^{2}\right)^{1/2}$
is used.
\item For the proof of Theorem 3.5, the fact $\sum_{i=1}^{n}\sum_{j\in N_{i}}X_{i}=\left(\sum_{i=1}^{n}X_{i}\right)^{2}$
is used.
\end{itemize}

\section{Approximate Computation of Expectations \cite{Ste86}}


\subsection{Summary}

I also draw from \cite{Ste92} and the first section of \cite{CDM05},
which provide similar explanations of the abstraction behind Stein's
method for exchangeable pairs.

Stein's method was originally conceived by Stein as a method of approximately
computing expectations. The common use of Stein's method to bound
the distances between distributions follows naturally from this, but
this paper gives insight as to why the methods for exchangeable pairs
in \cite{Ros11} work.

We redefine some notation from the discussion on \cite{Ros11}. $\E_{0}$
is the expectation operator with respect to an approximating distribution
(for example, Poisson). $T_{0}:\mathcal{F}\mapsto\mathcal{H}$ is
the characterizing operator of that distribution (for example, $f\mapsto\left(k\mapsto\lambda f\left(k+1\right)-kf\left(k\right)\right)$).
For $T_{0}$ to be characterizing means $\mbox{im}T_{0}=\ker\E_{0}$.
We also want an operator $U_{0}$ which captures the idea in \cite{Ros11}
of solving for $f_{h}$: $U_{0}$ must satisfy $T_{0}U_{0}h=h-\E_{0}h$.

From an exchangeable pair $\left(X,X'\right)$ we can construct an
operator $T:F\mapsto\E^{X}\left[F\left(X,X'\right)\right]$ from the
set of antisymmetric functions $\mathcal{F}$. Under some trivially
satisfied conditions, we have $\mbox{im}T=\ker\E$. That is, $T$
is a ``characterizing operator'' for $\E$. We then choose a connection
operator $\alpha:\mathcal{F}_{0}\to\mathcal{F}$.

With this data, Stein's lemma says that $\E_{X}=\E_{0}+\E\circ R$
for some ``remainder'' operator $R$. If our choices were good then
$R$ is provably small. From here we can estimate distance metrics,
or try to estimate specific expectations $\E_{X}h$.


\subsection{Remarks}
\begin{itemize}
\item We frequently use spaces of functions like $\mathcal{X}$ and $\mathcal{F}$.
I think ``space'' means vector space
\end{itemize}

\section{A way of Using Auxiliary Randomization\cite{Ste92}}


\subsection{Remarks}
\begin{itemize}
\item Page 14 says that a characterizing operator should satisfy $\ker\E_{0}=\mbox{im}T_{0}$,
but i think this should really read $\mathcal{X}_{0}\cap\ker\E_{0}=\mbox{im}T_{0}$
or $\E_{0}$ should be restricted somewhere (maybe it's implied, it's
not very clear).
\end{itemize}

\section{Exchangeable pairs and Poisson approximation\cite{CDM05}}


\subsection{Remarks}
\begin{itemize}
\item The ``top row'' and ``bottom row'' of the diagram seem to be mixed
up.
\end{itemize}

\section{Exchangeable Pairs, Switchings and Random Regular Graphs \cite{Joh11}}


\subsection{Summary}

A previous paper \cite{MWW04} proved that small cycle counts were
asymptotically Poisson under some ``natural'' conditions on the
growth of the maximum cycle length, and conjectured that this was
a threshold; if the maximum cycle length was to grow faster the cycle
counts would no longer be asymptotically Poisson. Johnson disproves
the conjecture with Stein's method.

The use of switchings is quite different to Hasheminezhad and McKay.
The switching operation (relation) induces a markov chain on the space
of all regular graphs; by adding loops this chain can be adjusted
so that the stationary distribution is uniform and the chain is reversible.
The Markov chain then gives an exchangeable pair with which to apply
Stein's method.


\subsection{Remarks}
\begin{itemize}
\item Proof of Theorem 7: I think every vertex of $\mathfrak{G}$ needs
to have total \textit{weight} $d_{0}$, not degree $d_{0}$.
\item Unlike most (?) applications of Stein's method, Johnson is more interested
in asymptotics than explicit quantitative bounds between distributions,
so there are undetermined constants. I'm not sure whether it's feasible
or useful to obtain these.
\end{itemize}

\section{Stein's method and the rank distribution of random matrices over
finite fields \cite{FS12}}


\subsection{Summary}

This paper uses the techniques of Poisson approximation in a more
general context. If $X\sim\Po\left(\lambda\right)$ then $\lambda\Pr\left(X=k-1\right)=k\Pr\left(X=k\right)$.
The methods are extended to the case where $a\left(k\right)\Pr\left(X=k-1\right)=b\left(k\right)\Pr\left(X=k\right)$.
In particular, the limiting distribution of the rank of a random matrix
over a finite field is known to have this form.


\section{On Stein's method and perturbations \cite{BCX07}}


\subsection{Summary}

This paper uses Stein's method to compare distributions to ``perturbations''
of the Poisson and Normal distributions. Perturbations are defined
in terms of ``generators'' of measures, which I probably need to
learn about before tackling this paper properly.

\bibliographystyle{amsalpha}
\addcontentsline{toc}{section}{\refname}\bibliography{readings}

\end{document}
