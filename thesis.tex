%% LyX 2.0.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt,english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm,headheight=3cm,headsep=3cm,footskip=1.5cm}
\setlength{\parskip}{\bigskipamount}
\setlength{\parindent}{0pt}
\synctex=1
\usepackage{babel}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{esint}
\setstretch{1.5}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=false]
 {hyperref}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}[section]
  \theoremstyle{plain}
  \newtheorem{cor}[thm]{\protect\corollaryname}
  \theoremstyle{plain}
  \newtheorem{lem}[thm]{\protect\lemmaname}
  \theoremstyle{plain}
  \newtheorem{prop}[thm]{\protect\propositionname}
  \theoremstyle{plain}
  \newtheorem{conjecture}[thm]{\protect\conjecturename}
  \theoremstyle{definition}
  \newtheorem{defn}[thm]{\protect\definitionname}
  \theoremstyle{definition}
  \newtheorem{example}[thm]{\protect\examplename}
  \theoremstyle{remark}
  \newtheorem{rem}[thm]{\protect\remarkname}
  \theoremstyle{remark}
  \newtheorem{claim}[thm]{\protect\claimname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% clickable links
%\usepackage[bookmarks,hidelinks]{hyperref} 

% list hackery
\usepackage{enumitem}
\setlist[enumerate]{label=\textup{(\roman*)},topsep=-5pt}
\setlist[itemize]{topsep=-5pt}

% cleveref allows \ref{thm:asdf} instead of Theorem~\ref{thm:asdf}
\usepackage[nameinlink,capitalise]{cleveref}
\AtBeginDocument{\renewcommand{\ref}[1]{\cref{#1}}}

% LyX won't let me include cleveref before theorem declarations so I need to redefine everything as a hack
\theoremstyle{plain}
\newtheorem{mythm}{\protect\theoremname}[section]
\renewenvironment{thm}{\begin{mythm}}{\end{mythm}}
\theoremstyle{definition}
\newtheorem{mydefn}[mythm]{\protect\definitionname}
\renewenvironment{defn}{\begin{mydefn}}{\end{mydefn}}
\theoremstyle{definition}
\newtheorem{myexample}[mythm]{\protect\examplename}
\renewenvironment{example}{\begin{myexample}}{\end{myexample}}
\theoremstyle{plain}
\newtheorem{myprop}[mythm]{\protect\propositionname}
\renewenvironment{prop}{\begin{myprop}}{\end{myprop}}
\theoremstyle{plain}
\newtheorem{mycor}[mythm]{\protect\corollaryname}
\renewenvironment{cor}{\begin{mycor}}{\end{mycor}}
\theoremstyle{plain}
\newtheorem{mylem}[mythm]{\protect\lemmaname}
\renewenvironment{lem}{\begin{mylem}}{\end{mylem}}
\theoremstyle{plain}
\newtheorem{myconjecture}[mythm]{\protect\conjecturename}
\renewenvironment{conjecture}{\begin{myconjecture}}{\end{myconjecture}}
\theoremstyle{remark}
\newtheorem{myrem}[mythm]{\protect\remarkname}
\renewenvironment{rem}{\begin{myrem}}{\end{myrem}}
\theoremstyle{remark}
\newtheorem{myclaim}[mythm]{\protect\claimname}
\renewenvironment{claim}{\begin{myclaim}}{\end{myclaim}}

% equation cref format
\crefformat{equation}{#2(#1)#3}

% make cref play nicely with different kinds of lists
\newlist{conditions}{enumerate}{3}
\setlist[conditions]{label=\textup{(\roman*)},topsep=-5pt}
\if@cref@capitalise
  \crefname{conditions}{Condition}{Conditions}
\else
  \crefname{conditions}{Condition}{Conditions}
\fi
\crefalias{conditionsi}{conditions}
\crefalias{conditionsii}{conditions}
\crefalias{conditionsiii}{conditions}

% ordered lists should use parens instead of a point
%\renewcommand\theenumi{\arabic{enumi}}
%\renewcommand\labelenumi{(\theenumi)}

% \left(\right) should behave the same as ()
\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

% table of contents spacing tweaks
\usepackage{tocloft}
\setlength\cftparskip{-2pt}
\setlength\cftbeforesecskip{1pt}
\setlength\cftaftertoctitleskip{2pt}

% comment environment
\usepackage{verbatim}

% mainly for light colours color!percent
\usepackage{xcolor}

% shaded WIP notes
\theoremstyle{definition}
\newtheorem{commentthm}{Comment}[section]
\newtheorem{todothm}[commentthm]{Issue}
\usepackage{framed}
\usepackage{lipsum}
\newenvironment{note}
{\colorlet{shadecolor}{blue!5}\begin{shaded}\begin{commentthm}}{\end{commentthm}\end{shaded}}
\newenvironment{todo}
{\colorlet{shadecolor}{red!5}\begin{shaded}\begin{todothm}}{\end{todothm}\end{shaded}}

\makeatother

  \providecommand{\claimname}{Claim}
  \providecommand{\conjecturename}{Conjecture}
  \providecommand{\corollaryname}{Corollary}
  \providecommand{\definitionname}{Definition}
  \providecommand{\examplename}{Example}
  \providecommand{\lemmaname}{Lemma}
  \providecommand{\propositionname}{Proposition}
  \providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

\begin{document}

\title{Stein's Method}


\author{Matthew Kwan}

\maketitle
\begin{comment}
\begin{thm}
t\end{thm}
\begin{cor}
c\end{cor}
\begin{lem}
l\end{lem}
\begin{prop}
p\end{prop}
\begin{conjecture}
c\end{conjecture}
\begin{defn}
d\end{defn}
\begin{example}
e\end{example}
\begin{rem}
r\end{rem}
\begin{claim}
c
\end{claim}
\end{comment}

\global\long\def\floor#1{\left\lfloor #1\right\rfloor }


\global\long\def\ceil#1{\left\lceil #1\right\rceil }


\global\long\def\i{i}


\global\long\def\ii{j}


\global\long\def\NN{\mathbb{N}}


\global\long\def\ZZ{\mathbb{Z}}


\global\long\def\RR{\mathbb{R}}


\global\long\def\CC{\mathbb{C}}


\global\long\def\F{\mathcal{F}}


\global\long\def\range#1{\left[#1\right]}


\global\long\def\rest{|}


\global\long\def\d{\operatorname d}


\global\long\def\id{\operatorname{id}}


\global\long\def\one{\operatorname{\boldsymbol{1}}}


\global\long\def\falling#1#2{\left(#1\right)_{#2}}


\begin{comment}\begin{note}Commentary in blue\end{note}\begin{todo}Issues
in red\end{todo}\end{comment}

\tableofcontents{}

\begin{note}

Here's the general plan:
\begin{itemize}
\item Introduction: introduce with limit theorems (weak convergence) to
motivate Stein's method
\item probability theory review
\item discussion of weak convergence, definition of stein metrics and proofs
that stein metrics are ``compatible'' with weak convergence.

\begin{itemize}
\item Highlight: proof that bounded lipschitz metric metrizes the weak topology
\end{itemize}
\item general discussion of the idea of stein's method

\begin{itemize}
\item bare-hands proof of Berry-Esseen theorem
\end{itemize}
\item Exchangeable pairs

\begin{itemize}
\item fixed points in permutations example
\end{itemize}
\item (maybe) Size-bias coupling

\begin{itemize}
\item subgraphs of erdos-renyi random graphs
\end{itemize}
\end{itemize}
\end{note}


\section{Introduction}

The \emph{central limit theorem}, roughly speaking, says that sums
of independent random variables are ``approximately normal'', with
the approximation improving as the sumber of terms in the sum increases.
Similarly, the \emph{Poisson limit theorem} says that the number of
occurences of independent ``rare'' events over a given time period
is ``approximately Poisson''. These two limit theorems are archetypal
examples of a large class of related results in statistics and probabilistic
combinatorics.

These types of results are generally formally stated asymptotically,
with a particular type of convergence called \emph{weak convergence},
or \emph{convergence in distribution}. Such results are very powerful,
but an obvious shortcoming is that they do not quantify the convergence
any way. It may be that a sequence of random variables is ``asymptotically
normal'', but we cannot say that any particular random variable in
that sequence is individually ``close to normal''. Even if we are
only interested in asymptotic results, it can be problematic that
we cannot say convergence is ``uniform'' in any sense.

The solution to both these problems is to define a distance metric
between probability distributions, and study convergence with respect
to this metric. Fortunately, weak convergence is in fact convergence
with respect to a topology, and this topology is metrizable. That
is, there is a metric on the set of probability distributions such
that the convergence in this metric space is equivalent to weak convergence.

\emph{Stein's method }was introduced by Charles Stein in 1972. It
is most generally a method for approximating expected values. However,
at least in probabilistic combinatorics, Stein's method has proved
especially useful for bounding the distance between probability distributions,
in a variety of metrics consistent with weak convergence. This can
be used to quantify existing limit theorems, and can also be used
as a tool to prove purely asymptotic results.

In this thesis, we set out the theoretical groundwork for Stein's
method, including a rigorous overview of probability theory and a
discussion of weak convergence. We then present a very general framework
for the Stein's method, and outline a few specific ways of applying
the method. We include a number of specific examples and applications.

Unless stated otherwise, all proofs are ``original'' in that I came
up with them, although many are quite straightforward and probably
exist elsewhere. For the proofs that I have adapted from other sources,
I tried to add some new explanation or details.


\subsection{Notation and Assumed Knowledge}

For this thesis, the set of natural numbers $\NN$ includes zero.
We write $\one_{A}$ for the characteristic function of a set $A$:
$\one_{A}\left(x\right)=1$ if $x\in A$, otherwise $\one_{A}\left(x\right)=0$.
The function $f$ restricted to the set $A$ is denoted $f\rest_{A}$.
The falling factorial $n\left(n-1\right)\dots\left(n-k+1\right)$
is denoted $\falling nk$. Finally, $\range k$ denotes the set $\left\{ 1,\dots,k\right\} $. 

Unless otherwise specified, all asympotics are as $n\to\infty$. Apart
from standard asymptotic notation, $f\sim g$ means $f=g\left(1+o\left(1\right)\right)$,
$f\prec g$ means $f=O\left(g\right)$ and $f\asymp g$ means $f=\Theta\left(g\right)$.

In this thesis, unless stated otherwise, graphs are labelled. That
is, they are distinguished even within isomorphism classes. A graph
may not have loops or multiple edges; an object which is allowed to
have loops and/or multiple edges will be called a multigraph. We write
$G\subseteq G'$ to indicate that $G$ is a (not necessarily induced)
subgraph of $G'$.

I will occasionally use results from analysis without proof. I will
usually refer to a numbered theorem in Rudin's Real and Complex Analysis
\cite{Rud66} or Functional Analysis \cite{Rud73} when doing so.


\section{General Probability Theory}

\global\long\def\L#1{\mathcal{L}_{#1}}


\DeclareRobustCommand{\L}[1]{\ifmmode{\mathcal{L}_{#1}}\else\polishL\fi}

\global\long\def\Lb#1{\mathcal{L}\left(#1\right)}


\global\long\def\Om{\Omega}


\global\long\def\om{\omega}


\global\long\def\cA{\mathcal{A}}


\global\long\def\B{\mathcal{B}}


\global\long\def\E{\mathbb{E}}


\global\long\def\F{F}


\global\long\def\N{\mathcal{N}}


\global\long\def\Po{\operatorname{Po}}


\global\long\def\Var{\operatorname{Var}}


\global\long\def\im{\operatorname{im}}


\global\long\def\sgn{\operatorname{sign}}


\global\long\def\supp{\operatorname{supp}}


\global\long\def\Pr{\mathbb{P}}


\global\long\def\m{\mu}


\global\long\def\X{X}


\global\long\def\bX{\mathbf{X}}


\global\long\def\A{A}


\global\long\def\P{\mathcal{P}}


For many combinatorial applications, an informal understanding of
probability theory (often considering only discrete spaces) will suffice.
However, in this thesis a rigorous foundation in probability theory
will be useful. This section will therefore assume knowledge of basic
measure theory; see, for example, \cite{Rud66}. However, where possible,
we will note any simplifications that arise from assuming discreteness,
for the benefit of those less familiar with general probability theory.

No knowledge of probability theory is assumed; the first few subsections
will briefly review the foundations of probability. The reader may
nevertheless want to refer to a probability theory book such as \cite{Kal02}
for some additional detail and further reading.


\subsection{Probability Spaces}
\begin{defn}
A \emph{probability space} is a measure space $\left(\Om,\cA,\Pr\right)$
with $\Pr\left(\Om\right)=1$. In this case we say $\Pr$ is a \emph{probability
measure}, and denote the set of all probability measures on $\left(\Om,\cA\right)$
by $\P\left(\Om,\cA\right)$ or $\P\left(\Om\right)$ if there is
no ambiguity.\end{defn}
\begin{rem}
\label{rem:discrete-space}For our purposes $\Om$ will often be countable,
with $\cA$ as the power set of $\Om$. In this case $\Pr$ is uniquely
defined by the probabilities $\Pr\left(\om\right):=\Pr\left(\left\{ \om\right\} \right)$,
for each $\om\in\Om$. We will discuss specific probability spaces
on combinatorial objects in \ref{sec:random-structures}.\end{rem}
\begin{defn}
An \emph{event} in a probability space $\left(\Om,\cA,\Pr\right)$
is a measurable set $\A\in\cA$.
\end{defn}
For an event $\A$, $\Pr\left(\A\right)$ is interpreted as the ``probability
that $\A$ occurs''. Events will usually be of the form $\A=\left\{ \om\in\Om:\, P\left(\om\right)\mbox{ holds}\right\} $,
where $P\left(\om\right)$ is some property of an object $\om$. For
clarity, we often abuse notation slightly and write $\Pr\left(P\mbox{ holds}\right)$
instead of $\Pr\left(\left\{ \om\in\Om:\, P\left(\om\right)\mbox{ holds}\right\} \right)$.


\subsection{Random Elements}
\begin{defn}
A \emph{random element} is a measurable function $\X:\left(\Om_{1},\cA_{1}\right)\to\left(\Om_{2},\cA_{2}\right)$
between measurable spaces. If $\Om_{2}=\RR^{n}$ for some $n\in\NN$,
with $\cA_{2}$ the Borel $\sigma$-algebra on $\RR^{n}$, then we
say $\X$ is a \emph{random vector}. A one-dimensional random vector
is a \emph{random variable}. If $\Om_{2}$ is countable then we say
$\X$ is \emph{discrete}.\end{defn}
\begin{rem}
Especially in combinatorial spaces, $\Om_{1}$ is often countable.
In this case, any function from a probability space $\left(\Om_{1},2^{\Om_{1}},\Pr\right)$
is measurable.
\end{rem}
To interpret a random variable, we need a probability measure $\Pr$
on the underlying measurable space $\left(\Om_{1},\cA_{1}\right)$
(often, this will be implicit). Then, $\Pr\left(\X\in\A\right)=\Pr\left(\left\{ \om\in\Om_{1}:\X\left(\om\right)\in\A\right\} \right)$
is the probability that $\X$ takes a value in the set $\A$. Often,
we will only be interested in such probabilities: that is, we do not
care about the realization of a random variable as a function on an
underlying probability space. This motivates the following definition:
\begin{defn}
Suppose $\X$ is a random element which takes values in the measurable
space $\left(\Om,\cA\right)$. The \emph{distribution} (or \emph{law})
$\L{\X}$ of $\X$ with respect to an underlying probability $\Pr$
is the pushforward measure with respect to $\X$. That is, it is a
probability measure defined by $\L{\X}\left(\A\right)=\Pr\left(\X^{-1}\left(\A\right)\right)$
for $\A\subseteq\cA$. Also, we occasionally use the notation $\Lb{\X}:=\L{\X}$
for ease of reading.
\end{defn}
It is worth noting that in fact any probability measure is the distribution
of some random element, so we can define a probability distribution
in the abstract and then assert the existence of a random variable
with that distribution. To see this, note that given a probability
measure $\L{}\in\P\left(\Om\right)$, we can choose $\X=\id_{\Om}$
to have $\L{\X}=\L{}$ with respect to the underlying probability
measure $\L{}$. We also use the notation $\X\in\L{}$ to indicate
that $\X$ has distribution $\L{}$.
\begin{example}
The normal distribution with parameters $\mu$ and $\sigma$ is denoted
$\N\left(\mu,\sigma\right)$ or $\N_{\mu,\sigma}$ and is defined
by $\N_{\mu,\sigma}\left(B\right)=\frac{1}{\sigma\sqrt{2\pi}}\int_{B}e^{-\frac{\left(x-\mu\right)^{2}}{2\sigma^{2}}}\d x$
for any Borel set $B$.
\end{example}

\begin{example}
The Poisson distribution with parameter $\lambda$ is denoted $\Po\left(\lambda\right)=\Po_{\lambda}$;
this is defined by $\Po_{\lambda}\left(k\right)=\frac{\lambda^{k}e^{-\lambda}}{k!}$
for all $k\in\NN$. (see \ref{rem:discrete-space}).
\end{example}
It is in general a little tricky to define ``the set of values a
random element can take'', but this is straightforward in the discrete
case.
\begin{defn}
The \emph{support} of a discrete random element $\X$ is the set 
\[
\supp\left(\X\right)=\left\{ k\in\Om:\Pr\left(\X=k\right)>0\right\} .
\]

\end{defn}

\subsection{Dependence and Coupling}
\begin{defn}
Suppose $\X$ and $\X'$ are random elements $\left(\Om_{1},\cA_{1},\Pr\right)\to\left(\Om_{2},\cA_{2}\right)$.
We say that $\X$ and $\X'$ are \emph{independent} if 
\[
\Pr\left(\X\in\A_{2}\right)\Pr\left(\X'\in\A_{2}\right)=\Pr\left(\X\in\A_{2}\mbox{ and }\X'\in\A_{2}\right)
\]
for all $\A_{2}\in\cA_{2}$. If $\A_{1},\A_{1}'\in\cA$ then we analogously
say $\A_{1}$ and $\A_{1}'$ are independent if 
\[
\Pr\left(\A_{1}\right)\Pr\left(\A_{1}'\right)=\Pr\left(\A_{1}\cap\A_{1}'\right).
\]
We can similarly say an event is independent of a random element.
If two objects are not independent, then we say they are \emph{dependent}.
\end{defn}
Intuitively, two objects are dependent if information about one object
can give information about the other. For example, we might be interested
in the probability of an event $\A$, under the assumption that the
event $\A'$ occurs.
\begin{defn}
The \emph{conditional probability} of an even $\A$ given an event
$\A'$ with nonzero probability is $\Pr\left(\A|\A'\right)=\Pr\left(\A\cap\A'\right)/\Pr\left(\A'\right)$.
\end{defn}
We have $\Pr\left(\A|\A'\right)=\Pr\left(\A\right)$ if and only if
$\A$ and $\A'$ are independent.

We can also condition random elements on an event.
\begin{defn}
Suppose $\X:\left(\Om_{1},\cA_{1},\Pr\right)\to\left(\Om_{2},\cA_{2}\right)$
is a random element, and $\A_{1}\in\cA_{1}$ is an event with nonzero
probability. Then the \emph{distribution of $\X$ conditioned on $\A_{1}$}
is denoted by $\L{\X|\A_{1}}$ and defined by $\L{\X|\A_{1}}\left(\A_{2}\right)=\Pr\left(\X\in\A_{2}|\A_{1}\right)$
for $\A_{2}\in\cA_{2}$.
\end{defn}
Given a finite collection of measure spaces $\left(\Om_{1},\cA_{1},\m_{1}\right),\dots,\left(\Om_{n},\cA_{n},\m_{n}\right)$,
recall the construction of the product measure space $\left(\Om,\cA,\m\right):=\left(\prod_{i=1}^{n}\Om_{i},\bigotimes_{i=1}^{n}\cA_{i},\prod_{i=1}^{n}\m_{i}\right)$
(see \cite[Chapter 8]{Rud66}). If a random element takes values in
a product space then each component is measurable, and conversely
if the components of a random tuple are measurable then that tuple
is measurable in the product space. So, we can make the following
definitions:
\begin{defn}
Given random elements $\X_{1},\dots,\X_{n}$ on the same underlying
probability space, $\Lb{\X_{1},\dots,\X_{n}}:=\Lb{\left(\X_{1},\dots,\X_{n}\right)}$
is called the \emph{joint distribution} of $\X_{1},\dots,\X_{n}$.
Conversely, given a random tuple $\left(\X_{1},\dots,\X_{n}\right)$,
each $\Lb{\X_{\i}}$ is called a \emph{marginal distribution}.
\end{defn}
Suppose we have two distributions of random elements $\Lb{\X_{1}}$
and $\Lb{\X_{2}}$. \emph{Coupling} is the technique of constructing
a random ordered pair $\left(\X_{1},\X_{2}\right)$ which realizes
the given distributions as marginal distributions. Usually this is
done by specifying the joint distribution $\Lb{\X_{1},\X_{2}}$.

The idea is that coupling creates a particular kind of dependence
between $\X_{1}$ and $\X_{2}$ that allows us to compare the two
distributions. Often, we are able to make conclusions about the distributions
$\Lb{\X_{\i}}$ which are independent of their specific realizations
as random elements in the coupling.


\subsection{Expected Value}
\begin{defn}
The \emph{expected value }of a random variable $\X$ (or its distribution)
is $\E\X=\int x\d\L{\X}\left(x\right)$.\end{defn}
\begin{rem}
For a random variable $\X$ that takes integer values, this definition
is equivalent to the well-known formula $\E\X=\sum_{x\in\ZZ}x\,\Pr\left(\X=x\right)$.
\end{rem}

\begin{rem}
\label{rem:indicator-expectation}If $\X$ is a random variable that
can be interpreted as counting the number of objects that satisfy
some property, then we can express $\X$ as a sum of indicator variables
$\sum_{\i}\one_{\A_{\i}}$, where $\A_{\i}$ is the event that the
$\i$th object satisfies our property. Noting that $\E$ is linear,
we have $\E\X=\sum_{\i}\E\one_{\A_{\i}}=\sum_{\i}\Pr\A_{\i}$. So,
in order to compute the expectation of $\X$ we just need to compute
the probability that each object satisfies our required property.
\end{rem}
If we fix a particular underlying probability space $\left(\Om,\cA,\Pr\right)$,
we can also equivalently view expectation as a linear functional on
the space of integrable functions (i.e. random variables): $\E f=\int f\left(\om\right)\d\Pr$.
Sometimes we will define a new probability space $\left(\Om,\cA,\Pr'\right)$
on an existing measurable space. In this case we will write $\E_{\Pr'}$
to indicate expectation with respect to the measure $\Pr'$, to avoid
ambiguity. We can also define the expectation functional of a random
variable $\E_{\X}:=\E_{\Lb{\X}}$, so that $\E_{\X}f=\E f\left(\X\right)$.

\begin{comment}

If $\iota$ is the natural injection that takes $r\in\RR$ to the
constant function $x\mapsto r$, and $I$ is the identity operator
on $L^{1}\left(\Om,\Pr\right)$, then $\iota\E$ and $I-\iota\E$
are both projection operators on $L^{1}\left(\Om,\Pr\right)$. In
particular

\end{comment}

In fact, probability measures are uniquely determined by their expectation
functional, because $\E_{\Pr}\one_{\A}=\Pr\left(\A\right)$ for all
events $\A$. In fact, it will be an important fact for later that
much weaker classes than $\left\{ \one_{\A}:\A\mbox{ is an event}\right\} $
can distinguish expectation operators.

\global\long\def\cH{\mathcal{H}}


\global\long\def\h{h}

\begin{defn}
\label{def:determining-class}A set of real functions $\cH$ is a
\emph{determining class} if $\E_{\Pr_{1}}\h=\E_{\Pr_{2}}\h$ for all
$\h\in\cH$ implies that $\Pr_{1}=\Pr_{2}$.
\end{defn}
\begin{comment}
\begin{defn}
The \emph{variance }of a random variable $\X$ is $\Var\X=\E\left(\X-\E X\right)^{2}$.
The \emph{$n$th moment} of a random variable $\X$ is $\E\left[\X^{n}\right]$.
\end{defn}
conditional variance?

\end{comment}

Another important concept later will be the idea of conditional expectation.
\begin{defn}
The expected value of a random variable with distribution $\L{\X|\A_{1}}$
is called the \emph{conditional expected value of $\X$ given $\A_{1}$}
and is denoted $\E\left[\X|\A_{1}\right]$.
\end{defn}
We can also define conditional expectation with respect to another
random variable. If $\X_{1}$ and $\X_{2}$ are random variables defined
on the same underlying probability space $\left(\Om,\cA,\Pr\right)$,
then the sets $\X_{2}^{-1}\left(B\right)$ for Borel $B$ comprise
a sub-$\sigma$-algebra $\cA'$ of $\cA$. Then, $\m:\A'\mapsto\E\left[\X_{1}\one_{\A'}\right]$
is a signed measure on $\cA'$ that is absolutely continuous with
respect to the restriction of $\Pr$ to $\cA'$. By the Radon-Nikodym
theorem (see \cite[Theorem 6.10]{Rud66}) there is an $\cA'$-measurable
random variable $\E\left[\X_{1}|\X_{2}\right]$ that satisfies $\E\left[\X_{1}\one_{\A'}\right]=\E\left[\E\left[\X_{1}|\X_{2}\right]\one_{\A'}\right]$
for all $\A'$ in $\cA'$. This random variable is almost uniquely
defined: for any two choices of $\E\left[\X_{1}|\X_{2}\right]$, the
probability that they differ is zero.
\begin{defn}
The random variable $\E\left[\X_{1}|\X_{2}\right]$ as defined above
is called the \emph{conditional expectation of $\X_{1}$ with respect
to $\X_{2}$}. We can also view conditional expectation as a linear
operator between functions: we define $\E^{\X_{2}}$ by $\X_{1}\mapsto\E\left[\X_{1}|\X_{2}\right]$.\end{defn}
\begin{rem}
This definition generalizes the previous definition of expectation
conditioned on an event: if $\om\in\A$ and $\Pr\left(\A\right)>0$
then $\E\left[\X|\one_{\A}\right]\left(\om\right)=\E\left[\X|\A\right]$.
\end{rem}

\begin{rem}
Note that if $\X_{2}$ is discrete then we do not need to invoke Radon-Nikodym.
We can define $\E\left[\X_{1}|\X_{2}\right]$ by $\E\left[\X_{1}|\X_{2}\right]\left(\om\right)=\E\left[\X_{1}|\X_{2}=\X_{2}\left(\om\right)\right]$
for all $\om\in\Om$ with $\Pr\left(\X_{2}=\X_{2}\left(\om\right)\right)>0$;
this defines $\E\left[\X_{1}|\X_{2}\right]$ up to a set of probability
zero.
\end{rem}
We finally present a simple consequence of the definition of conditional
expectation.
\begin{prop}
[Tower Law of Expectation]\label{prop:tower-law}Suppose $\X_{1}$
and $\X_{2}$ are random variables defined on the same underlying
probability space $\left(\Om,\cA\right)$. Then $\E\left[\E^{\X_{2}}\X_{1}\right]=\E\left[\X_{1}\right]$.\end{prop}
\begin{proof}
$\E\left[\E^{\X_{2}}\X_{1}\right]=\E\left[\E\left[\X_{1}|\X_{2}\right]\one_{\Om}\right]=\E\left[\X_{1}\one_{\Om}\right]=\E\left[\X_{1}\right]$
\end{proof}

\subsection{Markov Chains}

\begin{note}

I'll need to define Markov Chains, stationary distributions, irreducibility
and time-reversibility.

Perhaps I should talk more generally about stochastic processes, because
applying exchangeable pairs to Stein's method is has connections with
Ornstein-Uhlenbeck processes and also Stein's method can be applied
to Poisson processes.

\end{note}

\begin{todo}\label{todo:graph-to-markov}

Every regular graph naturally defines a reversible markov chain on
its vertex set, stationary with respect to the uniform distribution.

\end{todo}


\subsection{\label{sec:weak-topology}The Weak Topology on Probability Measures}

\global\long\def\cH{\mathcal{H}}


\global\long\def\h{h}


\global\long\def\TV{\mathrm{TV}}


\global\long\def\BL{\mathrm{BL}}


\global\long\def\K{\mathrm{K}}


\global\long\def\W{\mathrm{W}}

\begin{defn}
Let $\left(\X_{n}\right)_{n\in\NN}$ be a sequence of random variables.
We say $\X_{n}$ \emph{converges in distribution} to a random variable
$\X$ if $\E f\left(\X_{n}\right)\to\E f\left(\X\right)$ for all
bounded continuous functions $f$. Alternatively, we say $\Lb{\X_{n}}$
converges \emph{weakly} to $\Lb{\X}$, or simply $\Lb{\X_{n}}\to\Lb{\X}$.
\end{defn}
This is only one of a number of equivalent and very natural definitions
for convergence in distribution.
\begin{defn}
The \emph{distribution function} $\F_{\X}$ of a random variable $\X$
is defined by $\F_{\X}\left(x\right)=\Pr\left(\X\le x\right)$.\end{defn}
\begin{thm}
\label{prop:dist}The following are equivalent.

\begin{conditions}

\item \label{prop:dist-def}$\Lb{\X_{n}}\to\Lb{\X}$

\item \label{prop:dist-liminf}$\liminf_{n\to\infty}\L{\X_{n}}\left(U\right)\ge\L{\X}\left(U\right)$
for every open $U\subseteq\RR$

\item \label{prop:dist-F}$\F_{\X_{n}}\left(x\right)\to\F_{\X}\left(x\right)$
for all $x$ where $\F_{\X}$ is continuous

\item (L\'evy's continuity theorem, see \cite[Theorem 4.3]{Kal02})
$\E e^{it\X_{n}}\to\E e^{it\X}$ for all $t\in\RR$.

\end{conditions}
\end{thm}
The equivalence of \ref{prop:dist-def,prop:dist-liminf,prop:dist-F}
is (part of) a well-known and relatively elementary result called
the Portmanteau Theorem (\cite[Theorem 3.25]{Kal02}).

When $\X$ and each $\X_{n}$ are integer random variables, then \ref{prop:dist-F}
reduces to the condition that $\Pr\left(\X_{n}=k\right)\to\Pr\left(\X=k\right)$
for all $k$. This characterization is usually used to prove the Poisson
limit theorem.

Classically, distributional convergence results are often proved by
L\'evy's continuity theorem. For example, this approach is usually
used to prove the central limit theorem. For combinatorial applications,
convergence in distribution can also be proved by the ``method of
moments'': if $\X$ is the only random variable with the moments
$\left(\E\X^{k}\right)_{k\in\NN}$, then $\Lb{\X_{n}}\to\Lb{\X}$
if $\E\X_{n}^{k}\to\E\X^{k}$. Convergence in distribution can also
sometimes be inferred from stronger forms of convergence when $\X$
and all the $\X_{n}$ are coupled to the same underlying space.

A disadvantage of all these approaches is that they provide little
information about the rate of convergence.

In functional analysis terms, note that expectation operators are
bounded linear functionals on the space of real bounded continuous
functions. Then, $\Lb{\X_{n}}\to\Lb{\X}$ just means that $\E_{\X_{n}}\to\E_{\X}$
in the weak-star topology. The subspace corresponding to $\P\left(\RR\right)$
is confusingly called the \emph{weak topology} on probability distributions.

Although $C_{b}\left(\RR\right)^{*}$ is not metrizable, the unit
ball (in operator norm) of $C_{b}\left(\RR\right)^{*}$ is in fact
metrizable (see \cite[Theorems 3.15 and 3.16]{Rud73}). Every expectation
functional $\E$ has unit operator norm because $\E\left|\h\right|\le\E\left|1\right|=1$
for $\h$ with unit uniform norm. So, the weak topology is metrizable.
\begin{defn}
\label{def:general-metrics}Let $\cH$ be a determining class of real
measurable ``test'' functions that are uniformly absolutely bounded.
Define $d_{\cH}:\P\left(\RR\right)^{2}\to\RR^{+}$ by $d_{\cH}\left(\Pr_{1},\Pr_{2}\right)=\sup_{\h\in\cH}\left|\E_{\Pr_{1}}\h-\E_{\Pr_{2}}\h\right|$.
For random variables $\X_{1},\X_{2}$, we write $d_{\cH}\left(\X_{1},\X_{2}\right)$
instead of $d_{\cH}\left(\Lb{\X_{1}},\Lb{\X_{2}}\right)$.\end{defn}
\begin{prop}
Each $d_{\cH}$ as defined above is a metric.\end{prop}
\begin{proof}
Since the functions in $\cH$ are bounded, $\E_{\Pr_{1}}\h$ is well-defined
for every $\h\in\cH$. Since the bound is uniform, the supremum in
the definition of $d_{\cH}$ is finite. It is immediate that $d_{\cH}$
is non-negative, symmetric and satisfies the triangle inequality.
Finally, $d_{\cH}\left(\Pr_{1},\Pr_{2}\right)=0$ implies that $\E_{\Pr_{1}}\h=\E_{\Pr_{2}}\h$
for all $\h\in\cH$. Since $\cH$ is a determining class, $\Pr_{1}=\Pr_{2}$.\end{proof}
\begin{defn}
\label{def:special-metrics}We define some special cases of $d_{\cH}$.\begin{itemize}

\item If $\cH_{\K}=\left\{ \one_{\left(-\infty,x\right]}:x\in\RR\right\} $
then $d_{\K}:=d_{\cH_{\K}}$ is called the \emph{Kolmogorov metric}.

\item Let $\cH_{\BL}$ be the set of functions $\h$ that are absolutely
bounded by 1 (that is, $\left|\h\left(x\right)\right|\le1$ for all
$x\in\RR$), and have Lipschitz constant at most 1 (that is, $\left|\h\left(x_{1}\right)-\h\left(x_{2}\right)\right|\le\left|x_{1}-x_{2}\right|$
for all $x_{1},x_{2}\in\RR$). Then, $d_{\BL}:=d_{\cH_{\BL}}$ is
called the \emph{Bounded Lipschitz metric}.

\item If $\cH_{\TV}$ is the set of functions $\one_{B}$ for Borel
$B$, then $d_{\TV}:=d_{\cH_{\TV}}$ is called the \emph{total variation
metric}.

\item If $\cH_{\W}$ is the set of functions with Lipschitz constant
at most 1, then $d_{\W}:=d_{\cH_{\W}}$ is called the \emph{Wasserstein
}metric. However, since $\cH$ is not uniformly bounded, $d_{\W}$
is not strictly speaking a metric on $\P\left(\RR\right)$; the Wasserstein
metric can only be used to compare distributions with finite first
moment.\end{itemize}\end{defn}
\begin{prop}
\label{prop:metric}The ``metrics'' in \ref{def:special-metrics}
are actually metrics.\end{prop}
\begin{proof}
First, note that if $\Pr_{1},\Pr_{2}\in\P\left(\RR\right)$ have finite
first moment then (with $\X_{1}\in\Pr_{1}$ and $\X_{2}\in\Pr_{2}$),
then for all $\h\in\cH_{\W}$,
\[
\left|\E h\left(\X_{1}\right)-\E h\left(\X_{2}\right)\right|\le\left|\left(\E\left|\X_{1}\right|+h\left(0\right)\right)-\left(\E\left|\X_{2}\right|+h\left(0\right)\right)\right|\le\E\left|\X_{1}\right|+\E\left|\X_{2}\right|.
\]
So,
\[
d_{\W}\left(\Pr_{1},\Pr_{2}\right)<\infty.
\]
Now, we just need to check that each of $\cH_{\K}$, $\cH_{\BL}$,
$\cH_{\TV}$, $\cH_{\W}$ are determining classes. Let $\Pr_{1},\Pr_{2}\in\P\left(\RR\right)$
satisfy $\E_{\Pr_{1}}\h=\E_{\Pr_{2}}\h$ for each $\h$ in $\cH$.

If $\cH$ is $\cH_{\K}$ or $\cH_{\TV}$ then $\one_{\left(-\infty,x\right]}\in\cH$
for all $x\in\RR$ so 
\[
\Pr_{1}\left(\left(-\infty,x\right]\right)=\E_{\Pr_{1}}\one_{\left(-\infty,x\right]}=\E_{\Pr_{2}}\one_{\left(-\infty,x\right]}=\Pr_{2}\left(\left(-\infty,x\right]\right).
\]
Since $\left\{ \left(-\infty,x\right]:x\in\RR\right\} $ generates
the Borel $\sigma$-algebra, $\Pr_{1}=\Pr_{2}$. We have shown that
$\cH_{\K}$ and $\cH_{\TV}$ are determining classes.

For $x\in\RR$ and $0<\varepsilon\le1$, let $\h_{x,\varepsilon}$
be the continuous function which takes the value 1 on the set $\left(-\infty,x\right]$,
takes the value 0 on the set $\left[x+\varepsilon,\infty\right)$,
and is linearly interpolated in the range $\left[x,x+\varepsilon\right]$.
Suppose $\cH$ is $\cH_{\BL}$ or $\cH_{\W}$, so that each $\varepsilon\h_{x,\varepsilon}\in\cH$.
It follows that $\E_{\Pr_{1}}\h_{x,\varepsilon}=\E_{\Pr_{2}}\h_{x,\varepsilon}$
for each $x\in\RR$ and $0<\varepsilon\le1$. For each $x\in\RR$,
note that $\h_{x,1/n}\to\one_{\left(-\infty,x\right]}$ pointwise,
and each $\h_{x,1/n}\le1$. By the dominated convergence theorem (see
\cite[Theorem 1.34]{Rud66}), 
\[
\Pr_{1}\left(\left(-\infty,x\right]\right)=\E_{\Pr_{1}}\one_{\left(-\infty,x\right]}=\lim_{n\to\infty}\E_{\Pr_{1}}\h_{x,1/n}=\lim_{n\to\infty}\E_{\Pr_{2}}\h_{x,1/n}=\E_{\Pr_{2}}\one_{\left(-\infty,x\right]}=\Pr_{2}\left(\left(-\infty,x\right]\right),
\]
so $\cH_{\BL}$ and $\cH_{\W}$ are determining classes, as above.\end{proof}
\begin{prop}
\label{prop:stronger-than-weak}The metrics in \ref{def:special-metrics}
are each stronger than the weak topology.\end{prop}
\begin{proof}
We show that $d_{\cH}\left(\X_{n},\X\right)\to0$ implies $\Lb{\X_{n}}\to\Lb{\X}$
for $\cH\in\left\{ \cH_{\K},\cH_{\W},\cH_{\TV}\right\} $.

If $d_{\K}\left(\X_{n},\X\right)\to0$ or $d_{\TV}\left(\X_{n},\X\right)\to0$
then $\F_{\X_{n}}\to\F_{\X}$ uniformly, so certainly \ref{prop:dist-F}
of \ref{prop:dist} holds.

Now, suppose $d_{\BL}\left(\X_{n},\X\right)\to0$ (this will automatically
be true if $d_{\K}\left(\X_{n},\X\right)\to0$). Let $d_{n}=\sqrt{d_{\BL}\left(\X_{n},\X\right)}$
and recall the definition of $\h_{x,\varepsilon}$ from the proof
of \ref{prop:metric}. Since $d_{n}\h_{x,d_{n}}\in\cH_{\W}$ for each
$n\in\NN$, we have 
\[
\E_{\X_{n}}\h_{x,d_{n}}-\E_{\X}\h_{x,d_{n}}\le d_{\W}\left(\X_{n},\X\right)/d_{n}=d_{n}\to0
\]
 uniformly for $x\in\RR$. Now, note that 
\[
\F_{\X}\left(x-\varepsilon\right)\le\E_{\X}h_{x-\varepsilon,\varepsilon}\le\F_{\X}\left(x\right)\le\E_{\X}h_{x,\varepsilon}\le\F_{\X}\left(x+\varepsilon\right)
\]
 for any random variable $\X$. If $\F_{\X}$ is continuous at $x$
then 
\begin{align*}
\F_{\X_{n}}\left(x\right)-\F_{\X}\left(x\right) & \le\left(\E_{\X_{n}}\h_{x,d_{n}}-\E_{\X}\h_{x,d_{n}}\right)+\left(\F_{\X}\left(x+d_{n}\right)-\F_{\X}\left(x\right)\right)\to0\\
\F_{\X_{n}}\left(x\right)-\F_{\X}\left(x\right) & \ge\left(\E_{\X_{n}}\h_{x-d_{n},d_{n}}-\E_{\X}\h_{x-d_{n},d_{n}}\right)+\left(\F_{\X}\left(x-d_{n}\right)-\F_{\X}\left(x\right)\right)\to0
\end{align*}
so \ref{prop:dist-F} of \ref{prop:dist} holds.\end{proof}
\begin{thm}
\label{thm:BL-metrizes-weak}The bounded Lipschitz metric metrizes
the weak topology.
\end{thm}
We will need a small lemma to prove \ref{thm:BL-metrizes-weak}.
\begin{lem}
\label{lem:prokhorov}Let $S\subseteq\P\left(\RR\right)$ be compact
in the weak topology. For each $\varepsilon>0$, there is $k\in\NN$
such that with $\sup_{\Pr\in S}\Pr\left(\left(-k,k\right)^{c}\right)\le\varepsilon$.\end{lem}
\begin{proof}
Fix $\varepsilon>0$. Suppose for the purpose of contradiction that
for all $k\in\NN$ there is $\Pr_{k}\in S$ with $\Pr_{k}\left(\left(-k,k\right)^{c}\right)>\varepsilon$.
Since $S$ is compact, there is a subsequence $\left(\Pr_{k_{n}}\right)_{n\in\NN}$
and a measure $\Pr\in S$ with $\Pr_{k_{n}}\to\Pr$ as $n\to\infty$.
By \ref{prop:dist-liminf} of \ref{prop:dist}, for each $k\in\NN$
we have
\[
\Pr\left(\left(-k,k\right)\right)\le\liminf_{n\to\infty}\Pr_{k_{n}}\left(\left(-k,k\right)\right)\le\liminf_{n\to\infty}\Pr_{k_{n}}\left(\left(-k_{n},k_{n}\right)\right)\le1-\varepsilon.
\]
This is a contradiction because $\Pr\left(\left(-k,k\right)\right)=\E_{\Pr}\one_{\left(-k,k\right)}\to\E_{\Pr}1=1$
as $k\to\infty$, by the dominated convergence theorem (see \cite[Theorem 1.34]{Rud66}).
\end{proof}

\begin{proof}
[Proof of \ref{thm:BL-metrizes-weak}](Adapted from \cite[Theorem 7.12]{Vil03}).
Let $\Pr_{n}\to\Pr_{\infty}$ weakly, and suppose for the purpose
of contradiction that $d_{\BL}\left(\Pr_{n},\Pr_{\infty}\right)\not\to0$.
Then there is $\varepsilon>0$ and a subsequence $\left(\Pr_{k_{n}}\right)_{n\in\NN}$
with $d_{\BL}\left(\Pr_{k_{n}},\Pr_{\infty}\right)>2\varepsilon$
for all $n$. To simplify notation, redefine $\Pr_{n}$ to be $\Pr_{k_{n}}$
for each $n$ (we still have $\Pr_{n}\to\Pr_{\infty}$ weakly).

Now, by the definition of $d_{\BL}$, for each $n\in\NN$ there is
$\h_{n}=\h_{n}^{\left(0\right)}\in\cH_{\BL}$ with 
\[
\left|\E_{\Pr_{n}}\h_{n}-\E_{\Pr}\h_{n}\right|\ge d_{\BL}\left(\Pr_{n},\Pr\right)-\varepsilon>\varepsilon.
\]
For each $k\in\ZZ^{+}$, let $\cH_{\BL}^{\left(k\right)}=\left\{ \h\rest_{\left[-k,k\right]}:\h\in\cH_{\BL}\right\} $.
By the Arzela-Ascoli theorem (see \cite[Theorem 11.28]{Rud66}), each
$\cH_{\BL}^{\left(k\right)}$ is a compact subset of $C_{b}\left(\left[-k,k\right]\right)$
with the uniform norm. So, for each $k\in\ZZ^{+}$, we can inductively
choose a subsequence $\left(\h_{n}^{\left(k\right)}\right)_{n\in\NN}$
of $\left(\h_{n}^{\left(k-1\right)}\right)_{n\in\NN}$ such that $\h_{n}^{\left(k\right)}\rest_{\left[-k,k\right]}$
converges uniformly to some $\h^{\left(k\right)}\in\cH_{\BL}^{\left(k\right)}$.

Note that $\h_{n}^{\left(n\right)}\rest_{\left[-k,k\right]}\to\h^{\left(k\right)}$
uniformly for each $k\in\ZZ^{+}$, so that $\h_{n}^{\left(n\right)}$
converges pointwise to some function $\h:\RR\to\RR$ that satisfies
$\h\rest_{\left[-k,k\right]}=\h^{\left(k\right)}$ for all $k\in\ZZ^{+}$.
Since $\h$ is bounded by 1 and has Lipschitz constant less than 1
on each $\left[-k,k\right]$, it follows that $\h\in\cH_{\BL}$ and
in particular $\h$ is bounded and continuous. Finally, note
\begin{alignat*}{1}
\left|\E_{\Pr_{n}}\h_{n}-\E_{\Pr_{\infty}}\h_{n}\right| & \le\left|\E_{\Pr_{n}}\left[\left(\h_{n}-\h\right)\one_{\left[-k,k\right]}\right]\right|+\left|\E_{\Pr_{\infty}}\left[\left(\h_{n}-\h\right)\one_{\left[-k,k\right]}\right]\right|\\
 & +\left|\E_{\Pr_{n}}\left[\left(\h_{n}-\h\right)\one_{\left[-k,k\right]^{c}}\right]\right|+\left|\E_{\Pr_{\infty}}\left[\left(\h_{n}-\h\right)\one_{\left[-k,k\right]^{c}}\right]\right|\\
 & +\left|\E_{\Pr_{n}}\h-\E_{\Pr_{\infty}}\h\right|
\end{alignat*}
By the definition of weak convergence, there is $N\in\NN$ such that
\[
\left|\E_{\Pr_{n}}\h-\E_{\Pr_{\infty}}\h\right|\le\frac{\varepsilon}{5}
\]
for $n>N$. Since the weak topology is metrizable, $\left\{ \Pr_{n}\right\} _{n\in\NN\cup\left\{ \infty\right\} }$
is compact so by \ref{lem:prokhorov}, there is $k\in\NN$ such that
\[
\E_{\Pr_{n}}\left[\left(\h_{n}-\h\right)\one_{\left[-k,k\right]^{c}}\right]\le2\Pr_{n}\left[\left(-k,k\right)^{c}\right]\le\frac{\varepsilon}{5}
\]
for $n\in\NN\cup\left\{ \infty\right\} $. Since $\left(\h_{n}^{\left(n\right)}\right)_{n\in\NN}$
is a subsequence of $\left(\h_{n}\right)_{n\in\NN}$ and $\h_{n}^{\left(n\right)}\one_{\left[-k,k\right]}$
converges uniformly to $\h\one_{\left[-k,k\right]}$, there is $n>N$
such that 
\[
\left\Vert \left(\h_{n}-\h\right)\one_{\left[-k,k\right]}\right\Vert _{\infty}\le\frac{\varepsilon}{5}.
\]
For this $n$ we have $\left|\E_{\Pr_{n}}\h_{n}-\E_{\Pr_{\infty}}\h_{n}\right|\le\varepsilon$.
This is a contradiction.
\end{proof}
\ref{prop:stronger-than-weak} tells us that our selection of ``special''
metrics are all ``consistent'' with weak convergence in some way,
and \ref{thm:BL-metrizes-weak} tells us that convergence in the Bounded
Lipschitz metric is exactly the same as weak convergence. Typically,
it will be natural to work with the total variation metric for Poisson
approximation, and to work with the Wasserstein metric for Normal
approximation. In applications, we may be most interested in the Kolmogorov
metric. Therefore, it is sometimes useful to transfer results between
metrics (though, this usually results in worse constants than working
directly in the desired metric).

\begin{todo}It may be worthwhile to actually characterize the Wasserstein,
Kolmogorov and Total Variation topologies. In particular, Wasserstein
convergence is just weak convergence plus convergence of the first
moment.\end{todo}
\begin{defn}
If (for all $x$), $\F_{\X}\left(x\right)=\int_{-\infty}^{x}f_{\X}\left(x\right)\d x$
for some $f_{\X}$, then $f_{\X}$ is called the \emph{Lebesgue density}
of $\X$, and $\X$ is called a \emph{continuous} random variable.
\end{defn}
If $\X$ is a continuous random variable, then $\E_{\X}\one_{\A}=\int_{\RR}\one_{\A}\left(x\right)f_{\X}\left(x\right)\d x$,
so by linearity $\E_{\X}\h=\int_{\RR}\h\left(x\right)f_{\X}\left(x\right)\d x$
for simple $\h$. This extends to all $\h$ by the measure-theoretic
definition of the integral.
\begin{prop}
Let $\X_{1},\X_{2}$ be random variables.\begin{enumerate}

\item \label{prop:transfer-K/TV}$d_{\K}\left(\X_{1},\X_{2}\right)\le d_{\TV}\left(\X_{1},\X_{2}\right)$

\item \label{prop:transfer-BL/W}$d_{\BL}\left(\X_{1},\X_{2}\right)\le d_{\W}\left(\X_{1},\X_{2}\right)$

\item \label{prop:transfer-K/W}If $\left|f_{\X_{2}}\left(x\right)\right|\le C$
for all $x$, then $d_{\K}\left(\X_{1},\X_{2}\right)\le\sqrt{2Cd_{\BL}\left(\X_{1},\X_{2}\right)}$.

\end{enumerate}\end{prop}
\begin{proof}
(Adapted from \cite[Proposition 1.2]{Ros11}). \ref{prop:transfer-K/TV,prop:transfer-BL/W}
are immediate from the definition. Then, as in the proof of \ref{prop:stronger-than-weak},\vspace{-10pt}
\begin{eqnarray*}
\F_{\X_{n}}\left(x\right)-\F_{\X}\left(x\right) & \le & \left(\E_{\X_{n}}\h_{x,\varepsilon}-\E_{\X}\h_{x,\varepsilon}\right)+\left(\E_{\X}\h_{x,\varepsilon}-\F_{\X}\left(x\right)\right)\\
 & \le & d_{\BL}\left(\X_{1},\X_{2}\right)/\varepsilon+\int_{x}^{x+\varepsilon}\h_{x,\varepsilon}\, f_{\X}\left(x\right)\d x\\
 & \le & d_{\BL}\left(\X_{1},\X_{2}\right)/\varepsilon+C\varepsilon/2
\end{eqnarray*}
and similarly\vspace{-10pt}
\begin{eqnarray*}
\F_{\X_{n}}\left(x\right)-\F_{\X}\left(x\right) & \ge & -d_{\BL}\left(\X_{1},\X_{2}\right)/\varepsilon-C\varepsilon/2,
\end{eqnarray*}
So, we can take $\varepsilon=\sqrt{2d_{\BL}\left(\X_{1},\X_{2}\right)/C}$
to prove \ref{prop:transfer-K/W}.\end{proof}
\begin{example}
If $\L{\X_{2}}=\N\left(0,1\right)$ then $f_{\X_{2}}\left(x\right)=\left(2\pi\right)^{-1/2}e^{-x^{2}/2}$
so we can take $C=\left(2\pi\right)^{-1/2}$ to obtain $d_{\K}\le\left(2/\pi\right)^{1/4}\sqrt{d_{\BL}\left(\X_{1},\X_{2}\right)}$.
\end{example}
\begin{comment}In a combinatorial setting, many of our results are
about integer random variables. The total variation metric is usually
exclusively used in this case.
\begin{prop}
If $\X_{1},\X_{2}$ are integer-valued random variables, then
\[
d_{\TV}\left(\X_{1},\X_{2}\right)=\frac{1}{2}\sum_{k\in\ZZ}\left|\Pr\left(\X_{1}=k\right)-\Pr\left(\X_{2}=k\right)\right|.
\]
\end{prop}
\begin{proof}
For any Borel set $\A$, let $d_{\A}=\Pr\left(\X_{1}\in\A\right)-\Pr\left(\X_{2}\in\A\right)$,
so that $d_{\TV}\left(\X_{1},\X_{2}\right)=\sup_{\A}\left|d_{\A}\right|$.
Define 
\begin{align*}
\A_{<} & =\left\{ k\in\ZZ:\,\Pr\left(\X_{1}=k\right)<\Pr\left(\X_{2}=k\right)\right\} ,\\
\A_{>} & =\left\{ k\in\ZZ:\,\Pr\left(\X_{1}=k\right)>\Pr\left(\X_{2}=k\right)\right\} .
\end{align*}
For any Borel $\A$, we have 
\begin{align*}
d_{\A} & =\sum_{k\in\ZZ\cap\A}\left(\Pr\left(\X_{1}=k\right)-\Pr\left(\X_{2}=k\right)\right)\\
 & \le\sum_{k\in\A_{>}}\left(\Pr\left(\X_{1}=k\right)-\Pr\left(\X_{2}=k\right)\right)\\
 & =d_{\A_{>}}
\end{align*}
and similarly $-d_{\A}\ge d_{\A_{<}}$. Since $d_{\A_{>}}=-d_{\A_{<}}$,
we have 
\[
d_{\TV}\left(\X_{1},\X_{2}\right)=\left(d_{\A_{>}}-d_{\A_{<}}\right)/2=\frac{1}{2}\sum_{k\in\ZZ}\left|\Pr\left(\X_{1}=k\right)-\Pr\left(\X_{2}=k\right)\right|.
\]

\end{proof}
\end{comment}


\section{Random Combinatorial Structures\label{sec:random-structures}}

\global\long\def\rG#1#2{\mathcal{G}_{#1,#2}}


\global\long\def\rS#1{\mathcal{S}_{#1}}

\begin{defn}
Given a finite space of combinatorial objects $\Om$, a probability
space $\left(\Om,2^{\Om},\Pr\right)$ is often called a \emph{model}
of $\Om$.
\end{defn}

\begin{defn}
In a probability space $\left(\Om,2^{\Om},\Pr\right)$ where $\Om$
is finite, if $\Pr\left(\om\right)=1/\left|\Om\right|$ for each $\om\in\Om$,
then we say the space is \emph{uniform}.
\end{defn}
Uniform models are the simplest examples of random structures. For
example, the uniform space $\rS n$ of permutations on $n$ elements
has $\Pr\left(\sigma\right)=1/n!$ for each $\sigma\in S_{n}$. The
uniform random graph model $\rG nM$ has $\Pr\left(G\right)={{n \choose 2} \choose M}^{-1}$
for each graph $G$ on the vertex set $\range n$ which has $M$ edges.
The uniform random regular graph model $\rG nd$ is uniform on the
set of all $d$-regular graphs on the vertex set $\range n$, though
an explicit formula for the number of such graphs is not known.

As an important example of a (generally) non-uniform model, the (Erd\H{o}s-R\'enyi)
binomial random graph model $\rG np$ has
\[
\Pr\left(G\right)=p^{\left|E\left(G\right)\right|}\left(1-p\right)^{{n \choose 2}-\left|E\left(G\right)\right|}
\]
for each graph $G$ on the vertex set $\range n$. When $p=1/2$,
we obtain the uniform model on all graphs on the vertex set $\range n$.

One way to conceptualize the binomial model is to consider a sequence
of independent coin tosses, where the coin is biased to land heads
with probability $p$. Each coin toss corresponds to a particular
potential edge, and determines whether that edge is present in the
final random graph. When we define more complicated random models,
we will often use this kind of informal description rather than giving
an explicit formula for each $\Pr\left(\om\right)$.

As another example, the uniform model $\rG nM$ can be alternatively
defined recursively: $\rG n0$ is always the trivial graph with no
edges, and for each $M>0$, to obtain $\rG nM$ we choose $G\in\rG n{M-1}$
and add one of the ${n \choose 2}-\left(M-1\right)$ possible edges
at random.

\begin{note}

This section is unfinished, I'll probably want random matrices and
maybe the pairing model on random regular graphs

\end{note}


\section{Stein's Method in Generality}

\begin{note}There are a few quite different presentations of Stein's
method. One thing I'm trying to do here is to unify Stein's functional
analysis approach for exchangeable pairs \cite{Ste86} with Ross'
general presentation\cite{Ros11}.

The reason I want to look at Stein's original, more abstract presentation
is that I think it does a better job motivating why things work. Before
I read that, the steps taken to apply Stein's method seemed like blindly
doing things and it turns out they work.\end{note}

\begin{todo}

Maybe go through a bare-hands proof of Berry Esseen throughout this
section

\end{todo}

\global\long\def\Lo{\L 0}


\global\long\def\Eo{\E_{0}}


\global\long\def\cF{\mathcal{F}}


\global\long\def\Fo{\cF_{0}}


\global\long\def\cY{\mathcal{Y}}


\global\long\def\fo{f}


\global\long\def\T{T}


\global\long\def\To{\T_{0}}


\global\long\def\U{U}


\global\long\def\Uo{\U_{0}}


\global\long\def\cX{\mathcal{X}}


\global\long\def\Xo{\cX_{0}}


Suppose we have a potentially complicated random variable $\X$, and
we believe the distribution of $\X$ is close to a ``standard''
distribution $\Lo$. Then, Stein's method allows us to compare the
operators $\E_{\X}$ and $\Eo:=\E_{\Lo}$. This is sometimes directly
useful for approximating statistics of $\X$ (for example, $\Pr\left(\X\in\A\right)=\E_{\X}\one_{\A}$).
However, particularly for combinatorial applications, Stein's method
is most often used to bound the distance $d_{\cH}\left(\L{\X},\Lo\right)$
for some $\cH$, where the metric $d_{\cH}$ from \ref{def:general-metrics}
is defined in terms of $\E_{\X}$ and $\Eo$.

Stein's method is motivated by the idea of a characterizing operator.
\begin{defn}
Let $\Fo$ be a vector space and $\Xo$ be a vector space of measurable
functions which contains the constant functions. We say a linear operator
$\To:\Fo\to\Xo$ is a \emph{characterizing operator} for the distribution
$\Lo$ if $\im\To=\Xo\cap\ker\Eo$. For convenience, where there is
no ambiguity we will often implicitly restrict $\Eo$ to $\Xo$, so
we can write $\im\To=\ker\Eo$.
\end{defn}
The following proposition shows why $\To$ is called a characterizing
operator.
\begin{prop}
\label{prop:characterizing}If $\To:\Fo\to\Xo$ is a characterizing
operator and $\Xo$ is a determining class then $\im\To\subseteq\ker\E_{\X}$
implies $\L{\X}=\Lo$.\end{prop}
\begin{proof}
If $\h\in\Xo$, then $\h-\Eo\h\in\ker\Eo=\im\To$ so $\E_{\X}\left[\h-\Eo\h\right]=0$.
That is, $\E_{\X}\h=\Eo\h$ for all $\h\in\Xo$, which means $\L{\X}=\Lo$
by the definition of a determining class.\end{proof}
\begin{prop}
\label{prop:U_0-characterizing}$\To:\Fo\to\Xo$ is characterizing
if and only if there is a linear operator $\Uo:\Xo\to\Fo$ (called
a \emph{Stein transform}) such that the following two equations hold.
\begin{align}
\Eo\To & =0_{\Fo},\label{eq:E_0T_0-consistency}\\
\To\Uo+\Eo & =\id_{\Xo}.\label{eq:U_0-characterizing}
\end{align}
\end{prop}
\begin{proof}
Suppose $\To$ is a characterizing operator. Equation \ref{eq:E_0T_0-consistency}
is immediate. Let $\left\{ \h_{\i}\right\} _{\i\in\mathcal{I}}$ be
a (Hamel) basis of $\Xo$. For each $\i\in\mathcal{I}$ we have $\h_{\i}-\Eo\h_{\i}\in\ker\Eo$
so there is some $\fo_{\i}$ (not necessarily unique) that solves
$\To\fo_{\i}=\h_{\i}-\Eo\h_{\i}$. The operator $\Uo$ can then be
defined by $\sum_{\i\in\mathcal{I}}a_{\i}\h_{\i}\mapsto\sum_{\i\in\mathcal{I}}a_{\i}\fo_{\i}$,
satisfying \ref{eq:U_0-characterizing}.

Conversely, suppose \ref{eq:E_0T_0-consistency} holds and $\Uo$
exists satisfying \ref{eq:U_0-characterizing}. For $\h\in\ker\Eo$
we have $\To\left(\Uo\h\right)=\h$ and hence $\h\in\im\To$, so $\ker\Eo\subseteq\im\To$.
Equation \ref{eq:E_0T_0-consistency} immediately says that $\im\To\subseteq\ker\Eo$,
so $\To$ is a characterizing operator.\end{proof}
\begin{rem}
In full generality (and in accordance with \cite{Ste86}), we do not
require any topological structure on $\Fo$ and $\Xo$. The proof
of \ref{prop:U_0-characterizing} uses the axiom of choice and does
not ensure that the Stein transform $\Uo$ is particularly well-behaved.
In practice, we will usually require that $\Uo$ is well-behaved to
apply Stein's method (in fact, we will usually have an explicit formula
for $\Uo$).
\end{rem}
\begin{note}

I managed to prove a necessary and sufficient condition for $\Uo$
to be bounded if $\To$ is a Banach space operator (namely, $\ker\To$
is complementable). I think it's probably a little off-topic to include
this though.

\end{note}

\begin{comment}
\begin{rem}
but an equivalent condition for existence of a bounded Stein transform
is that the kernel of $\To$ is complementable. That is, there is
a closed subspace $S\subseteq\Fo$ with $S\oplus\ker\To=\Fo$. For
example, this condition will always be satisfied if $\Fo$ is (isomorphic
to a) Hilbert space, by the existence of orthogonal complements. In
fact, Hilbert spaces are the only spaces where every closed subspace
is complementable \cite{LT71}.\end{rem}
\begin{proof}
[Proof of claim]First assume $\ker\To$ is complementable, with complement
$S$. Now, $\ker\Eo=\im\To$ is a closed (Banach) subspace of $\Xo$,
so $\To$ restricts to a linear operator $\T':S\to\im\To$ between
Banach spaces. By construction, $\T'$ is bijective so by the bounded
inverse theorem, $\T'$ has a bounded inverse $\U':\im\To\to S$.
This gives a bounded Stein transform
\[
\Uo:\Xo\to\Fo:\h\mapsto\U'\left(\h-\Eo\h\right).
\]
Conversely, if $\Uo$ is a bounded Stein transform for $\To$, let
$J=\id_{\Fo}-\Uo\To$. If $\fo\in\ker J\cap\ker\To$ then $\To\fo=0$
so $J\fo=\fo-\Uo0=\fo=0$. That is, $\ker J\cap\ker\To=\left\{ 0\right\} $.
For any $\fo\in\Fo$, we have $\fo=J\fo+\Uo\To\fo$. Now, 
\[
J\Uo\To=\Uo\To-\Uo\left(\id_{\Xo}-\Eo\right)\To=\Uo\To-\Uo\To+\Uo\Eo\To=0
\]
and 
\[
\To J=\To-\left(\id_{\Xo}-\Eo\right)\To=\To-\To+\Eo\To=0,
\]
so $J\fo\in\ker\To$ and $\Uo\To\fo\in\ker J$. That is, $\ker\To\oplus\ker J=\Fo$
and we conclude that $\ker\To$ is complementable.
\end{proof}
\end{comment}

We'll use \ref{prop:U_0-characterizing} to give two important examples
of characterizing operators.
\begin{thm}
\label{thm:normal-characterizing}Let $\X_{\N}\in\N\left(0,1\right)$
and let $\cX_{\N}=\left\{ \h\in C^{\infty}\left(\RR\right):\E\left[\left|\X_{\N}\right|^{k}\left|\h\left(\X_{\N}\right)\right|\right]<\infty\mbox{ for all }k\in\NN\right\} $.
Let $\cF_{\N}$ be the set of continuously differentiable $\fo$ with
$\fo'\in\cX_{\N}$.

The operator $\T_{\N}:\cF_{\N}\to\cX_{\N}$ given by $\T_{\N}\fo\left(x\right)=\fo'\left(x\right)-x\fo\left(x\right)$
is a characterizing operator for $\N\left(0,1\right)$.\end{thm}
\begin{proof}
(Adapted from \cite[Lecture II]{Ste86}). First we prove that $\T_{\N}$
is well-defined as an operator with codomain $\cX_{\N}$. Fix $\fo\in\cF_{\N}$.
We have
\[
\E\left[\left|\X_{\N}\right|^{k}\left|\T_{\N}\fo\left(\X_{\N}\right)\right|\right]\le\E\left[\left|\X_{\N}\right|^{k}\left|\fo'\left(\X_{\N}\right)\right|\right]+\E\left[\left|\X_{\N}\right|^{k+1}\left|\fo\left(\X_{\N}\right)\right|\right].
\]
\begin{comment}Now, $\E\left|\X\right|^{a}\le\left(\E\left|\X\right|^{b}\right)^{a/b}$
for $a\le b$ by Hlder's inequality (see \cite[Theorem 3.6 (1)]{Rud66})
\begin{alignat*}{1}
\E\left|\X_{\N}\right|^{k+1} & \prec\int_{0}^{\infty}x^{k+1}e^{-x^{2}/2}\d x\\
 & \prec\int_{0}^{\infty}x^{k}\frac{\d}{\d x}e^{-x^{2}/2}\d x\\
 & \prec\left.x^{k}e^{-x^{2}/2}\right|_{0}^{\infty}-\int_{0}^{\infty}x^{k-1}e^{-x^{2}/2}\d x\\
 & \prec\int_{0}^{\infty}x^{k-1}e^{-x^{2}/2}\d x
\end{alignat*}
Proceeding inductively, we have $\E\left|\X_{\N}\right|^{k+1}\prec\int_{0}^{\infty}xe^{-x^{2}/2}\d x\prec\left.e^{-x^{2}/2}\right|_{0}^{\infty}<\infty$
if $k$ is even, or $\E\left|\X_{\N}\right|^{k+1}\prec\int_{0}^{\infty}e^{-x^{2}/2}\d x<\infty$
if $k$ is odd\end{comment}

Now, $\E\left|\X_{\N}\right|^{k+1}<\infty$ (this is a standard result
that is easily proved inductively, using integration by parts and
the Gaussian integral). So,
\begin{alignat*}{1}
\E\left[\left|\X_{\N}\right|^{k+1}\left|\fo\left(\X_{\N}\right)\right|\right] & \prec\int_{-\infty}^{\infty}\left|x\right|^{k+1}\left|\fo\left(x\right)-\fo\left(0\right)\right|e^{-x^{2}/2}\d x\\
 & =\int_{-\infty}^{\infty}x^{k+1}\left|\fo\left(0\right)+\int_{0}^{x}\fo'\left(t\right)\d t\right|e^{-x^{2}/2}\d x\\
 & \le\E\left|\X_{\N}\right|^{k+1}\left(\fo\left(0\right)+\int_{-\infty}^{\infty}\left|\fo'\left(t\right)\right|\d t\right)<\infty.
\end{alignat*}
It follows that $\E\left[\left|\X_{\N}\right|^{k}\left|\T_{\N}\fo\left(\X_{\N}\right)\right|\right]<\infty$
and $\T_{\N}\fo\in\cX_{\N}$.

For any $\fo\in\cF_{\N}$, integration by parts gives
\[
\E_{\N}\T_{\N}\fo=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-t^{2}/2}\fo'\left(t\right)\d t-\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}te^{-t^{2}/2}\fo\left(t\right)\d t=0
\]
so $\E_{\N}\T_{\N}=0$ and \ref{eq:E_0T_0-consistency} holds. Then,
define $\U_{\N}:\cX_{\N}\to\cF_{\N}$ by 
\[
\U_{\N}\h\left(x\right)=e^{x^{2}/2}\int_{-\infty}^{x}\left(\h\left(t\right)-\E_{\N}\h\right)e^{-t^{2}/2}\d t=-e^{x^{2}/2}\int_{x}^{\infty}\left(\h\left(t\right)-\E_{\N}\h\right)e^{-t^{2}/2}\d t.
\]
(These two definitions are the same because their difference is $e^{x^{2}/2}\left(\E_{\N}\h-\E_{\N}\h\right)=0$).

A straightforward calculation involving the product rule and the fundamental
theorem of calculus yields
\begin{align*}
\T_{\N}\U_{\N}\h\left(x\right) & =\h\left(x\right)-\E_{\N}\h.
\end{align*}
(Indeed, $\U_{\N}$ could have been obtained by solving this differential
equation). So, \ref{eq:U_0-characterizing} holds. To apply \ref{prop:U_0-characterizing},
it remains to prove that $\U_{\N}$ is indeed well-defined as an operator
with codomain $\cF_{\N}$. For $\h\in\cX_{\N}$,
\[
\E\left[\left|\X_{\N}\right|^{k}\left|\left(\U_{\N}\h\right)'\left(\X_{\N}\right)\right|\right]\le\E\left[\left|\X_{\N}\right|^{k+1}\left|\U_{\N}\h\left(\X_{\N}\right)\right|\right]+\E\left[\left|\X_{\N}\right|^{k}\left|\h\left(x\right)\right|\right]+\E\left|\X_{\N}\right|^{k}\E_{\N}\h.
\]
By Fubini's theorem (see \cite[Theorem 8.8]{Rud66}),
\begin{alignat*}{1}
\E\left[\left|\X_{\N}\right|^{k+1}\left|\U_{\N}\h\left(\X_{\N}\right)\right|\right] & \prec\int_{0}^{\infty}x^{k+1}\int_{x}^{\infty}\left|\h\left(t\right)-\E_{\N}\h\right|e^{-t^{2}/2}\d t\d x\\
 & \qquad-\int_{-\infty}^{0}x^{k+1}\int_{\infty}^{x}\left|\h\left(t\right)-\E_{\N}\h\right|e^{-t^{2}/2}\d t\d x\\
 & =\int_{0}^{\infty}\left|\h\left(t\right)-\E_{\N}\h\right|e^{-t^{2}/2}\int_{0}^{t}x^{k+1}\d x\d t\\
 & \qquad-\int_{-\infty}^{0}x^{k+1}\int_{t}^{0}\left|\h\left(t\right)-\E_{\N}\h\right|e^{-t^{2}/2}\d t\d x\\
 & \prec\int_{-\infty}^{\infty}\left|t\right|^{k+2}\left|\h\left(t\right)-\E_{\N}\h\right|e^{-t^{2}/2}\d x\d t<\infty,
\end{alignat*}
so $\U_{\N}\h\in\cF_{\N}$.
\end{proof}

\begin{thm}
\label{thm:poisson-characterizing}Let $\X_{\Po\left(\lambda\right)}\in\Po\left(\lambda\right)$
and let $\cX_{\N}=\cF_{\N}=\left\{ \h:\NN\to\RR:\E\left[\X_{\Po\left(\lambda\right)}^{k}\left|\h\left(\X_{\Po\left(\lambda\right)}\right)\right|\right]<\infty\mbox{ for all }k\in\NN\right\} $.

The operator $\T_{\Po\left(\lambda\right)}:\cF_{\N}\to\cX_{\N}$ defined
by $\T_{\Po\left(\lambda\right)}\fo\left(k\right)=\lambda\fo\left(k+1\right)-k\fo\left(k\right)$
is a characterizing operator for $\Po\left(\lambda\right)$.
\end{thm}
\begin{todo}This proof is unfinished; I need to prove that $\T_{\Po\left(\lambda\right)}$
and $\U_{\Po\left(\lambda\right)}$ have appropriate codomain. See
\cite{Ste92}, page 18.\end{todo}
\begin{proof}
For any $f\in\cF_{\Po\left(\lambda\right)}$, we have
\[
\E_{\Po\left(\lambda\right)}\T_{\Po\left(\lambda\right)}\fo=e^{-\lambda}\sum_{\i=0}^{\infty}\frac{\lambda^{\i+1}}{\i!}\fo\left(\i+1\right)-e^{-\lambda}\sum_{\i=1}^{\infty}\frac{\lambda^{\i}}{\left(\i-1\right)!}\fo\left(\i\right)=0
\]
so $\E_{\Po\left(\lambda\right)}\T_{\Po\left(\lambda\right)}=0$ and
\ref{eq:E_0T_0-consistency} holds. Then, define $\U_{\Po\left(\lambda\right)}$
by 
\[
\U_{\Po\left(\lambda\right)}\h\left(k\right)=\frac{\left(k-1\right)!}{\lambda^{k}}\sum_{\i=0}^{k-1}\frac{\lambda^{\i}}{\i!}\left(\h\left(\i\right)-\E_{\Po\left(\lambda\right)}\h\right)
\]
for $k\ge1$. Substituting and simplifying gives
\[
\T_{\Po\left(\lambda\right)}\U_{\Po\left(\lambda\right)}\h\left(k\right)=\h\left(k\right)-\E_{\Po\left(\lambda\right)}\h,
\]
so \ref{eq:U_0-characterizing} holds and \ref{prop:U_0-characterizing}
completes the proof.
\end{proof}
Note that $\cH_{\TV}\subseteq\cX_{\N}$, where $\cH_{\TV}$ is as
defined in \ref{def:special-metrics}. Since $\cH_{\TV}$ is a determining
class, $\T_{\N}$ is a characterizing operator in the sense of \ref{prop:characterizing}.
We can say the same about $\T_{\Po\left(\lambda\right)}$ if we restrict
our attention to integer-valued random variables.

The utility of the introduction of a characterizing operator is that
for each $\h\in\Xo$, Equation \ref{eq:U_0-characterizing} allows
us to to make the transformation
\begin{equation}
\E_{\X}\h=\Eo\h+\E_{\X}\To\Uo\h.\label{eq:stein-transformation}
\end{equation}
The original purpose of Stein's method was to estimate some particular
$\E_{\X}\h$. If $\Lo$ was chosen to be a ``simple'', well-understood
distribution then the term $\Eo\h$ should be easy to compute or estimate,
and if the distribution of $\X$ was ``close'' to $\Lo$, then it
should be possible to show that the remainder $\E_{\X}\To\Uo\h$ is
small.

For our purposes, the main use of \ref{eq:stein-transformation} is
to bound $d_{\cH}\left(\X,\Lo\right)$ for some $\cH\subseteq\Xo$.
For any $\cY\supseteq\Uo\cH$, we have 
\[
d_{\cH}\left(\X,\Lo\right)=\sup_{\h\in\cH}\left|\E_{\X}\To\Uo\h\right|\le\sup_{\fo\in\cY}\left|\E_{\X}\To\fo\right|.
\]
We have reduced the problem of bounding $d_{\cH}\left(\X,\Lo\right)$
to that of bounding $\left|\E_{\X}\To\fo\right|$ (uniformly over
$\fo\in\cY$). Especially in the cases where $\Lo$ is normal or Poisson
and $\cH$ is one of the standard choices in \ref{def:special-metrics},
there are a number of known convenient choices of $\cY$, and a number
of methods that are known to be effective to bound $\left|\E_{\X}\To\fo\right|$.
\begin{example}
\label{example:poisson-bound}If $\cH=\cH_{\TV}$ and $\Lo=\Po\left(\lambda\right)$,
using the characterizing operator in \ref{thm:poisson-characterizing},
then we can choose 
\[
\cY=\left\{ \fo\in\Fo:\left\Vert \fo\right\Vert _{\infty}\le\min\left\{ 1,\lambda^{-1/2}\right\} ,\,\left\Vert \Delta\fo\right\Vert _{\infty}\le\min\left\{ 1,\lambda^{-1}\right\} \right\} ,
\]
where $\Delta\fo\left(k\right)=\fo\left(k+1\right)-\fo\left(k\right)$.
\end{example}
\begin{todo}

Maybe I can bound $\left\Vert \U_{\Po\left(\lambda\right)}\right\Vert $
instead?

\end{todo}

Proving that this choice of $\cY$ satisfies $\cY\supseteq\U_{\Po\left(\lambda\right)}\cH$
is nontrivial. But, we can prove that the constraints are of the ``correct''
order of magnitude.

\begin{todo}\label{todo:poisson-norm-bound}

The proof is in \cite[Remark 10.2.4]{BHJ92}. There's also a simpler
proof in \cite[Lemma 1.1.1]{BHJ92} that $\left\Vert \fo\right\Vert _{\infty}\le2\min\left\{ 1,\lambda^{-1/2}\right\} $
suffices. I'll revisit this later.

\end{todo}
\begin{proof}
[Proof that this $\cY$ satisfies $\cY\supseteq\U_{\Po\left(\lambda\right)}\cH$](Adapted
from \cite[Lemma 1.1.1]{BHJ92}). For any Borel $\A$ and any $k\in\NN$,
\begin{align*}
\U_{\Po\left(\lambda\right)}\one_{\A}\left(k\right) & =\frac{e^{\lambda}\,\left(k-1\right)!}{\lambda^{k}}\left(\Po_{\lambda}\left(\A\cap\range{k-1}\right)-\Po_{\lambda}\left(\A\right)\Po_{\lambda}\left(\range{k-1}\right)\right)\\
 & =\frac{e^{\lambda}\,\left(k-1\right)!}{\lambda^{k}}\left(\vphantom{\int}\Po_{\lambda}\left(\A\cap\range{k-1}\right)\left(\vphantom{\sum}1-\Po_{\lambda}\left(\range{k-1}\right)\right)\right.\\
 & \qquad\qquad\qquad\quad-\left.\vphantom{\int}\left(\vphantom{\sum}\Po_{\lambda}\left(\A\right)-\Po_{\lambda}\left(\A\cap\range{k-1}\right)\right)\Po_{\lambda}\left(\range{k-1}\right)\right)\\
 & =\frac{e^{\lambda}\,\left(k-1\right)!}{\lambda^{k}}\left(\vphantom{\sum}\Po_{\lambda}\left(\A\cap\range{k-1}\right)\Po_{\lambda}\left(\RR\backslash\range{k-1}\right)-\Po_{\lambda}\left(\A\backslash\range{k-1}\right)\Po_{\lambda}\left(\range{k-1}\right)\right).
\end{align*}
Note that $\Po_{\lambda}\left(\A\cap\range{k-1}\right)$ is bounded
above by $\Po_{\lambda}\left(\range{k-1}\right)$ and $\Po_{\lambda}\left(\A\backslash\range{k-1}\right)$
is bounded above by $\Po_{\lambda}\left(\RR\backslash\range{k-1}\right)$,
so
\[
\left|\U_{\Po\left(\lambda\right)}\one_{\A}\left(k\right)\right|\le\frac{e^{\lambda}\,\left(k-1\right)!}{\lambda^{k}}\Po_{\lambda}\left(\range{k-1}\right)\Po_{\lambda}\left(\RR\backslash\range{k-1}\right).
\]
Note that we have equality when $\A=\range{k-1}$. If $k\asymp\lambda$
as $\lambda\to\infty$ then Stirling's approximation (unfinished...)\end{proof}
\begin{example}
\label{example:wasserstein-bound}If $\cH=\cH_{\W}$ or $\cH=\cH_{\BL}$
and $\Lo=\N\left(0,1\right)$, using the characterizing operator in
\ref{thm:poisson-characterizing}, then we can choose 
\[
\cY=\left\{ \fo\in\Fo:\left\Vert \fo\right\Vert _{\infty}\le2,\,\left\Vert \fo'\right\Vert _{\infty}\le\sqrt{2/\pi},\,\left\Vert \fo''\right\Vert _{\infty}\le4\right\} .
\]
\end{example}
\begin{proof}
[Proof that this $\cY$ satisfies $\cY\supseteq\U_{\N}\cH$]to do
\end{proof}

\subsection{The Berry-Esseen Theorem}

Before we proceed further, we give a proof of a quantitative central
limit theorem using Stein's method bare-handed.

\begin{todo}

give some background; the theorem was apparently originally proved
before Stein but the proof was complicated.

\end{todo}
\begin{thm}
[Berry-Esseen]Let $Q_{1},\dots,Q_{n}$ be independent random variables
with common distribution $Q$, such that $\E Q=0$, $\E Q^{2}=1$
and $\E\left|Q\right|^{3}<\infty$. Let 
\[
\X=\frac{1}{\sqrt{n}}\sum_{\i=1}^{n}Q_{i}.
\]
Then 
\[
d_{\W}\left(n^{-1/2}\X,\N\left(0,1\right)\right)\le\frac{3}{\sqrt{n}}\E\left(\left|Q\right|^{3}\right).
\]
\end{thm}
\begin{proof}
Fix $f\in\cY$.

We need to compare 
\[
\E\left[\X f\left(\X\right)\right]=\E\left[\frac{1}{\sqrt{n}}\sum_{\i=1}^{n}Q_{\i}f\left(\X\right)\right]
\]
with $\E f'\left(\X\right)$. Define $W_{\i}=\X-\frac{1}{\sqrt{n}}Q_{\i}$,
so $W_{\i}$ and $Q_{\i}$ are independent. For each $\i$, we then
have 
\[
\E\left[Q_{\i}f\left(\X\right)\right]=\E\left[Q_{\i}\left(f\left(\X\right)-f\left(W_{\i}\right)\right)\right]=\frac{1}{\sqrt{n}}\E\left[Q_{\i}^{2}f'\left(W_{\i}\right)\right]+\frac{1}{n}\E\left[Q_{\i}^{3}\frac{f''\left(E\right)}{2}\right]
\]
for some (random) $E$, by Taylor's theorem with the Lagrange form
of the remainder (noting $\X-W_{\i}=Q_{\i}$).

Now, note that
\[
\E\left[Q_{\i}^{2}f'\left(W_{\i}\right)\right]=\E Q_{\i}^{2}\E f'\left(W_{\i}\right)=\E f'\left(W_{\i}\right)=\E f'\left(\X\right)+\E\left[Q_{\i}f''\left(E'\right)\right]
\]
for some $E'$, again using the Lagrange form of the remainder. Now,
by the bound on $\left\Vert f''\right\Vert _{\infty}$ in the definition
of $\cY$, we have 
\begin{alignat*}{1}
\E\left|Q_{\i}^{3}\frac{f''\left(E\right)}{2}\right| & \le\E\left|Q^{3}\right|,\\
\E\left|Q_{\i}f''\left(E'\right)\right| & \le2\E\left|Q\right|
\end{alignat*}
Hlder's inequality (see \cite[Theorem 3.6 (1)]{Rud66}) for the $L^{3/2}$
norm gives 
\[
1=\E\left|Q^{2}\right|^{3/2}\le\left(\E\left|\left(Q^{2}\right)^{3/2}\right|^{2/3}\right)^{3/2}=\E\left|Q^{3}\right|,
\]
so another application of Hlder's inequality for the $L^{3}$ norm
gives
\[
\E\left|Q\right|\le\E\left|Q^{3}\right|^{1/3}\le\E\left|Q^{3}\right|.
\]
We conclude
\begin{alignat*}{1}
\left|\E\left[\X f\left(\X\right)\right]-\E f'\left(\X\right)\right| & \le\left|\frac{1}{\sqrt{n}}\sum_{\i=1}^{n}\left(\E\left[Q_{\i}f\left(\X\right)\right]-\frac{1}{\sqrt{n}}\E\left[Q_{\i}^{2}f'\left(W_{\i}\right)\right]\right)\right|\\
 & \qquad+\left|\frac{1}{n}\sum_{\i=1}^{n}\left(\E\left[Q_{\i}^{2}f'\left(W_{\i}\right)\right]-\E f'\left(\X\right)\right)\right|\\
 & \le\frac{1}{\sqrt{n}}\E\left|Q^{3}\right|+\frac{2}{\sqrt{n}}\E\left|Q^{3}\right|
\end{alignat*}

\end{proof}

\section{The method of exchangeable pairs}

\global\long\def\fX{f}


\global\long\def\hX{\h}


\global\long\def\XX{\cX_{\X}}


\global\long\def\eX{\mathbf{X}}


\global\long\def\FX{\cF_{\X}}


\global\long\def\TX{\T_{\eX}}


\global\long\def\OmX{\Om_{\X}}


\global\long\def\OmeX{\Om_{\eX}^{\left(2\right)}}


\global\long\def\conn{\alpha}


\global\long\def\x#1{x^{\left(#1\right)}}


This is Stein's original approach, and is effective in wide generality
for discrete random variables. In what follows, we assume $\X$ is
discrete. Let $\OmX$ be the support of $\X$.
\begin{example}
[adapted from {\cite[Example 4.21]{Ros11}}]\label{example:fixed-points}Throughout
this section, we will refer to an example problem to illustrate the
principles of Stein's method for exchangeable pairs.

We will say a \emph{fixed point} of a permutation $\sigma\in S_{n}$
is an index $k\in\range n$ that satisfies $\sigma\left(k\right)=k$.
Let $\X:S_{n}\to\left\{ 0\right\} \cup\range n$ give the number of
fixed points in each permutation from $S_{n}$. We interpret $\X$
as a random variable on the underlying space $\rS n$.

Now, if $n$ is large then fixed points are largely independent of
each other, and each of $n$ indices has a probability of $1/n$ to
be a fixed point. So (recalling \ref{rem:indicator-expectation}),
we might expect $\L{\X}$ to be ``close'' to $\Po\left(1\right)$.
We will attempt to bound $d_{\TV}\left(\X,\Po\left(1\right)\right)$
to validate and quantify this intuition.
\end{example}
The general idea is that we can use an object $\eX$ called an exchangeable
pair to construct a characterizing operator $\TX$ for $\X$. We then
use an operator $\conn$ to connect the domains of $\TX$ and $\To$
in such a way that $\TX\conn$ approximates $\To$. We then have $\E_{\X}\To=\E_{\X}\left(\To-\TX\conn\right)$,
so we can use the fact that $\To-\TX\conn$ is small to bound $\E_{\X}\To\fo$.
\begin{defn}
A 2-dimensional random pair $\eX=\left(\X_{1},\X_{2}\right)$ is an
\emph{exchangeable pair} if $\Lb{\X_{1},\X_{2}}=\Lb{\X_{2},\X_{1}}$.
We will denote the support of $\eX$ by $\OmeX$.
\end{defn}
That is, a pair $\eX$ is exchangeable if exchanging the components
of the pair does not change their joint distribution. In particular,
the marginal distributions of $\X_{1}$ and $\X_{2}$ must be the
same.

\begin{note}

All presentations of Stein's method I've seen use the notation $\left(\X,\X'\right)$
but I think that has the potential to be confusing because the $\X$
in that pair is defined on $\Om^{2}$ whereas the original random
variable $\X$ is defined on $\Om$.

\end{note}
\begin{prop}
There is a natural equivalence between time-homogeneous reversible
Markov chains with steady-state distribution $\L{\X}$, and exchangeable
pairs with margins $\L{\X}$.\end{prop}
\begin{proof}
Given an exchangeable pair $\eX$ with margins $\L{\X}$, we can define
a time-homogenous Markov chain $M$ with transition probabilities
$p\left(x_{1},x_{2}\right)=\Pr\left(\X_{2}=x_{2}|\X_{1}=x_{1}\right)$.
With $\pi\left(x\right)=\Pr\left(\X=x\right)$, we then have
\[
\pi\left(x_{1}\right)p\left(x_{1},x_{2}\right)=\Pr\left(\eX=\left(x_{1},x_{2}\right)\right)=\pi\left(x_{2}\right)p\left(x_{2},x_{1}\right)
\]
for any $x_{1},x_{2}\in\OmX$. So, $M$ is reversible with steady-state
distribution $\L{\X}$.

Conversely, suppose we have a time-homogeneous reversible Markov chain
with steady-state distribution $\L{\X}$. Let the transition probability
between $x$ and $x'$ be $p\left(x,x'\right)$ and let the probability
of state $x$ in the steady-state distribution be $\pi\left(x\right)$.
We can then define an exchangeable pair $\eX$ by 
\[
\Pr\left(\eX=\left(x_{1},x_{2}\right)\right)=\pi\left(x_{1}\right)p\left(x_{1},x_{2}\right)=\pi\left(x_{2}\right)p\left(x_{2},x_{1}\right)=\Pr\left(\eX=\left(x_{2},x_{1}\right)\right)
\]
and the proposition is proved.\end{proof}
\begin{defn}
We say an exchangeable pair $\eX$ is \emph{connected} if the corresponding
Markov chain is irreducible.\end{defn}
\begin{rem}
\label{rem:underlying-pair}We are particularly interested in exchangeable
pairs $\eX$ with marginal distributions $\L{\X_{1}}=\L{\X_{2}}=\L{\X}$.
If $\X$ is defined on an underlying combinatorial probability space
$\left(\Om,2^{\Om},\Pr\right)$, it is often convenient to first construct
an exchangeable pair $\mathbf{W}=\left(W_{1},W_{2}\right)$ with margins
$\Pr$, so that the vector $\eX_{\mathbf{W}}=\left(\X\left(W_{1}\right),\X\left(W_{2}\right)\right)$
is an exchangeable pair with margins $\L{\X}$. If $\left(W_{1},W_{2}\right)$
is connected, then $\eX_{\mathbf{W}}$ is connected also.\end{rem}
\begin{example}
\label{example:fixed-points-exchangeable-pair}We continue \ref{example:fixed-points}.
We will define a specific exchangeable pair $\mathbf{W}=\left(W_{1},W_{2}\right)$
with margins $\rS n$ by 
\[
\Pr\left(\left(W_{1},W_{2}\right)=\left(\sigma_{1},\sigma_{2}\right)\right)=\begin{cases}
\left(n!{n \choose 2}\right)^{-1} & \mbox{if }\sigma_{1}=\sigma_{2}\left(\i\,\ii\right)\mbox{ for some transposition \ensuremath{\left(\i\,\ii\right)}}\\
0 & \mbox{otherwise}.
\end{cases}
\]
The relation of differing by a transposition is symmetric, so $\mathbf{W}$
is indeed an exchangeable pair. The Markov chain associated with $\mathbf{W}$
has a simple interpretation. Given a random permutation $\sigma$,
to make a transition in the Markov chain we just randomly choose one
of the ${n \choose 2}$ possible transpositions and compose it with
$\sigma$. Because the transpositions generate $S_{n}$, the pair
$\mathbf{W}$ is connected, so we can use the construction from \ref{rem:underlying-pair}
to produce a connected exchangeable pair $\eX$ with margins $X$.
\end{example}
The Markov chain underlying a connected exchangeable pair can be naturally
viewed as a connected one-dimensional simplicial complex. The zeroth
reduced homology group $\ker\partial_{0}/\im\partial_{1}$ of a connected
simplicial complex has dimension zero, and this motivates the construction
of a characterizing operator in a natural way. (The following theorem
is self-contained and requires no knowledge of homology theory).
\begin{thm}
Suppose\textup{ $\eX$ }is a connected exchangeable pair with margins
$\L{\X}$. Let $\FX\subseteq L^{1}\left(\OmX^{2},\L{\eX}\right)$
be the set of functions $\fX:\OmX^{2}\to\RR$ which satisfy $\E\left|\fX\left(\eX\right)\right|<\infty$,
and are antisymmetric in the sense that $\fX\left(x_{1},x_{2}\right)=-\fX\left(x_{2},x_{1}\right)$.
Let $\XX=L^{1}\left(\OmX,\L{\X}\right)$ be the set of functions $\hX:\OmX\to\RR$
that satisfy $\E_{\X}\left|\hX\right|<\infty$.

Define $\TX:\FX\to\XX$ by $\TX\fX\left(x\right)=\sum_{x_{2}\in\OmX}\fX\left(x,x_{2}\right)p\left(x,x_{2}\right)=\E\left[\fX\left(\eX\right)|\X_{1}=x\right]$,
so that $\TX\fX\left(\X\right)$ is distributed as $\E^{\X_{1}}\fX\left(\eX\right)$.
Then $\TX$ is a characterizing operator for $\X$.\end{thm}
\begin{proof}
To see that $\im\TX\subseteq\ker\E_{\X}$, fix $\fX\in\cF$ and note
that by the tower law of expectation (\ref{prop:tower-law}), 
\[
\E_{\X}\TX\fX=\E\E^{\X_{1}}\fX\left(\eX\right)=\E\fX\left(\eX\right).
\]
By exchangeability and antisymmetry, $\E\fX\left(\eX\right)=\E\fX\left(\X_{2},\X_{1}\right)=-\E\fX\left(\eX\right)$,
so $\E\fX\left(\X_{1},\X_{2}\right)=\E_{\X}\TX\fX=0$. This did not
require the connectedness condition. We can similarly prove that $\TX$
is well-defined as an operator from $\FX$ to $\XX$: note that $\E_{\X}\left|\TX\fX\right|=\E\left|\fX\left(\eX\right)\right|$
so $\TX\FX\subseteq\XX$.

We will next prove $\ker\E_{\X}\subseteq\im\TX$, but first we make
some definitions. For each $x\in\OmX$, let $\hX_{x}$ be the function
that takes the value $\pi\left(x\right)^{-1}$ on $x$ and is zero
elsewhere, so that $\hX=\sum_{x\in\OmX}\hX\left(x\right)\pi\left(x\right)\hX_{x}$
for each $\hX\in\XX$. For each $\left(x_{1},x_{2}\right)\in\OmeX$,
define $\fX_{x_{1},x_{2}}\in\FX$ as the function that takes the value
${\displaystyle \left(\pi\left(x_{1}\right)p\left(x_{1},x_{2}\right)\right)^{-1}}$
on $\left(x_{1},x_{2}\right)$, takes the value $-\left(\pi\left(x_{2}\right)p\left(x_{2},x_{1}\right)\right)^{-1}$
on $\left(x_{2},x_{1}\right)$, and takes the value zero elsewhere.
Note that this function is antisymmetric by the reversibility of the
Markov chain of $\eX$. We have $\TX\fX_{x_{1},x_{2}}=\hX_{x_{2}}-\hX_{x_{1}}$.

Let $\hX\in\ker\E_{\X}$, and fix an arbitrary $x^{*}\in\OmX$. By
the connectedness assumption, for each $x\in\OmX$ there is a sequence
\[
x=\x 0,\x 1,\dots,\x{k-1},\x k=x^{*}
\]
 with $\left(\x{\i-1},\x{\i}\right)\in\OmeX$ for $\i=1,\dots k$.
Note that 
\[
\hX_{x^{*}}-\hX_{x}=\TX\sum_{\i=1}^{k}\fX_{\x{\i-1},\x{\i}}=:\TX\fX_{x}^{*},
\]
and it follows that
\[
\hX=\sum_{x\in\OmX}\hX\left(x\right)\pi\left(x\right)\left(\hX_{x^{*}}-\TX\fX_{x}^{*}\right)=\left(\E_{\X}\hX\right)\hX_{x^{*}}-\sum_{x\in\OmX}\TX\hX\left(x\right)\pi\left(x\right)\fX_{x}^{*}.
\]
By assumption $\left(\E_{\X}\hX\right)=0$. If $\OmX$ is finite,
as it will be in our applications, then it would immediately follow
that $\hX\in\im\TX$. Otherwise we will need some functional analysis.
Note that $\TX$ is actually an isometry between a subspace $\FX$
of $L^{1}\left(\OmX^{2},\L{\eX}\right)$ and $L^{1}\left(\OmX,\L{\X}\right)$
. First we prove that $\FX$ is closed and therefore a Banach space.
For any $\fX\in L^{1}\left(\OmX^{2},\L{\eX}\right)$, let $\bar{\fX}$
be defined by $\left(x_{1},x_{2}\right)\mapsto-\fX\left(x_{2},x_{1}\right)$,
so that $\fX\in\FX$ implies that $\fX=\bar{\fX}$. Suppose $\fX_{n}\to\fX$,
with $\fX_{n}\in\FX$ for all $n\in\NN$. By exchangeability we have
\[
\left\Vert \fX_{n}-\fX\right\Vert =\E\left|\fX_{n}\left(\eX\right)-\fX\left(\eX\right)\right|=\E\left|\fX_{n}\left(\X_{2},\X_{1}\right)-\bar{\fX}\left(\X_{2},\X_{1}\right)\right|=\left\Vert \fX_{n}-\bar{\fX}\right\Vert 
\]
so $\fX=\bar{\fX}$ and $\fX\in\FX$. Finally, it is a simple fact
that an isometry between Banach spaces has a closed range. For, if
$\left(\TX\fX_{n}\right)_{n\in\NN}$ is a Cauchy sequence in $\im\TX$,
then $\left(\fX_{n}\right)_{n\in\NN}$ is a Cauchy sequence in $\FX$
which converges to some $\fX\in\FX$. It follows that $\TX\fX_{n}\to\TX\fX\in\im\TX$.

We have proved that $\hX\in\im\TX$, completing the proof that $\ker\E_{\X}\subseteq\im\TX$.
\end{proof}
The final step is to choose an operator $\conn:\Fo\to\FX$ in such
a way that $\TX$ can be easily compared with $\To\conn$.

\begin{note}

It's not clear if characterizing operators are in any sense unique
so that for two characterizing operators $\T_{1},\T_{2}$ there is
\emph{always} a connection $\conn$ that makes $\T_{1}=\T_{2}\conn$.

\end{note}
\begin{example}
\label{example:poisson-connecting-operator}For the Poisson case in
\ref{thm:poisson-characterizing}, we need to compare $\lambda\fo\left(\X+1\right)$
with $\X\fo\left(\X\right)$. It is often fruitful to define $\conn$
by 
\[
\conn\fo\left(x_{1},x_{2}\right)=c\fo\left(x_{2}\right)\one\left\{ x_{2}=x_{1}+1\right\} -c\fo\left(x_{1}\right)\one\left\{ x_{1}=x_{2}+1\right\} 
\]
for some $c\in\RR$. We will then have
\[
\To\fo-\TX\conn\fX=\fo\left(\X+1\right)\left(\lambda-c\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\right)-\fo\left(\X_{1}\right)\left(\X_{1}-c\Pr\left(\X_{1}=\X_{2}+1|\X_{1}\right)\right).
\]
Using the triangle inequality and the choice of $\cY$ in \ref{example:poisson-bound},
for all $\fo\in\cY$: 
\[
\E_{\X}\To\fo\le\min\left(1,\lambda^{-1/2}\right)\left(\E\left|\lambda-c\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\right|+\E\left|\X_{1}-c\Pr\left(\X_{1}=\X_{2}+1|\X_{1}\right)\right|\right).
\]
This approximation is effective when 
\begin{align}
\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right) & \approx\lambda/c,\label{eq:immigration-death}\\
\Pr\left(\X_{2}=\X_{1}-1|\X_{1}\right) & \approx\X_{1}/c.\nonumber 
\end{align}
The interpretation of these approximate equalities is that the Markov
chain associated with $\eX$ is approximately an immigration-death
process. This is likely to happen when $\X\left(\om\right)$ is in
some sense a statistic of the amount of local structure over the object
$\om$, and $\eX$ is defined by a Markov chain on $\Om$ (as in \ref{rem:underlying-pair})
that (uniformly) randomly disturbs local structure. The conclusion
to \ref{example:fixed-points} should make this clear:
\end{example}
\begin{todo}

I should include sub-$\sigma$-algebra note to simplify conditional
expectation stuff

\end{todo}


\begin{example}
We continue \ref{example:fixed-points}, recalling the exchangeable
pairs $\mathbf{W}$ and $\eX$ from \ref{example:fixed-points-exchangeable-pair}.
The interpretation of 
\[
\Pr\left(\X_{2}=\X_{1}-1|\X_{1}\right)
\]
is the probability of a transposition destroying exactly one out of
an existing $\X_{1}$ fixed points. In order to destroy exactly one
fixed point, we have to choose a fixed point to destroy, and swap
it with a non-fixed-point. There are $\X_{1}\left(n-\X_{1}\right)$
out of ${n \choose 2}$ transpositions that do this, so
\[
\Pr\left(\X_{1}=\X_{2}+1|\X_{1}\right)=\frac{\X_{1}\left(n-\X_{1}\right)}{{n \choose 2}}.
\]
Next, we will find a formula for $\Pr\left(\X\left(W_{2}\right)=\X\left(W_{1}\right)+1|W_{1}\right)$,
noting that 
\[
\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)=\E\left[\Pr\left(\X\left(W_{2}\right)=\X\left(W_{1}\right)+1|W_{1}\right)|\X_{1}\right].
\]
In order to create exactly one fixed point, we have to choose an index
$k$ that is not fixed in $W_{1}$ (there are $n-\X_{1}$ such) and
compose $W_{1}$ with $\left(k\,\sigma\left(k\right)\right)$. This
creates exactly one fixed point unless $\sigma^{-1}\left(k\right)=k$,
in which case it creates two. We have counted this second case twice
for every transposition in the cycle decomposition of $W_{1}$. Let
$Y$ be the number of transpositions in the cycle decomposition of
$W_{1}$. We have 
\[
\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)=\frac{n-\X_{1}-2\E\left[Y|\X_{1}\right]}{{n \choose 2}}.
\]
In order to satisfy \ref{eq:immigration-death} as closely as possible,
we choose $c={n \choose 2}/n$. Recalling that $\E\X_{1}=1$, we then
have
\begin{align*}
\E\left|1-c\Pr\left(X_{2}=X_{1}+1|X_{1}\right)\right| & =\E\left[1-\frac{n-\X_{1}-2\E\left[Y|\X_{1}\right]}{n}\right]\\
 & =1/n+2\E Y/n,\\
\E\left|X_{1}-c\Pr\left(X_{2}=X_{1}-1|X_{1}\right)\right| & =\E\left[\X_{1}-\frac{\X_{1}\left(n-\X_{1}\right)}{n}\right]\\
 & =\E\left[\X_{1}^{2}\right]/n.
\end{align*}
Now, the probability that a transposition $\left(\i\,\ii\right)$
is in the cycle decomposition of $W_{1}$ is $\left(n\left(n-1\right)\right)^{-1}$
because $\i$ must map to $\ii$ out of the $n$ possible options
in $\range n$, then $\ii$ must map to $\i$ out of the $n-1$ possible
options in $\range n\backslash\left\{ \ii\right\} $. There are ${n \choose 2}=n\left(n-1\right)/2$
possible transpositions so by \ref{rem:indicator-expectation} it
follows that $\E Y=1/2$.

Now, $\E\left[\X_{1}\left(\X_{1}-1\right)/2\right]$ is the expected
number of unordered pairs of distinct fixed points in a permutation
$\sigma\in\rS n$. For any unordered pair of distinct indices $\left\{ \i,\ii\right\} $,
the probability that both are fixed is $\left(n\left(n-1\right)\right)^{-1}$
because $\i$ must map to $\i$ out of the $n$ possible options in
$\range n$, then $\ii$ must map to $\ii$ out of the $n-1$ possible
options in $\range n\backslash\left\{ \i\right\} $. The total number
of unordered pairs is ${n \choose 2}=n\left(n-1\right)/2$, so again
applying \ref{rem:indicator-expectation}, we have $\E\left[\X_{1}\left(\X_{1}-1\right)/2\right]=1/2$
and $\E\left[\X_{1}^{2}\right]=2$.

We conclude that $d_{\TV}\left(\X,\Po\left(1\right)\right)\le4/n$.
\end{example}
\begin{note}

The ``generator method'' \cite{BC05} says that the Poisson characterizing
operator can be obtained with the generator of an immigration-death
process and the Normal characterizing operator can be obtained with
the generator of an O-U process. Investigate the link here?

\end{note}


\subsection{An Application: Switchings and Short Cycles in Random Regular Graphs}

A particularly interesting (and new!) application of Stein's method
is to piggyback onto results proved using combinatorial\emph{ switchings.}
In this subsection, I'll describe how a certain Poisson limit theorem
concerning short cycles in random regular graphs was improved by applying
Stein's method in a quite natural, and generally applicable, way.
First, we need some background on short cycles in random regular graphs,
and switchings.


\subsubsection{Short Cycles in Random Regular Graphs}

Let $\X_{\ell,n}^{\left(d\right)}$ be the number of cycles of length
$\ell$ in a random $d$-regular graph on $n$ vertices. The following
is a critical theorem describing the structure of random regular graphs.
\begin{thm}
\label{thm:short-cycles-fixed}Fix $\ell\in\NN$ and $d\in\NN$ with
$d\ge3$. Then
\[
\Lb{\X_{\ell,n}^{\left(d\right)}}\to\Po\left(\frac{\left(d-1\right)^{\ell}}{2\ell}\right)
\]
as $n\to\infty$, with $n$ restricted to the even integers if $d$
is odd.
\end{thm}
One reason \ref{thm:short-cycles-fixed} is interesting is because
it tells us the number of ``short cycles'' in a random regular graph
does not grow (to infinity) with the size of the graph. That is, random
regular graphs are likely to ``locally'' ``look like forests''.
\ref{thm:short-cycles-fixed} is most easily proved with the method
of moments; see \cite{Wor99}.

\ref{thm:short-cycles-fixed} theorem can be extended to the case
where $d$ is allowed to grow modestly with $n$. In \cite{MWW04},
a natural variant of \ref{thm:short-cycles-fixed} was proved for
$\left(d-1\right)^{2\ell-1}=o\left(n\right)$, using the idea of \emph{switchings}.
The condition $\left(d-1\right)^{2\ell-1}=o\left(n\right)$ appeared
to be a natural threshold; it was conjectured that short cycle counts
are no longer asymptotically Poisson if $d$ grows any faster.

It was recently proved that in fact cycle counts remain asymptotically
Poisson past this boundary:
\begin{thm}
[\cite{Joh11}]\label{thm:short-cycles-variable}Let $\ell$ and $d$
depend on $n$, in such a way that $d\ge3$ and 
\[
\sqrt{\ell}\left(d-1\right)^{3\ell/2-1}=o\left(n\right).
\]
Then,
\[
d_{\TV}\left(\X_{\ell,n}^{\left(d\right)},\Po\left(\frac{\left(d-1\right)^{\ell}}{2\ell}\right)\right)\to0.
\]
if $n\to\infty$ in such a way that $nd$ is always even.
\end{thm}
The proof of \ref{thm:short-cycles-variable} combines switchings
with Stein's method.


\subsubsection{Switchings}

Formally speaking, a switching is a binary relation $\rightsquigarrow$
on a set of combinatorial objects $\Om$ (in our case, $\Om$ is the
set of $d$-regular graphs on $n$ vertices). Usually, switchings
are understood as an ``action'' that changes one combinatorial object
to another. The switching used in \cite{MWW04} is defined as follows:
\begin{defn}
\label{def:cycle-switching}Let $C=v_{1}\dots v_{\ell}$ be an $\ell$-cycle
in a $d$-regular graph $G$. For each $\i\in\ZZ/\ell\ZZ$, suppose
$e_{\i}=u_{\i}w_{\i+1}\in E\left(G\right)$ satisfies $u_{\i}v_{\i}\notin E\left(G\right)$
and $v_{\i}w_{\i}\notin E\left(G\right)$. Let $G'$ be the $d$-regular
graph obtained from $G$ by deleting all the edges in $C$ and adding
each of the edges $u_{\i}v_{\i}$ and $v_{\i}w_{\i}$. Further, suppose
this operation does not create or delete any $\ell$-cycle except
$C$. Then, we say $G\rightsquigarrow G'$.
\end{defn}
\begin{todo}

Need picture here. Also need precise definition: are edges/cycle oriented?
Can we have some $u_{\i}=v_{\ii}$?

\end{todo}

The typical application of a switching is to estimate the relative
sizes of some subsets of $\Om$, by analysing the ``flow'' of switchings
between the subsets. For example, define $S\left(k\right)$ to be
the set of $d$-regular graphs with $k$ cycles of length $\ell$.
In \cite[Section 3]{MWW04}, bounds were obtained for the ``number
of ways to switch out'' of each graph $G$ (that is, the number of
graphs $G'$ satisfying $G\rightsquigarrow G'$), and the ``number
of ways to switch in'' to each graph. This gives an estimate on the
relative sizes of $S\left(k\right)$ and $S\left(k-1\right)$ for
each $k$, which is then used to estimate the distribution of $\X_{\ell,n}^{\left(d\right)}$.
The relevant estimates work in the regime $\left(d-1\right)^{2\ell-1}=o\left(n\right)$.


\subsubsection{Stein's method and Switchings}

\global\long\def\fG{\mathfrak{G}}


The application of switchings in the proof of \ref{thm:short-cycles-variable}
is different conceptually, but the same kind of estimates are needed.
Hopefully it is possible that Stein's method can be applied similarly
to a variety of different switching arguments unrelated to short cycles
in random regular graphs.

The approach is conceptually similar to that of \ref{example:fixed-points}.
In the case of permuations, the transposition of two random indices
is a simple and effective way to perturb a permutation in an appropriate
way to apply the method of \ref{example:poisson-connecting-operator}.
However, it is not so easy to perturb a regular graph in a way that
maintains regularity. We accomplish this with the switching from \ref{def:cycle-switching}.

We define a multigraph $\fG$ with vertex set $\Om$. Make an edge
in $\fG$ for every pair $\left(G,G'\right)$ with $G\rightsquigarrow G'$.
Let $\Delta$ be the maximum degree so far. Add enough loops to each
vertex of $\fG$ so that $\fG$ is a $\Delta$-regular multigraph.
Recalling \ref{todo:graph-to-markov}, $\fG$ induces a reversible
Markov chain on $\Om$ with stationary distribution $\rG nd$, which
corresponds to an exchangeable pair $\mathbf{G}=\left(G_{1},G_{2}\right)$
with margins $\rG nd$. With \ref{rem:underlying-pair}, this provides
us with an exchangeable pair $\bX=\left(\X_{1},\X_{2}\right)$ with
margins $\L{\X}$ to apply Stein's method.


\subsection{Proof of \ref{thm:short-cycles-variable}}

(Simplified from the proof of \cite[Theorem 11]{Joh11}).

Let $\X=\X_{\ell,n}^{\left(d\right)}$, let $\Om$ be the set of all
$d$-regular graphs on $n$ vertices, and let $\lambda=\left(d-1\right)^{\ell}/\left(2\ell\right)\ge1$.

The approach is conceptually similar to that of \ref{example:fixed-points}.
In the case of permuations, the transposition of two random indices
is a simple and effective way to perturb a permutation in an appropriate
way to apply the method of \ref{example:poisson-connecting-operator}.
However, it is not so easy to perturb a regular graph in a way that
maintains regularity. We accomplish this with the switching from \ref{def:cycle-switching}.

\global\long\def\fG{\mathfrak{G}}


We define a multigraph $\fG$ with vertex set $\Om$. Make an edge
in $\fG$ for every pair $\left(G,G'\right)$ with $G\rightsquigarrow G'$.
Let $\Delta$ be the maximum degree so far. Add enough loops to each
vertex of $\fG$ so that $\fG$ is a $\Delta$-regular multigraph.
Recalling \ref{todo:graph-to-markov}, $\fG$ induces a reversible
Markov chain on $\Om$ with stationary distribution $\rG nd$, which
corresponds to an exchangeable pair $\mathbf{G}=\left(G_{1},G_{2}\right)$
with margins $\rG nd$. With \ref{rem:underlying-pair}, this provides
us with an exchangeable pair $\bX=\left(\X_{1},\X_{2}\right)$ with
margins $\Lb{\X_{\ell,n}^{\left(d\right)}}$ to apply Stein's method.

\begin{comment}

Is it confusing for $G_{1},G_{2}$ to be random graphs, while graphs
denoted $G$ or $G'$ generally refer to a specific, non-random graph?
Maybe I should denote particular graphs with $\om$ instead or something.

\end{comment}

For $G\in\Om$ and an $\ell$-cycle $C$, let $F_{C}\left(G\right)$
be the number of ways to switch from $G$ by destroying $C$. That
is,
\[
F_{C}\left(G\right)=\left|\left\{ G'\in\Om:\, G\rightsquigarrow G',\, C\subseteq G,\, C\nsubseteq G'\right\} \right|,
\]
so that $\sum_{C\in K_{n}}F_{C}\left(G\right)$ is the number of ways
to switch from $G$, destroying one $\ell$-cycle. Then,
\begin{alignat*}{1}
\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right) & =\E\left[\Pr\left(\X_{2}=\X_{1}+1|G_{1}\right)|\X_{1}\right]\\
 & =\E\left[\left.\frac{1}{\Delta}\sum_{C\in K_{n}}F_{C}\left(G_{1}\right)\right|\X_{1}\right].
\end{alignat*}


\begin{todo}

Conditioning on $\X_{1}$ is annoying, I'd rather condition on $G_{1}$.
I should add a remark to this effect when introducing the method in
the theory section, and make the same adjustment to the permutations
example.

\end{todo}

Next, the automorphism group of an $\ell$-cycle has size $2\ell$,
so the number of $\ell$-cycles in $K_{n}$ is $\falling n{\ell}/\left(2\ell\right)$.
It follows that
\begin{alignat*}{1}
\E\left|\X_{1}-\frac{\Delta}{\falling n{\ell}d^{\ell}}\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\right| & =\E\left|\sum_{C\in K_{n}}\one_{C\subseteq G_{1}}-\frac{1}{\falling n{\ell}d^{\ell}}\sum_{C\in K_{n}}F_{C}\left(G\right)\right|\\
 & \le\frac{\falling n{\ell}}{2\ell}\E\left|\one_{C\subseteq G_{1}}-\frac{F_{C}\left(G_{1}\right)}{\falling n{\ell}d^{\ell}}\right|.
\end{alignat*}
Similarly, let $B_{C}\left(G\right)$ be the number of ways to switch
into $G$ by destroying $C$. We have
\[
\E\left|\lambda-\frac{\Delta}{\falling n{\ell}d^{\ell}}\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\right|\le\frac{\falling n{\ell}}{2\ell}\E\left|\frac{\left(d-1\right)^{n}}{\falling n{\ell}}-\frac{B_{C}\left(G_{1}\right)}{\falling n{\ell}d^{\ell}}\right|.
\]
The majority of this section will consist of combinatorially bounding
these expectations, resulting in the following lemmas:
\begin{lem}
\label{lem:forward-switching-bound}Uniformly in $C$,
\[
\E\left|\one_{C\subseteq G_{1}}-\frac{F_{C}\left(G_{1}\right)}{\falling n{\ell}d^{\ell}}\right|=O\left(\frac{\ell\left(d-1\right)^{2\ell-1}}{n^{\ell+1}}\right).
\]

\end{lem}

\begin{lem}
\label{lem:backward-switching-bound}Uniformly in $C$,
\[
\E\left|\frac{\left(d-1\right)^{n}}{\falling n{\ell}}-\frac{B_{C}\left(G_{1}\right)}{\falling n{\ell}d^{\ell}}\right|=O\left(\frac{\ell\left(d-1\right)^{2\ell-1}}{n\falling n{\ell}}\right).
\]

\end{lem}
After proving these lemmas, we can conclude that
\begin{alignat*}{1}
d_{\TV}\left(\X,\Po\left(\lambda\right)\right) & \le\lambda^{-1/2}\left(\E\left|\lambda-\Delta\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\right|+\E\left|\X_{1}-\Delta\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\right|\right)\\
 & =O\left(\frac{\sqrt{\ell}\left(d-1\right)^{3\ell/2-1}}{n}\right).
\end{alignat*}
This concludes the proof of \ref{thm:short-cycles-variable}.

In order to prove \ref{lem:forward-switching-bound,backward-switching-bound},
we will need a few lemmas.
\begin{lem}
Uniformly in $C$ and $G$ such that $C$ does not share an edge with
another $\ell$-cycle in $G$,
\[
F_{C}\left(G\right)\ge\falling n{\ell}d^{\ell}\left(1-\frac{2\ell\gamma\left(G\right)+O\left(\ell\left(d-1\right)^{\ell}\right)}{nd}\right),
\]
where $\gamma\left(G\right)$ is the number of $\ell$-cycles in $G$.\end{lem}
\begin{proof}
adsf\end{proof}
\begin{lem}
Uniformly in $C$ and $G$,
\[
B_{C}\left(G\right)\ge\left(d\left(d-1\right)\right)^{\ell}\left(1-\frac{O\left(\ell\left(d-1\right)^{\ell-1}\right)}{n}\right)
\]
\end{lem}
\begin{proof}
adsf
\end{proof}

\begin{proof}
[Proof of \ref{lem:forward-switching-bound}]We partition $\rG nd$
into three events:
\begin{alignat*}{1}
\A_{1} & =\left\{ G\in\Om:C\nsubseteq G\right\} ,\\
\A_{2} & =\left\{ G\in\Om:C\subseteq G,\, C\mbox{ does not share an edge with another \ensuremath{\ell}-cycle in }G\right\} ,\\
\A_{3} & =\left\{ G\in\Om:C\subseteq G,\, C\mbox{ shares an edge with another \ensuremath{\ell}-cycle in }G\right\} .
\end{alignat*}
If $G\in\A_{1}$ then $C\nsubseteq G$, and no switching can destroy
$C$, so $F_{C}\left(G\right)=0$. If $G\in\A_{3}$, then a switching
that destroys $C$ also destroys some other $\ell$-cycle, which is
not allowed in the definition of $\rightsquigarrow$. So, $F_{C}\left(G\right)=0$.
Also, note that a switching that deletes $C$ is determined by the
$\ell$ oriented edges $e_{\i}$ in the definition of $\rightsquigarrow$.
There are at most $\falling n{\ell}d^{\ell}$ ways to choose these
edges (choose the $\ell$ vertices $u_{\i}$ then choose a neighbor
$w_{\i}$ for each), so $F_{C}\le\falling n{\ell}d^{\ell}$.

It follows that
\begin{alignat*}{1}
\E\left|\one_{C\subseteq G_{1}}-\frac{F_{C}\left(G_{1}\right)}{\falling n{\ell}d^{\ell}}\right| & =\E\left[\one_{A_{1}}\left|0-0\right|+\one_{\A_{2}}\left|1-\frac{F_{C}\left(G_{1}\right)}{\falling n{\ell}d^{\ell}}\right|+\one_{\A_{1}}\left(1-0\right)\right]\\
 & =\E\left[\one_{\A_{2}}\left(1-\frac{F_{C}\left(G_{1}\right)}{\falling n{\ell}d^{\ell}}\right)\right]+\Pr\left(\A_{3}\right)\\
\end{alignat*}


Note that a switching that deletes $C$ is determined by the $\ell$
oriented edges $e_{\i}$ in the definition of $\rightsquigarrow$.
There are at most $\falling n{\ell}d^{\ell}$ ways to choose these
edges (choose the $\ell$ vertices $u_{\i}$ then choose a neighbor
$w_{\i}$ for each), so $F_{C}\le\falling n{\ell}d^{\ell}$.

, so $F_{C}\left(G\right)\le\falling n{\ell}d^{\ell}$ and
\end{proof}

\begin{proof}
[Proof of \ref{lem:backward-switching-bound}]asfd
\end{proof}

\section{Size-Bias Coupling}

\begin{comment}

I'd like to go into a number of small examples (perhaps interspersed
in the discussion of Stein's method in Part I), but I'd like to also
go through a number of ``big'' examples. I'd like these examples
to showcase
\begin{itemize}
\item different types of results: most applications give quantitative estimates.
\cite{Joh11} gives a non-quantitative distributional convergence
result that was not previously proved using other methods. There are
also results that have no connection with distribution metrics, such
as the concentration inequalities in \cite{Ros11}. In particular,
the Latin rectangle example in \cite{Ste86} is interesting in that
the final result is not probabilistic.
\item different types of distributions: definitely at least the Poisson
and normal case, perhaps also an example of a more exotic distribution
like the one in \cite{FS12} or perturbations of Poisson/normal distributions
as in \cite{BCX07}.
\item different ways to apply stein's method: definitely exchangeable pairs
and probably size-biasing. Maybe also Zero-bias coupling.
\end{itemize}
\end{comment}

\bibliographystyle{amsalpha}
\bibliography{readings}

\end{document}
