%% LyX 2.0.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt,english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm,headheight=3cm,headsep=3cm,footskip=1.5cm}
\setlength{\parskip}{\bigskipamount}
\setlength{\parindent}{0pt}
\synctex=1
\usepackage{babel}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{esint}
\setstretch{1.5}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=false]
 {hyperref}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}[section]
  \theoremstyle{plain}
  \newtheorem{cor}[thm]{\protect\corollaryname}
  \theoremstyle{plain}
  \newtheorem{lem}[thm]{\protect\lemmaname}
  \theoremstyle{plain}
  \newtheorem{prop}[thm]{\protect\propositionname}
  \theoremstyle{plain}
  \newtheorem{conjecture}[thm]{\protect\conjecturename}
  \theoremstyle{definition}
  \newtheorem{defn}[thm]{\protect\definitionname}
  \theoremstyle{definition}
  \newtheorem{example}[thm]{\protect\examplename}
  \theoremstyle{remark}
  \newtheorem{rem}[thm]{\protect\remarkname}
  \theoremstyle{remark}
  \newtheorem{claim}[thm]{\protect\claimname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%bold caption labels
\usepackage[labelfont=bf]{caption}

\usepackage{ifthen}

%\allowdisplaybreaks

% clickable links
%\usepackage[bookmarks,hidelinks]{hyperref} 

%if I don't do this I get an error on URLs
\usepackage{url}

% list hackery
\usepackage{enumitem}
\setlist[enumerate]{label=\textup{(\roman*)},topsep=-5pt}
\setlist[itemize]{topsep=-5pt}

% cleveref allows \ref{thm:asdf} instead of Theorem~\ref{thm:asdf}
\usepackage[nameinlink,capitalise,noabbrev]{cleveref}
\AtBeginDocument{\renewcommand{\ref}[1]{\cref{#1}}}
\crefname{section}{Chapter}{Chapters}
\crefname{subsection}{Section}{Sections}
\crefname{remark}{Remark}{Remarks}

% LyX won't let me include cleveref before theorem declarations so I need to redefine everything as a hack
\theoremstyle{plain}
\newtheorem{mythm}{\protect\theoremname}[section]
\renewenvironment{thm}{\begin{mythm}}{\end{mythm}}
\theoremstyle{definition}
\newtheorem{mydefn}[mythm]{\protect\definitionname}
\renewenvironment{defn}{\begin{mydefn}}{\end{mydefn}}
\theoremstyle{definition}
\newtheorem{myexample}[mythm]{\protect\examplename}
\renewenvironment{example}{\begin{myexample}}{\end{myexample}}
\theoremstyle{plain}
\newtheorem{myprop}[mythm]{\protect\propositionname}
\renewenvironment{prop}{\begin{myprop}}{\end{myprop}}
\theoremstyle{plain}
\newtheorem{mycor}[mythm]{\protect\corollaryname}
\renewenvironment{cor}{\begin{mycor}}{\end{mycor}}
\theoremstyle{plain}
\newtheorem{mylem}[mythm]{\protect\lemmaname}
\renewenvironment{lem}{\begin{mylem}}{\end{mylem}}
\theoremstyle{plain}
\newtheorem{myconjecture}[mythm]{\protect\conjecturename}
\renewenvironment{conjecture}{\begin{myconjecture}}{\end{myconjecture}}
\theoremstyle{remark}
\newtheorem{myrem}[mythm]{\protect\remarkname}
\renewenvironment{rem}{\begin{myrem}}{\end{myrem}}
\theoremstyle{remark}
\newtheorem{myclaim}[mythm]{\protect\claimname}
\renewenvironment{claim}{\begin{myclaim}}{\end{myclaim}}

% equation cref format
\crefformat{equation}{#2(#1)#3}

% make cref play nicely with different kinds of lists
\newlist{conditions}{enumerate}{3}
\setlist[conditions]{label=\textup{(\roman*)},topsep=-5pt}
\if@cref@capitalise
  \crefname{conditions}{Condition}{Conditions}
\else
  \crefname{conditions}{Condition}{Conditions}
\fi
\crefalias{conditionsi}{conditions}
\crefalias{conditionsii}{conditions}
\crefalias{conditionsiii}{conditions}

% ordered lists should use parens instead of a point
%\renewcommand\theenumi{\arabic{enumi}}
%\renewcommand\labelenumi{(\theenumi)}

% \left(\right) should behave the same as ()
\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

% table of contents spacing tweaks
\usepackage{tocloft}
\setlength\cftparskip{-2pt}
\setlength\cftbeforesecskip{1pt}
\setlength\cftaftertoctitleskip{2pt}

% comment environment
\usepackage{verbatim}

% mainly for light colours color!percent
\usepackage{xcolor}

% shaded WIP notes
\theoremstyle{definition}
\newtheorem{commentthm}{Comment}[section]
\newtheorem{todothm}[commentthm]{Issue}
\usepackage{framed}
\usepackage{lipsum}
\newenvironment{note}
{\colorlet{shadecolor}{blue!5}\begin{shaded}\begin{commentthm}}{\end{commentthm}\end{shaded}}
\newenvironment{todo}
{\colorlet{shadecolor}{red!5}\begin{shaded}\begin{todothm}}{\end{todothm}\end{shaded}}

%more distinct calligraphic letters
%\usepackage{calrsfs}

%theorem names in uppercase?
\let\oldtextbf\textbf
\renewcommand\textbf[1]{\uppercase{#1}}

%graph drawing
\usepackage{tikz}
\tikzstyle{vertex}=[circle,draw=black,fill=black,inner sep=0,minimum size=0.2cm,text=white,font=\footnotesize]
\tikzset{every loop/.style={min distance=50,in=50,out=130,looseness=7}}

%\usepackage{titlepic}
\hypersetup{
 pdftitle = {Stein's Method},
 pdfauthor = {Matthew Kwan}
}

\makeatother

  \providecommand{\claimname}{Claim}
  \providecommand{\conjecturename}{Conjecture}
  \providecommand{\corollaryname}{Corollary}
  \providecommand{\definitionname}{Definition}
  \providecommand{\examplename}{Example}
  \providecommand{\lemmaname}{Lemma}
  \providecommand{\propositionname}{Proposition}
  \providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

\begin{document}
\begin{titlepage}


\title{\includegraphics[scale=0.3]{crest_4}\vfill{}
Stein's Method}


\author{Matthew Kwan\\
Supervisor: Catherine Greenhill\\
\\
School of Mathematics and Statistics,\\
The University of New South Wales}

\maketitle
\noindent \begin{center}
\vfill{}
Submitted in partial fulfillment of the requirements of the degree
of\\
Bachelor of Science with Honours
\par\end{center}

\thispagestyle{empty}\end{titlepage}

\pagenumbering{roman}

\begin{comment}
\begin{thm}
t\end{thm}
\begin{cor}
c\end{cor}
\begin{lem}
l\end{lem}
\begin{prop}
p\end{prop}
\begin{conjecture}
c\end{conjecture}
\begin{defn}
d\end{defn}
\begin{example}
e\end{example}
\begin{rem}
r\end{rem}
\begin{claim}
c
\end{claim}
\end{comment}

\global\long\def\floor#1{\left\lfloor #1\right\rfloor }


\global\long\def\ceil#1{\left\lceil #1\right\rceil }


\global\long\def\i{i}


\global\long\def\ii{j}


\global\long\def\iii{q}


\global\long\def\iiii{r}


\global\long\def\NN{\mathbb{N}}


\global\long\def\ZZ{\mathbb{Z}}


\global\long\def\RR{\mathbb{R}}


\global\long\def\CC{\mathbb{C}}


\global\long\def\F{\mathcal{F}}


\global\long\def\range#1{\left[#1\right]}


\global\long\def\rest#1#2{#1|_{#2}}


\global\long\def\cond{\,\middle\vert\,}


\global\long\def\d{\operatorname d}


\global\long\def\id{\operatorname{id}}


\global\long\def\one{\operatorname{\boldsymbol{1}}}


\global\long\def\falling#1#2{\left(#1\right)_{#2}}



\section*{Acknowledgements}

First and foremost I would like to thank my supervisor Catherine Greenhill
for her support and guidance for the duration of the thesis, and also
for providing me with invaluable advice and opportunities outside
of her role as thesis advisor.

I would like to thank the honours students of 2013 and 2014, for making
the honours room a great place to learn and collaborate.

I would also like to thank the honours coordinators Hendrik Grundling
and Daniel Chan, for running the honours program smoothly, providing
useful advice, and ensuring I knew how to give a seminar!

Finally, I would like to thank every one of my maths and stats lecturers
from 2010 to 2014, whose effort and expertise have managed to transform
me into an enthusiastic (and maybe even capable!) student of mathematics.

\newpage{}

\tableofcontents{}

\begin{comment}Things remaining:
\begin{itemize}
\item size-bias coupling
\item address and remove comments
\item fix overfull hboxes and large empty spaces
\end{itemize}
\end{comment}

\newpage{}

\pagenumbering{arabic}

\setcounter{section}{-1}


\section{Introduction}

The \emph{\textbf{c}entral \textbf{l}imit \textbf{t}heorem}, roughly
speaking, says that sums of independent, identically distributed random
variables are ``approximately normal'', with the approximation improving
as the number of terms in the sum increases. Similarly, the \emph{Poisson
\textbf{l}imit \textbf{t}heorem} says that the number of occurences
of independent ``rare'' events ``approximately Poisson''. These
two limit theorems are archetypal examples of a large class of related
results in statistics and probabilistic combinatorics.

These types of results are generally formally stated asymptotically,
with a particular type of convergence called \emph{weak convergence}.
Such results are powerful, but a shortcoming is that they do not ``quantify''
the convergence. It may be that a sequence of random variables is
``asymptotically normal'', but we cannot say anything about the
normality of any particular random variable in that sequence. Even
if we are only interested in asymptotic results, it can be problematic
that we cannot say convergence is ``uniform'' in any sense.

The solution to both these problems is to define a distance metric
between probability distributions, and study convergence with respect
to this metric. Fortunately, weak convergence is in fact convergence
with respect to a topology, and this topology is metrizable.

\emph{Stein's method }was introduced by Charles Stein in 1972, as
a general method for approximating expected values. Stein's method
has since proved especially useful for bounding the distance between
probability distributions, in a variety of metrics consistent with
weak convergence. Such bounds can be used to quantify existing limit
theorems, and can also be used as a tool to prove purely asymptotic
results.

In this thesis, we will first set out some theoretical groundwork,
including an overview of basic probability theory and weak convergence
(\ref{sec:probability-theory}). We then present a general framework
for Stein's method (\ref{sec:stein-general}) before specializing
to the Normal and Poisson cases (\ref{sec:normal,sec:poisson}). We
finally outline a few particular ways of applying the method (\ref{sec:size-bias-coupling,sec:exchangeable-pairs}).
We include a number of specific examples and applications throughout.

Unless stated otherwise, all proofs are ``original'' in that I came
up with them, although many are quite straightforward and probably
exist elsewhere. For the proofs that I have adapted from other sources
(which are all cited), I tried to add value with clearer and more
complete explanation, or by simplifying to a special case.


\subsection{Notation and Assumed Knowledge}

For this thesis, the set of natural numbers $\NN$ includes zero.
We write $\one_{A}$ for the characteristic function of a set $A$;
that is, $\one_{A}\left(x\right)=1$ if $x\in A$, otherwise $\one_{A}\left(x\right)=0$.
The function $f$ restricted to the set $A$ is denoted $\rest fA$.
The falling factorial $n\left(n-1\right)\dots\left(n-k+1\right)$
is denoted $\falling nk$. Finally, $\range k$ denotes the set $\left\{ 1,\dots,k\right\} $. 

All asymptotics are as $n\to\infty$ unless stated otherwise. We will
occasionally use big-oh notation non-asymptotically, to avoid writing
constant factors.

\begin{comment}, $f\prec g$ means $f=O\left(g\right)$ and $f\asymp g$
means $f=\Theta\left(g\right)$.\end{comment}

I will occasionally use results from analysis without proof. I will
usually refer to a numbered theorem in Rudin's \emph{Real and Complex
Analysis} \cite{Rud66} or \emph{Functional Analysis} \cite{Rud73}
when doing so.

I will also assume some familiarity with basic graph theory definitions
and terminology. The reader may refer to a textbook such as \cite{Die00}
if necessary. We also specify some conventions here. Unless stated
otherwise, graphs are labelled. That is, they are distinguished even
within isomorphism classes. A graph may not have loops or multiple
edges; an object which is allowed to have loops and/or multiple edges
will be called a multigraph. See \ref{fig:graph-conventions}. We
write $G\subseteq G'$ to indicate that $G$ is a (not necessarily
induced) subgraph of $G'$.\\


\begin{figure}[h]
\begin{center}
\foreach \j in {1,2,3}
{
\hspace{0.5cm}
\begin{tikzpicture}[scale=.6]
\node[vertex] (1) at (0,3) {};
\node at (-0.4,3.4) {\ifthenelse{\equal{\j}{3}}{2}{1}};
\node[vertex] (4) at (3,0) {};
\node at (3.4,-0.4) {4};
\node[vertex] (3) at (0,0) {};
\node at (-0.4,-0.4) {3};
\node[vertex] (2) at (3,3) {};
\node at (3.4,3.4) {\ifthenelse{\equal{\j}{3}}{1}{2}};
\ifthenelse{\equal{\j}{1}}{
\path
(1) edge [bend right] (3)
    edge [bend left] (3)
    edge (2)
(4) edge (3)
    edge [loop right,-] (4)
(2) edge (3);
}
{
\path
(1) edge (3)
    edge (4)
(4) edge (3)
(2) edge (3);
}
\end{tikzpicture}
\hspace{0.5cm}
}
\end{center}

\caption{\label{fig:graph-conventions}From left to right: a multigraph that
is not a graph, and two different graphs on the vertex set $\left\{ 1,2,3,4\right\} $.}
\end{figure}



\section{\label{sec:probability-theory}General Probability Theory}

\global\long\def\L#1{\mathcal{L}_{#1}}


\DeclareRobustCommand{\L}[1]{\ifmmode{\mathcal{L}_{#1}}\else\polishL\fi}

\global\long\def\Lb#1{\mathcal{L}\left(#1\right)}


\global\long\def\Om{\Omega}


\global\long\def\om{\omega}


\global\long\def\cA{\mathcal{A}}


\global\long\def\B{\mathcal{B}}


\global\long\def\E{\mathbb{E}}


\global\long\def\F{F}


\global\long\def\N{\mathcal{N}}


\global\long\def\Po{\operatorname{Po}}


\global\long\def\Var{\operatorname{Var}}


\global\long\def\im{\operatorname{im}}


\global\long\def\sgn{\operatorname{sign}}


\global\long\def\supp{\operatorname{supp}}


\global\long\def\Pr{\mathbb{P}}


\global\long\def\m{\mu}


\global\long\def\X{X}


\global\long\def\bX{\mathbf{X}}


\global\long\def\A{A}


\global\long\def\P{\mathcal{P}}


For many combinatorial applications, an informal understanding of
probability theory (often considering only discrete spaces) will suffice.
However, in this thesis a rigorous foundation in probability theory
will be useful. This section will assume knowledge of basic measure
theory; see, for example, \cite{Rud66}. However, where possible,
we will note any simplifications that arise from assuming discreteness.

No knowledge of probability theory is assumed; the first few subsections
will briefly review the foundations of probability. The reader may
nevertheless want to refer to a probability theory book such as \cite{Kal02}
for some additional detail and further reading.


\subsection{Probability Spaces}
\begin{defn}
A \emph{probability space} is a measure space $\left(\Om,\cA,\Pr\right)$
with $\Pr\left(\Om\right)=1$. In this case we say $\Pr$ is a \emph{probability
measure}, and denote the set of all probability measures on $\left(\Om,\cA\right)$
by $\P\left(\Om,\cA\right)$ or $\P\left(\Om\right)$ if there is
no ambiguity.\end{defn}
\begin{rem}
\label{rem:discrete-space}For our purposes $\Om$ will often be countable,
with $\cA$ as the power set of $\Om$. In this case $\Pr$ is uniquely
defined by the probabilities $\Pr\left(\om\right):=\Pr\left(\left\{ \om\right\} \right)$,
for each $\om\in\Om$. We will discuss specific probability spaces
on sets of combinatorial objects in \ref{sub:random-structures}.\end{rem}
\begin{defn}
An \emph{event} in a probability space $\left(\Om,\cA,\Pr\right)$
is a measurable set $\A\in\cA$.
\end{defn}
For an event $\A$, we interpret $\Pr\left(\A\right)$ as the ``probability
that $\A$ occurs''. Events will usually be of the form $\A=\left\{ \om\in\Om:\, P\left(\om\right)\mbox{ holds}\right\} $,
where $P\left(\om\right)$ is some property of an object $\om$. For
clarity, we often abuse notation slightly and write $\Pr\left(P\mbox{ holds}\right)$
instead of $\Pr\left(\left\{ \om\in\Om:\, P\left(\om\right)\mbox{ holds}\right\} \right)$.


\subsection{Random Elements}
\begin{defn}
A \emph{random element} is a measurable function $\X:\left(\Om_{1},\cA_{1}\right)\to\left(\Om_{2},\cA_{2}\right)$
between measurable spaces. If $\Om_{2}=\RR^{n}$ for some $n\in\NN$,
with $\cA_{2}$ the Borel $\sigma$-algebra $\mathcal{B}\left(\RR^{n}\right)$
on $\RR^{n}$, then we say $\X$ is a \emph{random vector}. A one-dimensional
random vector is a \emph{random variable}. If $\Om_{2}$ is countable
then we say $\X$ is \emph{discrete}.\end{defn}
\begin{rem}
Especially in combinatorial spaces, $\Om_{1}$ is often countable.
In this case, any function from a probability space $\left(\Om_{1},2^{\Om_{1}},\Pr\right)$
is measurable.
\end{rem}
To interpret a random variable, we need a probability measure $\Pr$
on the underlying measurable space $\left(\Om_{1},\cA_{1}\right)$
(often, this will be implicit). Then, $\Pr\left(\X\in\A\right)=\Pr\left(\left\{ \om\in\Om_{1}:\X\left(\om\right)\in\A\right\} \right)$
is the probability that $\X$ takes a value in the set $\A$. Often,
we will only be interested in such probabilities: that is, we do not
care about the realization of a random variable as a function on an
underlying probability space. This motivates the following definition:
\begin{defn}
Suppose $\X$ is a random element which takes values in the measurable
space $\left(\Om,\cA\right)$. The \emph{distribution} (or \emph{law})
$\L{\X}$ of $\X$ with respect to an underlying probability $\Pr$
is the pushforward measure with respect to $\X$. That is, it is a
probability measure defined by $\L{\X}\left(\A\right)=\Pr\left(\X^{-1}\left(\A\right)\right)$
for $\A\subseteq\cA$. Also, we occasionally use the notation $\Lb{\X}:=\L{\X}$
for ease of reading.
\end{defn}
It is worth noting that in fact any probability measure is the distribution
of some random element, so we can define a probability distribution
abstractly and then assert the existence of a random variable with
that distribution. To see this, note that given a probability measure
$\L{}\in\P\left(\Om\right)$, we can choose $\X=\id_{\Om}$ to have
$\L{\X}=\L{}$ with respect to the underlying probability measure
$\L{}$. We use the notation $\X\in\L{}$ to indicate that $\X$ has
distribution $\L{}$.
\begin{example}
The \emph{normal distribution} with parameters $\mu$ and $\sigma$
is denoted $\N\left(\mu,\sigma\right)$ or $\N_{\mu,\sigma}$ and
is defined by 
\[
\N_{\mu,\sigma}\left(B\right)=\frac{1}{\sigma\sqrt{2\pi}}\int_{B}e^{-\frac{\left(x-\mu\right)^{2}}{2\sigma^{2}}}\d x
\]
for any Borel set $B\in\mathcal{B}\left(\RR\right)$. We can use the
Gaussian integral to show that this is indeed a probability measure,
with $\N_{\mu,\sigma}\left(\RR\right)=1$.

The normal distribution will be of particular importance in this thesis,
so we make some remarks. Note that if $\X\in\N\left(\mu,\sigma\right)$
then $\E\X=\mu$ and $\Var\X=\sigma^{2}$ (this second fact can be
established using integration by parts and the Gaussian integral).
Also, if $\X\in\N\left(\mu,\sigma\right)$ then $\frac{\X-\mu}{\sigma}\in\N\left(0,1\right)$.
So, we will sometimes only consider the \emph{standard normal} distribution
$\N\left(0,1\right)$, with the understanding that it is very easy
to transfer results to other normal distributions.
\end{example}

\begin{example}
The \emph{Poisson distribution} with parameter $\lambda$ is denoted
$\Po\left(\lambda\right)=\Po_{\lambda}$; this is defined by 
\[
\Po_{\lambda}\left(k\right)=\frac{\lambda^{k}e^{-\lambda}}{k!}
\]
for all $k\in\NN$ (see \ref{rem:discrete-space}). The Taylor expansion
of the exponential can be used to show that this indeed defines a
probability measure.
\end{example}
It is in general a little tricky to define ``the set of values a
random element can take'', but this is straightforward in the discrete
case:
\begin{defn}
The \emph{support} of a discrete random element $\X$ is the set 
\[
\supp\left(\X\right)=\left\{ k\in\Om:\Pr\left(\X=k\right)>0\right\} .
\]

\end{defn}

\subsection{Dependence and Coupling}
\begin{defn}
Suppose $\X$ and $\X'$ are random elements $\left(\Om_{1},\cA_{1},\Pr\right)\to\left(\Om_{2},\cA_{2}\right)$.
We say that $\X$ and $\X'$ are \emph{independent} if 
\[
\Pr\left(\X\in\A_{2}\right)\Pr\left(\X'\in\A_{2}\right)=\Pr\left(\X\in\A_{2}\mbox{ and }\X'\in\A_{2}\right)
\]
for all $\A_{2}\in\cA_{2}$. If $\A_{1},\A_{1}'\in\cA$ then we analogously
say $\A_{1}$ and $\A_{1}'$ are independent if 
\[
\Pr\left(\A_{1}\right)\Pr\left(\A_{1}'\right)=\Pr\left(\A_{1}\cap\A_{1}'\right).
\]
We can similarly say an event is independent of a random element.
If two objects are not independent, then we say they are \emph{dependent}.
\end{defn}
Intuitively, two objects are dependent if information about one object
can give information about the other. For example, we might be interested
in the probability of an event $\A$, under the assumption that the
event $\A'$ occurs.
\begin{defn}
The \emph{conditional probability} of an event $\A$, given an event
$\A'$ which has nonzero probability, is $\Pr\left(\A|\A'\right)=\Pr\left(\A\cap\A'\right)/\Pr\left(\A'\right)$.
\end{defn}
Note that $\Pr\left(\A|\A'\right)=\Pr\left(\A\right)$ if and only
if $\A$ and $\A'$ are independent.

We can also condition random elements on an event.
\begin{defn}
Suppose $\X:\left(\Om_{1},\cA_{1},\Pr\right)\to\left(\Om_{2},\cA_{2}\right)$
is a random element, and $\A_{1}\in\cA_{1}$ is an event with nonzero
probability. Then the \emph{distribution of $\X$ conditioned on $\A_{1}$}
is denoted by $\L{\X|\A_{1}}$ and defined by $\L{\X|\A_{1}}\left(\A_{2}\right)=\Pr\left(\X\in\A_{2}|\A_{1}\right)$
for $\A_{2}\in\cA_{2}$.
\end{defn}
Given a finite collection of measure spaces $\left(\Om_{1},\cA_{1},\m_{1}\right),\dots,\left(\Om_{n},\cA_{n},\m_{n}\right)$,
recall the construction of the product measure space $\left(\Om,\cA,\m\right):=\left(\prod_{i=1}^{n}\Om_{i},\,\bigotimes_{i=1}^{n}\cA_{i},\,\prod_{i=1}^{n}\m_{i}\right)$
(see \cite[Chapter~8]{Rud66}). If a random element takes values in
a product space then each component is measurable, and conversely
if the components of a random tuple are measurable then that tuple
is measurable in the product space. So, we can make the following
definitions:
\begin{defn}
Given random elements $\X_{1},\dots,\X_{n}$ on the same underlying
probability space, $\Lb{\X_{1},\dots,\X_{n}}:=\Lb{\left(\X_{1},\dots,\X_{n}\right)}$
is called the \emph{joint distribution} of $\X_{1},\dots,\X_{n}$.
Conversely, given a random tuple $\left(\X_{1},\dots,\X_{n}\right)$,
each $\Lb{\X_{\i}}$ is called a \emph{marginal distribution}.
\end{defn}
Suppose we have two distributions of random elements $\Lb{\X_{1}}$
and $\Lb{\X_{2}}$. \emph{Coupling} is the technique of constructing
a random ordered pair $\left(\X_{1},\X_{2}\right)$ which realizes
the given distributions as marginal distributions. Usually this is
done by specifying the joint distribution $\Lb{\X_{1},\X_{2}}$.

The idea is that coupling creates a particular kind of dependence
between $\X_{1}$ and $\X_{2}$ that allows us to compare the two
distributions. Often, we are able to make conclusions about the distributions
$\Lb{\X_{\i}}$ which are independent of their specific realizations
as random elements in the coupling.
\begin{example}
Let $X$ satisfy $\L X\left(0\right)=\frac{1}{2}$ and $\L X\left(1\right)=\frac{1}{2}$,
and let $Y$ satisfy $\L Y\left(0\right)=\frac{1}{3}$ and $\L Y\left(1\right)=\frac{2}{3}$.
Then we can couple $X$ and $Y$ into a distribution $\L{X,Y}$ on
$\left\{ 0,1\right\} ^{2}$, defined by
\[
\L{X,Y}\left(0,0\right)=\frac{1}{3},\quad\L{X,Y}\left(0,1\right)=\frac{1}{6},\quad\L{X,Y}\left(1,0\right)=0,\quad\L{X,Y}\left(1,1\right)=\frac{1}{2}.
\]
In this coupling, whenever $Y=0$ we have $X=0$, which allows us
to make the observation $\L X\left(0\right)\ge\L Y\left(0\right)$.
Of course, this was already obvious, but hopefully this example illustrates
the principle that coupling random variables allows us to make direct
comparisons between their distributions.
\end{example}

\subsection{Expected Value}
\begin{defn}
The \emph{expected value} of a random variable $\X$ (or its distribution)
is the measure-theoretic integral $\E\X=\int x\d\L{\X}\left(x\right)$.
It can be intuitively understood as the ``average'' value that random
variable takes. The \emph{variance} of $\X$ is a measure of how much
variation there is from the expected value; it is defined by $\Var\X=\E\left(\X-\E\X\right)^{2}=\E\X^{2}-\left(\E\X\right)^{2}$.\end{defn}
\begin{rem}
\label{rem:discrete-expectation}For a random variable $\X$ that
takes integer values, our definition of expected value is equivalent
to the well-known formula $\E\X=\sum_{x\in\ZZ}x\,\Pr\left(\X=x\right)$.
\end{rem}

\begin{rem}
\label{rem:indicator-expectation}If $\X$ is a random variable that
can be interpreted as counting the number of objects that satisfy
some property, then we can express $\X$ as a sum of indicator variables
$\sum_{\i}\one_{\A_{\i}}$, where $\A_{\i}$ is the event that the
$\i$th object satisfies our property. Noting that $\E$ is linear,
we have $\E\X=\sum_{\i}\E\one_{\A_{\i}}=\sum_{\i}\Pr\A_{\i}$. So,
in order to compute the expectation of $\X$ we just need to compute
the probability that each object satisfies our required property.
\end{rem}
If we fix a particular underlying probability space $\left(\Om,\cA,\Pr\right)$,
we can also equivalently view expectation as a linear functional on
the space of real integrable functions (i.e. random variables). That
is, $\E:L^{1}\left(\Om,\Pr\right)\to\RR$ satisfies $\E f=\int f\left(\om\right)\d\Pr$.
Sometimes we will define a new probability measure on an existing
measurable space $\left(\Om,\cA\right)$. In this case we will write
$\E_{\Pr'}$ to indicate expectation with respect to the measure $\Pr'$,
to avoid ambiguity. We can also define the expectation functional
of a random variable: $\E_{\X}:=\E_{\Lb{\X}}$. We have $\E_{\X}f=\E f\left(\X\right)$
for all $f:L^{1}\left(\RR,\Lb{\X}\right)$.

\begin{comment}

If $\iota$ is the natural injection that takes $r\in\RR$ to the
constant function $x\mapsto r$, and $I$ is the identity operator
on $L^{1}\left(\Om,\Pr\right)$, then $\iota\E$ and $I-\iota\E$
are both projection operators on $L^{1}\left(\Om,\Pr\right)$. In
particular

\end{comment}

In fact, probability measures are uniquely determined by their expectation
functional, because $\E_{\Pr}\one_{\A}=\Pr\left(\A\right)$ for all
events $\A$. It will be an important fact for later that much weaker
classes than $\left\{ \one_{\A}:\A\mbox{ is an event}\right\} $ can
distinguish expectation operators.

\global\long\def\cH{\mathcal{H}}


\global\long\def\h{h}

\begin{defn}
\label{def:determining-class}A set of real functions $\cH$ is a
\emph{determining class} if $\rest{\E_{\Pr_{1}}}{\cH}=\rest{\E_{\Pr_{2}}}{\cH}$
implies $\Pr_{1}=\Pr_{2}$, for any $\Pr_{1},\Pr_{2}\in\P\left(\RR\right)$.
\end{defn}
\begin{comment}
\begin{defn}
The \emph{variance }of a random variable $\X$ is $\Var\X=\E\left(\X-\E X\right)^{2}$.
The \emph{$n$th moment} of a random variable $\X$ is $\E\left[\X^{n}\right]$.
\end{defn}
conditional variance?

\end{comment}

Another important concept later will be the idea of conditional expectation.
\begin{defn}
Let $\X$ be a random variable and $\A$ be an event with nonzero
probability. The expected value of the distribution $\L{\X|\A_{1}}$
is called the \emph{conditional expected value of $\X$ given $\A_{1}$}
and is denoted $\E\left[\X|\A_{1}\right]$.
\end{defn}
We can also define conditional expectation with respect to another
random variable. If $\X_{1}$ and $\X_{2}$ are random variables defined
on the same underlying probability space $\left(\Om,\cA,\Pr\right)$,
then the sets $\X_{2}^{-1}\left(B\right)$ for $B\in\mathcal{B}\left(\RR\right)$
comprise a sub-$\sigma$-algebra $\sigma\left(\X_{2}\right)$ of $\cA$.
Then, $\m:\A'\mapsto\E\left[\X_{1}\one_{\A'}\right]$ is a signed
measure on $\sigma\left(\X_{2}\right)$ that is absolutely continuous
with respect to the restriction of $\Pr$ to $\cA'$. By the Radon-Nikodym
\textbf{t}heorem (see \cite[Theorem~6.10]{Rud66}) there is an $\cA'$-measurable
random variable $\E\left[\X_{1}|\X_{2}\right]$ that satisfies $\E\left[\X_{1}\one_{\A'}\right]=\E\left[\E\left[\X_{1}|\X_{2}\right]\one_{\A'}\right]$
for all $\A'$ in $\cA'$. This random variable is almost uniquely
defined: for any two choices of $\E\left[\X_{1}|\X_{2}\right]$, the
probability that they differ is zero.
\begin{defn}
The random variable $\E\left[\X_{1}|\X_{2}\right]$ as defined above
is called the \emph{conditional expectation of $\X_{1}$ with respect
to $\X_{2}$}. We can also view conditional expectation as a linear
operator between functions: we define 
\[
\E^{\X_{2}}\,:\, L^{1}\left(\RR,\mathcal{B}\left(\RR\right),\Pr\right)\to L^{1}\left(\RR,\sigma\left(\X_{2}\right),\Pr\right)\,:\,\X_{1}\mapsto\E\left[\X_{1}|\X_{2}\right].
\]
\end{defn}
\begin{rem}
This definition generalizes the previous definition of expectation
conditioned on an event: for almost all $\om\in\A$ we have $\E\left[\X|\one_{\A}\right]\left(\om\right)=\E\left[\X|\A\right]$.
\end{rem}

\begin{rem}
\label{rem:discrete-conditional-expectation}Note that if $\X_{2}$
is discrete then we do not need to invoke Radon-Nikodym. We can define
$\E\left[\X_{1}|\X_{2}\right]$ by 
\[
\E\left[\X_{1}|\X_{2}\right]\left(\om\right)=\E\left[\X_{1}|\X_{2}=\X_{2}\left(\om\right)\right]
\]
for all $\om\in\Om$ with $\Pr\left(\X_{2}=\X_{2}\left(\om\right)\right)>0$.
This defines $\E\left[\X_{1}|\X_{2}\right]$ up to a set of probability
zero.
\end{rem}
We finally present some simple consequences of the definition of conditional
expectation.
\begin{prop}
[Tower Law of Expectation]\label{prop:tower-contractivity}Suppose
$\X_{1}$, $\X_{2}$ and $\X_{3}$ are random variables defined on
the same underlying probability space $\left(\Om,\cA\right)$. Further,
suppose $\X_{3}$ is $\sigma\left(\X_{2}\right)$-measurable (intuitively,
this means $\X_{3}$ is dependent only on the information in $\X_{2}$).
Then\begin{enumerate}

\item (Tower Law) $\E^{\X_{3}}\left[\E^{\X_{2}}\X_{1}\right]=\E^{\X_{3}}\left[\X_{1}\right]$.
In particular, if we take $\X_{3}=\one_{\Om}$ then it follows that
$\E\left[\E^{\X_{2}}\X_{1}\right]=\E\left[\X_{1}\right]$.

\item (Contractivity) $\E\left|\E^{\X_{2}}\X_{1}\right|\le\E\left|\X_{1}\right|$.

\end{enumerate}\end{prop}
\begin{proof}
Recall the definition of conditional expectation. For all $\A\in\sigma\left(\X_{3}\right)\subseteq\sigma\left(\X_{2}\right)$
we have
\[
\E\left[\E\left[\X_{1}|\X_{3}\right]\one_{\A}\right]=\E\left[\X_{1}\one_{\A}\right]=\E\left[\E\left[\X_{1}|\X_{2}\right]\one_{\A}\right].
\]
Since $\E\left[\X_{1}|\X_{3}\right]$ is $\sigma\left(\X_{3}\right)$-measurable,
the tower law is proved. Next, let $\A=\left\{ \E^{\X_{2}}\X_{1}\ge0\right\} $,
so
\[
\E\left|\E^{\X_{2}}\X_{1}\right|=\E\left[\one_{\A}\E^{\X_{2}}\X_{1}\right]-\E\left[\one_{\Om\backslash\A}\E^{\X_{2}}\X_{1}\right]=\E\left[\one_{\A}\X_{1}\right]-\E\left[\one_{\Om\backslash\A}\X_{1}\right]\le\E\left|\X_{1}\right|,
\]
proving contractivity.\end{proof}
\begin{rem}
Suppose $\X_{1}$ and $\X_{2}$ are discrete. Combining \ref{rem:discrete-expectation}
and \ref{rem:discrete-conditional-expectation}, the tower law says
that
\[
\E\left[\X_{1}\right]=\sum_{x\in\supp\X_{2}}\Pr\left(\X_{2}=x\right)\E\left[\X_{1}|\X_{2}=x\right].
\]
We will occasionally use this in applications.
\end{rem}

\subsection{Markov Chains\label{sub:markov-chains}}

\begin{comment}

I'll need to define Markov Chains, stationary distributions, irreducibility
and time-reversibility.

Perhaps I should talk more generally about stochastic processes, because
applying exchangeable pairs to Stein's method is has connections with
Ornstein-Uhlenbeck processes and also Stein's method can be applied
to Poisson processes.

\end{comment}

We discuss Markov \textbf{c}hains only superficially, to the extent
that is required for \ref{sec:exchangeable-pairs}. The theory of
Markov \textbf{c}hains (and Markov \textbf{p}rocesses in general)
is a very rich area of probability theory, see for example \cite[Chapter~7]{Kal02}.

For our purposes, a (two-sided) \emph{Markov \textbf{c}hain} is
a sequence of discrete random elements $\left(X_{\i}\right)_{\i\in\ZZ}$
taking values on a common set $\Om$ (the \emph{state space}), such
that
\[
\Lb{X_{\i+1}|X_{\i}=x_{\i},X_{\i-1}=x_{\i-1},\dots}=\Lb{X_{\i+1}|X_{\i}=x_{\i}}
\]
for all $\i\in\ZZ$ and $x_{\i},x_{\i-1},\dots\in\Om$. We can think
of $\left(X_{\i}\right)_{\i\in\ZZ}$ as a process evolving in time
in such a way that, given the current state, the next state does not
depend on any other past states.
\begin{defn}
A Markov \textbf{c}hain $\left(X_{\i}\right)_{\i\in\ZZ}$ is \emph{time-homogeneous}
if
\begin{equation}
\Lb{X_{\i+1}|X_{\i}=x}=\Lb{X_{\i}|X_{\i-1}=x}\label{eq:time-homogeneous-defn}
\end{equation}
for all $x\in\Om$, $\i\in\ZZ$.
\end{defn}
As the name suggests, time-homogeneity means the evolution of the
Markov \textbf{c}hain does not depend on the current point in time,
given the current state. The probability in \ref{eq:time-homogeneous-defn}
is called the \emph{transition probability} from $x$ to $y$, and
is denoted $p_{xy}$ or $p\left(x,y\right)$.
\begin{defn}
A time-homogeneous Markov \textbf{c}hain $\left(X_{\i}\right)_{\i\in\ZZ}$
is \emph{stationary} with respect to the distribution $\Lb X$ if
$\Lb{X_{\i}}=\Lb X$ for all $\i\in\ZZ$.
\end{defn}
Define $\pi_{x}=\Pr\left(X=x\right)$. Note that if $\left(X_{\i}\right)_{\i\in\ZZ}$
is stationary with respect to $\Lb X$, then we must have 
\begin{equation}
\pi_{y}=\Pr\left(X_{\i}=y\right)=\sum_{x\in\Om}\Pr\left(X_{\i}=y,\, X_{\i-1}=x\right)=\sum_{x\in\Om}\pi_{x}p_{xy}\label{eq:stationary-defn}
\end{equation}
for all $y\in\Om$. In fact, the distribution $\Lb{X_{\i}}_{\i\in\ZZ}$
of a stationary Markov \textbf{c}hain is uniquely defined by a stationary
distribution $\L X$ and a set of transition probabilities $\left\{ p_{xy}\right\} _{x,y\in\Om}$
which satisfy \ref{eq:stationary-defn} for all $y\in\Om$ and $\sum_{y\in\Om}p_{xy}=1$
for all $x\in\Om$.
\begin{defn}
A stationary Markov \textbf{c}hain $\left(X_{\i}\right)_{\i\in\ZZ}$
is \emph{reversible} if the distribution of the sequence $\left(X_{\i}\right)_{\i\in\ZZ}$
is the same as the distribution of the sequence $\left(X_{-\i}\right)_{\i\in\ZZ}$ 
\end{defn}
Reversibility means that a Markov\textbf{ c}hain can be equally well
understood as evolving forward or backward in time. As before, let
$\pi_{x}=\Pr\left(X=x\right)$ for the stationary distribution $\L X$.
Then, reversibility is equivalent to the condition $\pi_{x}p_{xy}=\pi_{y}p_{yx}$
for all $x,y\in\Om$.
\begin{defn}
A time-homogeneous Markov \textbf{c}hain $\left(X_{\i}\right)_{\i\in\ZZ}$
is \emph{irreducible} if for all $x,y\in\Om$ there is $t\in\ZZ$
such that $\Pr\left(X_{t}=y|X_{0}=x\right)>0$.
\end{defn}
Irreducibility means that every state is accessible from every other.
This is equivalent to the condition that for any $x,y\in\Om$, there
is a finite sequence $x=x_{0},x_{1},\dots,x_{k}=y$ with $p\left(x_{\i},x_{\i+1}\right)>0$
for all $\i<k$.
\begin{prop}
\label{prop:graph-to-markov}Let $G$ be a finite connected edge-weighted
multigraph. Suppose that each edge weight is positive, and the sum
of the weights at each vertex is $\Delta$. Then $G$ defines a reversible
Markov \textbf{c}hain on its vertex set $V$, which is stationary
with respect to the uniform distribution on $V$.\end{prop}
\begin{proof}
For $x,y\in V$, let $p_{xy}=p_{yx}$ be the sum of the weights of
all edges between $x$ and $y$, divided by $\Delta$. By construction,
$\sum_{y\in\Om}p_{xy}=1$. Let $\L X$ be the uniform distribution
on $V$, and let $\pi_{x}=\L X\left(x\right)$. Then,
\[
\pi_{y}=\frac{1}{\left|V\right|}=\frac{1}{\left|V\right|}\sum_{x\in V}p_{xy}=\sum_{x\in V}\pi_{x}p_{xy}
\]
for all $y\in V$, and also
\[
\pi_{x}p_{xy}=\frac{1}{\left|V\right|}p_{xy}=\pi_{y}p_{yx}
\]
for each $x,y\in V$. We conclude that the transition probabilities
$\left\{ p_{xy}\right\} _{x,y\in V}$ define a reversible Markov \textbf{c}hain
stationary with respect to $\L X$. \end{proof}
\begin{example}
Let $G$ be a cycle of length $n$, with a weight of 1 on each edge.
The corresponding Markov \textbf{c}hain is a random walk: at each
time-step, there is equal probability of moving one step clockwise
or counterclockwise around the cycle.
\end{example}

\subsection{\label{sub:weak-topology}The Weak Topology on Probability Measures}

\global\long\def\cH{\mathcal{H}}


\global\long\def\h{h}


\global\long\def\TV{\mathrm{TV}}


\global\long\def\BL{\mathrm{BL}}


\global\long\def\K{\mathrm{K}}


\global\long\def\W{\mathrm{W}}


We begin with a definition.
\begin{defn}
\label{def:weak-convergence}Let $\left(\X_{n}\right)_{n\in\NN}$
be a sequence of random variables, not necessarily on the same underlying
probability space. We say $\X_{n}$ \emph{converges in distribution}
to a random variable $\X$ if $\E f\left(\X_{n}\right)\to\E f\left(\X\right)$
for all bounded continuous functions $f:\RR\to\RR$. Alternatively,
we say $\Lb{\X_{n}}$ converges \emph{weakly} to $\Lb{\X}$, or simply
$\Lb{\X_{n}}\to\Lb{\X}$.
\end{defn}
This allows us to give a formal statement of the two limit theorems
mentioned in the introduction.
\begin{thm}
[Central \textbf limit \textbf theorem]Let $Q_{1},Q_{2},\dots\in\L Q$
be independent random variables, with $\E Q=0$ and $\E Q^{2}=\Var Q=1$.
Define 
\[
\X_{n}=\frac{1}{\sqrt{n}}\sum_{\i=1}^{n}Q_{i}.
\]
Then $\Lb{\X_{n}}\to\N\left(0,1\right)$.
\end{thm}

\begin{thm}
[Poisson \textbf limit \textbf theorem]For each $n$, let $Q_{n}^{\left(1\right)},\dots,Q_{n}^{\left(n\right)}\in\Lb{Q_{n}}$
be independent zero-one random variables, with $n\E Q_{n}=n\L{Q_{n}}\left(1\right)\to\lambda$.
Define 
\[
\X_{n}=\sum_{\i=1}^{n}Q_{n}^{\left(\i\right)}.
\]
Then $\Lb{\X_{n}}\to\Po\left(\lambda\right)$.
\end{thm}
\ref{def:weak-convergence} is only one of a number of equivalent
natural definitions for convergence in distribution.
\begin{defn}
The \emph{distribution function} $\F_{\X}$ of a random variable $\X$
is defined by\\
$\F_{\X}\left(x\right)=\Pr\left(\X\le x\right)$ for all $x\in\RR$.\end{defn}
\begin{thm}
\label{prop:dist}For a sequence of random variables $\left(\X_{n}\right)_{n\in\NN}$
and a random variable $\X$, the following are equivalent.

\begin{conditions}

\item \label{prop:dist-def}$\Lb{\X_{n}}\to\Lb{\X}$;

\item \label{prop:dist-liminf}$\liminf_{n\to\infty}\L{\X_{n}}\left(U\right)\ge\L{\X}\left(U\right)$
for every open $U\subseteq\RR$;

\item \label{prop:dist-F}$\F_{\X_{n}}\left(x\right)\to\F_{\X}\left(x\right)$
for all $x$ such that $\F_{\X}$ is continuous at $x$;

\item \label{prop:dist-levy}$\E e^{it\X_{n}}\to\E e^{it\X}$ for
all $t\in\RR$.

\end{conditions}
\end{thm}
The equivalence of \ref{prop:dist-def,prop:dist-liminf,prop:dist-F}
is (part of) a well-known and relatively elementary result called
the Portmanteau \textbf{t}heorem (\cite[Theorem~3.25]{Kal02}). The
equivalence of \ref{prop:dist-def,prop:dist-levy} is called \emph{L\'evy's
\textbf{c}ontinuity \textbf{t}heorem} (\cite[Theorem~4.3]{Kal02}).

If $\X$ and each $\X_{n}$ are integer random variables, then \ref{prop:dist-F}
reduces to the condition that $\Pr\left(\X_{n}=k\right)\to\Pr\left(\X=k\right)$
for all $k$. This characterization is usually used to prove the Poisson
\textbf{l}imit \textbf{t}heorem.

Classically, distributional convergence results are usually proved
with L\'evy's continuity theorem and Fourier analysis. For example,
this approach is used to prove the \textbf{c}entral \textbf{l}imit
\textbf{t}heorem. In combinatorial applications, convergence in distribution
is often proved with the ``method of moments'' (which is actually
the Fourier analytic method in disguise): under certain conditions,
$\Lb{\X_{n}}\to\Lb{\X}$ if $\E\X_{n}^{k}\to\E\X^{k}$ for all $k$.
Convergence in distribution can also sometimes be inferred from stronger
forms of convergence when $\X$ and all the $\X_{n}$ are coupled
to the same underlying space.

In functional analysis terms, note that expectation operators are
bounded linear functionals on the space of real bounded continuous
functions. Then, $\Lb{\X_{n}}\to\Lb{\X}$ just means that $\E_{\X_{n}}\to\E_{\X}$
in the weak-star topology. The subspace corresponding to the set of
probability distributions $\P\left(\RR\right)$ is confusingly called
the \emph{weak topology} on probability distributions.

Although $C_{b}\left(\RR\right)^{*}$ is not metrizable, the unit
ball (in operator norm) of $C_{b}\left(\RR\right)^{*}$ is in fact
metrizable (see \cite[Theorems~3.15~and~3.16]{Rud73}). Every expectation
functional $\E$ has unit operator norm because $\E\left|\h\right|\le\E\left|1\right|=1$
for $\h$ with unit uniform norm. So, the weak topology is metrizable.
\begin{defn}
\label{def:general-metrics}Let $\cH$ be a determining class of real
measurable ``test'' functions that are uniformly absolutely bounded.
Define $d_{\cH}:\P\left(\RR\right)^{2}\to\RR^{+}$ by 
\[
d_{\cH}\left(\Pr_{1},\Pr_{2}\right)=\sup_{\h\in\cH}\left|\E_{\Pr_{1}}\h-\E_{\Pr_{2}}\h\right|.
\]
For random variables $\X_{1},\X_{2}$, we write $d_{\cH}\left(\X_{1},\X_{2}\right)$
instead of $d_{\cH}\left(\Lb{\X_{1}},\Lb{\X_{2}}\right)$.\end{defn}
\begin{prop}
\label{prop:general-metric}Each $d_{\cH}$ as defined above is a
metric.\end{prop}
\begin{proof}
Since the functions in $\cH$ are bounded, $\E_{\Pr_{1}}\h$ is well-defined
for every $\h\in\cH$. Since the bound is uniform, the supremum in
the definition of $d_{\cH}$ is finite. It is immediate that $d_{\cH}$
is non-negative, symmetric and satisfies the triangle inequality.
Finally, $d_{\cH}\left(\Pr_{1},\Pr_{2}\right)=0$ implies that $\E_{\Pr_{1}}\h=\E_{\Pr_{2}}\h$
for all $\h\in\cH$. Since $\cH$ is a determining class, $\Pr_{1}=\Pr_{2}$.\end{proof}
\begin{defn}
\label{def:special-metrics}We define some special cases of $d_{\cH}$.\begin{itemize}

\item If $\cH_{\K}=\left\{ \one_{\left(-\infty,x\right]}:x\in\RR\right\} $
then $d_{\K}:=d_{\cH_{\K}}$ is called the \emph{Kolmogorov metric}.

\item Let $\cH_{\BL}$ be the set of functions $\h$ that are absolutely
bounded by 1 (that is, $\left|\h\left(x\right)\right|\le1$ for all
$x\in\RR$), and have Lipschitz constant at most 1 (that is, $\left|\h\left(x_{1}\right)-\h\left(x_{2}\right)\right|\le\left|x_{1}-x_{2}\right|$
for all $x_{1},x_{2}\in\RR$). Then, $d_{\BL}:=d_{\cH_{\BL}}$ is
called the \emph{Bounded Lipschitz metric}.

\item If $\cH_{\TV}$ is the set of functions $\one_{B}$ for Borel
$B$, then $d_{\TV}:=d_{\cH_{\TV}}$ is called the \emph{total variation
metric}.

\item If $\cH_{\W}$ is the set of functions with Lipschitz constant
at most 1, then $d_{\W}:=d_{\cH_{\W}}$ is called the \emph{Wasserstein}
\emph{metric}. However, since $\cH$ is not uniformly bounded, $d_{\W}$
is not strictly speaking a metric on $\P\left(\RR\right)$; the Wasserstein
metric can only be used to compare distributions with finite first
moment, as we will see below.\end{itemize}\end{defn}
\begin{prop}
\label{prop:metric}The ``metrics'' in \ref{def:special-metrics}
are actually metrics.\end{prop}
\begin{proof}
First, note that if $\X_{1}$ and $\X_{2}$ have finite first moment
then for all $\h\in\cH_{\W}$,
\[
\left|\E h\left(\X_{1}\right)-\E h\left(\X_{2}\right)\right|\le\left|\left(\E\left|\X_{1}\right|+h\left(0\right)\right)-\left(\E\left|\X_{2}\right|+h\left(0\right)\right)\right|\le\E\left|\X_{1}\right|+\E\left|\X_{2}\right|.
\]
So,
\[
d_{\W}\left(\X_{1},\X_{2}\right)<\infty.
\]
Recalling that every probability measure is the distribution of some
random variable, we have proved that $d_{\W}:\P\left(\RR\right)^{2}\to\RR^{+}$
is well-defined.

Now, by \ref{prop:general-metric} we just need to check that each
of $\cH_{\K}$, $\cH_{\BL}$, $\cH_{\TV}$, $\cH_{\W}$ are determining
classes. Let $\Pr_{1},\Pr_{2}\in\P\left(\RR\right)$ satisfy $\E_{\Pr_{1}}\h=\E_{\Pr_{2}}\h$
for each $\h$ in $\cH$.

If $\cH$ is $\cH_{\K}$ or $\cH_{\TV}$ then $\one_{\left(-\infty,x\right]}\in\cH$
for all $x\in\RR$ so 
\[
\Pr_{1}\left(\left(-\infty,x\right]\right)=\E_{\Pr_{1}}\one_{\left(-\infty,x\right]}=\E_{\Pr_{2}}\one_{\left(-\infty,x\right]}=\Pr_{2}\left(\left(-\infty,x\right]\right).
\]
Since $\left\{ \left(-\infty,x\right]:x\in\RR\right\} $ generates
the Borel $\sigma$-algebra $\mathcal{B}\left(\RR\right)$, it follows
that $\Pr_{1}=\Pr_{2}$. We have shown that $\cH_{\K}$ and $\cH_{\TV}$
are determining classes.

For $x\in\RR$ and $0<\varepsilon\le1$, let $\h_{x,\varepsilon}$
be the continuous function which takes the value 1 on the set $\left(-\infty,x\right]$,
takes the value 0 on the set $\left[x+\varepsilon,\infty\right)$,
and is linearly interpolated in the range $\left[x,x+\varepsilon\right]$.
Suppose $\cH$ is $\cH_{\BL}$ or $\cH_{\W}$, so that each $\varepsilon\h_{x,\varepsilon}\in\cH$.
It follows that $\E_{\Pr_{1}}\h_{x,\varepsilon}=\E_{\Pr_{2}}\h_{x,\varepsilon}$
for each $x\in\RR$ and $0<\varepsilon\le1$. For each $x\in\RR$,
note that $\h_{x,1/n}\to\one_{\left(-\infty,x\right]}$ pointwise,
and each $\h_{x,1/n}\le1$. By the \textbf{d}ominated \textbf{c}onvergence
\textbf{t}heorem (see \cite[Theorem~1.34]{Rud66}), 
\[
\Pr_{1}\left(\left(-\infty,x\right]\right)=\E_{\Pr_{1}}\one_{\left(-\infty,x\right]}=\lim_{n\to\infty}\E_{\Pr_{1}}\h_{x,1/n}=\lim_{n\to\infty}\E_{\Pr_{2}}\h_{x,1/n}=\E_{\Pr_{2}}\one_{\left(-\infty,x\right]}=\Pr_{2}\left(\left(-\infty,x\right]\right),
\]
so $\cH_{\BL}$ and $\cH_{\W}$ are determining classes, as above.\end{proof}
\begin{prop}
\label{prop:stronger-than-weak}The metrics in \ref{def:special-metrics}
are each stronger than the weak topology.\end{prop}
\begin{proof}
We show that $d_{\cH}\left(\X_{n},\X\right)\to0$ implies $\Lb{\X_{n}}\to\Lb{\X}$
for each $\cH$.

If $d_{\K}\left(\X_{n},\X\right)\to0$ or $d_{\TV}\left(\X_{n},\X\right)\to0$
then $\F_{\X_{n}}\to\F_{\X}$ uniformly, so certainly \ref{prop:dist-F}
of \ref{prop:dist} holds.

Now, suppose $d_{\BL}\left(\X_{n},\X\right)\to0$ (this will automatically
be true if $d_{\W}\left(\X_{n},\X\right)\to0$). Let $d_{n}=\sqrt{d_{\BL}\left(\X_{n},\X\right)}$
and recall the definition of $\h_{x,\varepsilon}$ from the proof
of \ref{prop:metric}. Since $d_{n}\h_{x,d_{n}}\in\cH_{\BL}$ for
each $n\in\NN$, we have 
\[
\E_{\X_{n}}\h_{x,d_{n}}-\E_{\X}\h_{x,d_{n}}\le d_{\BL}\left(\X_{n},\X\right)/d_{n}=d_{n}\to0
\]
 uniformly for $x\in\RR$. Now, note that 
\[
\F_{\X}\left(x-\varepsilon\right)\le\E_{\X}h_{x-\varepsilon,\varepsilon}\le\F_{\X}\left(x\right)\le\E_{\X}h_{x,\varepsilon}\le\F_{\X}\left(x+\varepsilon\right)
\]
 for any random variable $\X$. If $\F_{\X}$ is continuous at $x$
then 
\begin{align*}
\F_{\X_{n}}\left(x\right)-\F_{\X}\left(x\right) & \le\left(\E_{\X_{n}}\h_{x,d_{n}}-\E_{\X}\h_{x,d_{n}}\right)+\left(\F_{\X}\left(x+d_{n}\right)-\F_{\X}\left(x\right)\right)\to0,\\
\F_{\X_{n}}\left(x\right)-\F_{\X}\left(x\right) & \ge\left(\E_{\X_{n}}\h_{x-d_{n},d_{n}}-\E_{\X}\h_{x-d_{n},d_{n}}\right)+\left(\F_{\X}\left(x-d_{n}\right)-\F_{\X}\left(x\right)\right)\to0,
\end{align*}
so \ref{prop:dist-F} of \ref{prop:dist} holds.\end{proof}
\begin{thm}
\label{thm:BL-metrizes-weak}The bounded Lipschitz metric metrizes
the weak topology.
\end{thm}
We will need a small lemma to prove \ref{thm:BL-metrizes-weak}.
\begin{lem}
\label{lem:prokhorov}Let $S\subseteq\P\left(\RR\right)$ be compact
in the weak topology. For each $\varepsilon>0$, there is $k\in\NN$
such that $\sup_{\Pr\in S}\Pr\left(\left(-k,k\right)^{c}\right)\le\varepsilon$.\end{lem}
\begin{proof}
Fix $\varepsilon>0$. Suppose for the purpose of contradiction that
for all $k\in\NN$ there is $\Pr_{k}\in S$ with $\Pr_{k}\left(\left(-k,k\right)^{c}\right)>\varepsilon$.
Since $S$ is compact, there is a subsequence $\left(\Pr_{k_{n}}\right)_{n\in\NN}$
of $\left(\Pr_{k}\right)_{k\in\NN}$, and a measure $\Pr\in S$, with
$\Pr_{k_{n}}\to\Pr$ as $n\to\infty$. For each $k\in\NN$ we have
$k<k_{n}$ for sufficiently large $n$, so by \ref{prop:dist-liminf}
of \ref{prop:dist}, we have
\[
\Pr\left(\left(-k,k\right)\right)\le\liminf_{n\to\infty}\Pr_{k_{n}}\left(\left(-k,k\right)\right)\le\liminf_{n\to\infty}\Pr_{k_{n}}\left(\left(-k_{n},k_{n}\right)\right)\le1-\varepsilon.
\]
This is a contradiction because $\Pr\left(\left(-k,k\right)\right)=\E_{\Pr}\one_{\left(-k,k\right)}\to\E_{\Pr}1=1$
as $k\to\infty$, by the \textbf{d}ominated \textbf{c}onvergence \textbf{t}heorem
(see \cite[Theorem~1.34]{Rud66}).
\end{proof}

\begin{proof}
[Proof of \ref{thm:BL-metrizes-weak}]This proof is simplified from
a more general proof in \cite[Theorem~7.12]{Vil03} (this simplification
is in fact presented as an exercise).

Let $\Pr_{n}\to\Pr_{\infty}$ weakly, and suppose for the purpose
of contradiction that $d_{\BL}\left(\Pr_{n},\Pr_{\infty}\right)\not\to0$.
Then there is $\varepsilon>0$ and a subsequence $\left(\Pr_{k_{n}}\right)_{n\in\NN}$
with $d_{\BL}\left(\Pr_{k_{n}},\Pr_{\infty}\right)>2\varepsilon$
for all $n$. To simplify notation, redefine $\Pr_{n}$ to be $\Pr_{k_{n}}$
for each $n$ (we still have $\Pr_{n}\to\Pr_{\infty}$ weakly).

Now, by the definition of $d_{\BL}$, for each $n\in\NN$ there is
$\h_{n}=\h_{n}^{\left(0\right)}\in\cH_{\BL}$ with 
\[
\left|\E_{\Pr_{n}}\h_{n}-\E_{\Pr}\h_{n}\right|\ge d_{\BL}\left(\Pr_{n},\Pr\right)-\varepsilon>\varepsilon.
\]
For each $k\in\ZZ^{+}$, let $\cH_{\BL}^{\left(k\right)}=\left\{ \rest{\h}{\left[-k,k\right]}:\h\in\cH_{\BL}\right\} $.
By the Arzela-Ascoli \textbf{t}heorem (see \cite[Theorem~11.28]{Rud66}),
each $\cH_{\BL}^{\left(k\right)}$ is a compact subset of $C_{b}\left(\left[-k,k\right]\right)$
with the uniform norm. So, for each $k\in\ZZ^{+}$, we can inductively
choose a subsequence $\left(\h_{n}^{\left(k\right)}\right)_{n\in\NN}$
of $\left(\h_{n}^{\left(k-1\right)}\right)_{n\in\NN}$ such that $\rest{\h_{n}^{\left(k\right)}}{\left[-k,k\right]}$
converges uniformly to some $\h^{\left(k\right)}\in\cH_{\BL}^{\left(k\right)}$.

Note that $\rest{\h_{n}^{\left(n\right)}}{\left[-k,k\right]}\to\h^{\left(k\right)}$
uniformly for each $k\in\ZZ^{+}$, so that $\h_{n}^{\left(n\right)}$
converges pointwise to some function $\h:\RR\to\RR$ that satisfies
$\rest{\h}{\left[-k,k\right]}=\h^{\left(k\right)}$ for all $k\in\ZZ^{+}$.
Since $\h$ is bounded by 1 and has Lipschitz constant less than 1
on each $\left[-k,k\right]$, it follows that $\h\in\cH_{\BL}$ and
in particular $\h$ is bounded and continuous. Finally, note that
\begin{alignat*}{1}
\left|\E_{\Pr_{n}}\h_{n}-\E_{\Pr_{\infty}}\h_{n}\right| & \le\left|\E_{\Pr_{n}}\left[\left(\h_{n}-\h\right)\one_{\left[-k,k\right]}\right]\right|+\left|\E_{\Pr_{\infty}}\left[\left(\h_{n}-\h\right)\one_{\left[-k,k\right]}\right]\right|\\
 & \quad\quad+\left|\E_{\Pr_{n}}\left[\left(\h_{n}-\h\right)\one_{\left[-k,k\right]^{c}}\right]\right|+\left|\E_{\Pr_{\infty}}\left[\left(\h_{n}-\h\right)\one_{\left[-k,k\right]^{c}}\right]\right|\\
 & \quad\quad+\left|\E_{\Pr_{n}}\h-\E_{\Pr_{\infty}}\h\right|
\end{alignat*}
By the definition of weak convergence, there is $N\in\NN$ such that
\[
\left|\E_{\Pr_{n}}\h-\E_{\Pr_{\infty}}\h\right|\le\frac{\varepsilon}{5}
\]
for all $n>N$. Since the weak topology is metrizable, $\left\{ \Pr_{n}\right\} _{n\in\NN\cup\left\{ \infty\right\} }$
is compact so by \ref{lem:prokhorov}, there is $k\in\NN$ such that
\[
\E_{\Pr_{n}}\left[\left(\h_{n}-\h\right)\one_{\left[-k,k\right]^{c}}\right]\le2\Pr_{n}\left[\left(-k,k\right)^{c}\right]\le\frac{\varepsilon}{5}
\]
for $n\in\NN\cup\left\{ \infty\right\} $. Since $\left(\h_{n}^{\left(n\right)}\right)_{n\in\NN}$
is a subsequence of $\left(\h_{n}\right)_{n\in\NN}$ and $\h_{n}^{\left(n\right)}\one_{\left[-k,k\right]}$
converges uniformly to $\h\one_{\left[-k,k\right]}$, there is $n>N$
such that 
\[
\left\Vert \left(\h_{n}-\h\right)\one_{\left[-k,k\right]}\right\Vert _{\infty}\le\frac{\varepsilon}{5}.
\]
For this $n$ we have $\left|\E_{\Pr_{n}}\h_{n}-\E_{\Pr_{\infty}}\h_{n}\right|\le\varepsilon$.
This is a contradiction.
\end{proof}
\ref{prop:stronger-than-weak} tells us that our selection of ``special''
metrics are all ``consistent'' with weak convergence, and in particular
\ref{thm:BL-metrizes-weak} tells us that convergence in the Bounded
Lipschitz metric is exactly the same as weak convergence. Typically,
it will be natural to work with the total variation metric for Poisson
approximation, and to work with the Wasserstein metric for Normal
approximation. In applications, we may be most interested in the Kolmogorov
metric. Therefore, it is sometimes useful to transfer results between
metrics (though, this usually results in worse constants than working
directly in the desired metric).

\begin{comment}It may be worthwhile to further discuss the Wasserstein,
Kolmogorov and Total Variation topologies. In particular, Wasserstein
convergence is just weak convergence plus convergence of the first
moment.\end{comment}
\begin{defn}
If (for all $x\in\RR$), $\F_{\X}\left(x\right)=\int_{-\infty}^{x}f_{\X}\left(x\right)\d x$
for some $f_{\X}$, then $f_{\X}$ is called the \emph{Lebesgue density}
of $\X$, and $\X$ is called a \emph{continuous} random variable.
\end{defn}
If $\X$ is a continuous random variable, then $\E_{\X}\h=\int_{\RR}\h\left(x\right)f_{\X}\left(x\right)\d x$
for measurable $\h$. (This is immediate from the measure-theoretic
definition of the integral).
\begin{prop}
\label{prop:transfer}Let $\X_{1},\X_{2}$ be random variables. Then\begin{enumerate}

\item \label{prop:transfer-K/TV}$d_{\K}\left(\X_{1},\X_{2}\right)\le d_{\TV}\left(\X_{1},\X_{2}\right)$;

\item \label{prop:transfer-BL/W}$d_{\BL}\left(\X_{1},\X_{2}\right)\le d_{\W}\left(\X_{1},\X_{2}\right)$;

\item \label{prop:transfer-K/W}If there is $C>0$ such that $\left|f_{\X_{2}}\left(x\right)\right|\le C$
for all $x$, then $d_{\K}\left(\X_{1},\X_{2}\right)\le\sqrt{2Cd_{\BL}\left(\X_{1},\X_{2}\right)}$.

\end{enumerate}\end{prop}
\begin{proof}
(Adapted from \cite[Proposition~1.2]{Ros11}). \ref{prop:transfer-K/TV,prop:transfer-BL/W}
are immediate from the definitions. Then, as in the proof of \ref{prop:stronger-than-weak},\vspace{-10pt}
\begin{eqnarray*}
\F_{\X_{n}}\left(x\right)-\F_{\X}\left(x\right) & \le & \left(\E_{\X_{n}}\h_{x,\varepsilon}-\E_{\X}\h_{x,\varepsilon}\right)+\left(\E_{\X}\h_{x,\varepsilon}-\F_{\X}\left(x\right)\right)\\
 & \le & d_{\BL}\left(\X_{1},\X_{2}\right)/\varepsilon+\int_{x}^{x+\varepsilon}\h_{x,\varepsilon}\, f_{\X}\left(x\right)\d x\\
 & \le & d_{\BL}\left(\X_{1},\X_{2}\right)/\varepsilon+C\varepsilon/2
\end{eqnarray*}
and similarly\vspace{-10pt}
\begin{eqnarray*}
\F_{\X_{n}}\left(x\right)-\F_{\X}\left(x\right) & \ge & -d_{\BL}\left(\X_{1},\X_{2}\right)/\varepsilon-C\varepsilon/2.
\end{eqnarray*}
So, we can take $\varepsilon=\sqrt{2d_{\BL}\left(\X_{1},\X_{2}\right)/C}$
to prove \ref{prop:transfer-K/W}.\end{proof}
\begin{example}
If $\Lb{\X_{2}}=\N\left(0,1\right)$ then $f_{\X_{2}}\left(x\right)=\left(2\pi\right)^{-1/2}e^{-x^{2}/2}$
so we can take $C=\left(2\pi\right)^{-1/2}$ to obtain $d_{\K}\le\left(2/\pi\right)^{1/4}\sqrt{d_{\BL}\left(\X_{1},\X_{2}\right)}$.
\end{example}
\begin{comment}In a combinatorial setting, many of our results are
about integer random variables. The total variation metric is usually
exclusively used in this case.
\begin{prop}
If $\X_{1},\X_{2}$ are integer-valued random variables, then
\[
d_{\TV}\left(\X_{1},\X_{2}\right)=\frac{1}{2}\sum_{k\in\ZZ}\left|\Pr\left(\X_{1}=k\right)-\Pr\left(\X_{2}=k\right)\right|.
\]
\end{prop}
\begin{proof}
For any Borel set $\A$, let $d_{\A}=\Pr\left(\X_{1}\in\A\right)-\Pr\left(\X_{2}\in\A\right)$,
so that $d_{\TV}\left(\X_{1},\X_{2}\right)=\sup_{\A}\left|d_{\A}\right|$.
Define 
\begin{align*}
\A_{<} & =\left\{ k\in\ZZ:\,\Pr\left(\X_{1}=k\right)<\Pr\left(\X_{2}=k\right)\right\} ,\\
\A_{>} & =\left\{ k\in\ZZ:\,\Pr\left(\X_{1}=k\right)>\Pr\left(\X_{2}=k\right)\right\} .
\end{align*}
For any Borel $\A$, we have 
\begin{align*}
d_{\A} & =\sum_{k\in\ZZ\cap\A}\left(\Pr\left(\X_{1}=k\right)-\Pr\left(\X_{2}=k\right)\right)\\
 & \le\sum_{k\in\A_{>}}\left(\Pr\left(\X_{1}=k\right)-\Pr\left(\X_{2}=k\right)\right)\\
 & =d_{\A_{>}}
\end{align*}
and similarly $-d_{\A}\ge d_{\A_{<}}$. Since $d_{\A_{>}}=-d_{\A_{<}}$,
we have 
\[
d_{\TV}\left(\X_{1},\X_{2}\right)=\left(d_{\A_{>}}-d_{\A_{<}}\right)/2=\frac{1}{2}\sum_{k\in\ZZ}\left|\Pr\left(\X_{1}=k\right)-\Pr\left(\X_{2}=k\right)\right|.
\]

\end{proof}
\end{comment}


\subsection{Random Combinatorial Structures\label{sub:random-structures}}

\global\long\def\rG#1#2{\mathcal{G}_{#1,#2}}


\global\long\def\rS#1{\mathcal{S}_{#1}}

\begin{defn}
In a probability space $\left(\Om,2^{\Om},\Pr\right)$ where $\Om$
is finite, if $\Pr\left(\om\right)=1/\left|\Om\right|$ for each $\om\in\Om$,
then we say the space is \emph{uniform}.
\end{defn}
Uniform spaces are the simplest examples of random structures. For
example, the uniform space $\rS n$ of permutations on $n$ elements
has $\Pr\left(\sigma\right)=1/n!$ for each $\sigma\in S_{n}$. The
uniform random regular graph model $\rG nd$ is uniform on the set
of all $d$-regular graphs on the vertex set $\range n$, though an
explicit formula for the number of such graphs is not known. (Recall
that a $d$-regular graph is a graph where each vertex has exactly
$d$ incident edges).

\begin{comment}The uniform random graph model $\rG nM$ has $\Pr\left(G\right)=\begin{pmatrix}{n \choose 2}\\
M
\end{pmatrix}^{-1}$ for each graph $G$ on the vertex set $\range n$ which has $M$
edges. \end{comment}

As an important example of a (generally) non-uniform model, the (Erd\H{o}s-R\'enyi)
binomial random graph model $\rG np$ has
\[
\Pr\left(G\right)=p^{\left|E\left(G\right)\right|}\left(1-p\right)^{{n \choose 2}-\left|E\left(G\right)\right|}
\]
for each graph $G$ on the vertex set $\range n$. When $p=1/2$,
we obtain the uniform model on all graphs on the vertex set $\range n$.
It is unfortunate that the common notations for the random regular
model and the binomial model clash, but hopefully in this thesis the
intended meaning will always be very clear from context.

One way to conceptualize the binomial model is to consider a sequence
of independent coin tosses, where the coin is biased to land heads
with probability $p$. Each coin toss corresponds to a particular
potential edge, and determines whether that edge is present in the
final random graph.

\begin{comment}As another example, the uniform model $\rG nM$ can
be alternatively defined recursively: $\rG n0$ is always the trivial
graph with no edges, and for each $M>0$, to obtain $\rG nM$ we choose
$G\in\rG n{M-1}$ and add one of the ${n \choose 2}-\left(M-1\right)$
possible edges at random.\end{comment}


\section{Stein's Method in Generality\label{sec:stein-general}}

There are a number of different presentations of Stein's method, with
slightly different assumptions and different (though mostly equivalent)
definitions. I have decided to describe Stein's original approach
\cite{Ste86} because, in particular, it does a better job motivating
the method of exchangeable pairs in \ref{sec:exchangeable-pairs}.

\global\long\def\Lo{\L 0}


\global\long\def\Eo{\E_{0}}


\global\long\def\cF{\mathcal{F}}


\global\long\def\Fo{\cF_{0}}


\global\long\def\cY{\mathcal{Y}}


\global\long\def\fo{f}


\global\long\def\T{T}


\global\long\def\To{\T_{0}}


\global\long\def\U{U}


\global\long\def\Uo{\U_{0}}


\global\long\def\cX{\mathcal{V}}


\global\long\def\Xo{\cX_{0}}


Suppose we have a potentially complicated random variable $\X$, and
we believe the distribution of $\X$ is close to a ``special'' distribution
$\Lo$. Then, Stein's method allows us to compare the operators $\E_{\X}$
and $\Eo:=\E_{\Lo}$. This is sometimes directly useful for approximating
statistics of $\X$ (for example, $\Pr\left(\X\in\A\right)=\E_{\X}\one_{\A}$).
However, particularly for combinatorial applications, Stein's method
is most often used to bound the distance $d_{\cH}\left(\L{\X},\Lo\right)$
for some $\cH$, where the metric $d_{\cH}$ from \ref{def:general-metrics}
is defined in terms of $\E_{\X}$ and $\Eo$.

Stein's method is motivated by the idea of a characterizing operator.
\begin{defn}
\label{def:characterizing}Let $\Fo$ be a vector space and $\Xo$
be a vector space of measurable functions which contains the constant
functions. We say a linear operator $\To:\Fo\to\Xo$ is a \emph{characterizing
operator} for the distribution $\Lo$ if $\im\To=\Xo\cap\ker\Eo$.
For convenience, where there is no ambiguity we will often implicitly
restrict $\Eo$ to $\Xo$, so we can write $\im\To=\ker\Eo$.
\end{defn}
The following proposition shows why $\To$ is called a characterizing
operator.
\begin{prop}
\label{prop:characterizing}Suppose $\To:\Fo\to\Xo$ is a characterizing
operator and $\Xo$ is a determining class. If some random variable
$\X$ satisfies $\im\To\subseteq\ker\E_{\X}$, then $\X\in\Lo$.\end{prop}
\begin{proof}
If $\h\in\Xo$, then $\h-\Eo\h\in\ker\Eo=\im\To$ so $\E_{\X}\left[\h-\Eo\h\right]=0$.
That is, $\E_{\X}\h=\Eo\h$ for all $\h\in\Xo$, which means $\L{\X}=\Lo$
by the definition of a determining class.
\end{proof}
\ref{prop:characterizing} should already give some clue as to how
Stein's method works. If $\E_{\X}\To\fo=0$ for all $\fo$ means $\L{\X}=\Lo$,
then hopefully it is true that $\E_{\X}\To\fo\approx0$ for all $\fo$
means $\L{\X}\approx\Lo$. In practice it is a little unwieldy to
show an operator is characterizing using \ref{def:characterizing},
so we'll prove an equivalent definition.
\begin{prop}
\label{prop:U_0-characterizing}$\To:\Fo\to\Xo$ is characterizing
if and only if there is a linear operator $\Uo:\Xo\to\Fo$ (called
a \emph{Stein transform}) such that the following two equations hold:
\begin{align}
\Eo\To & =0,\label{eq:E_0T_0-consistency}\\
\To\Uo+\Eo & =\id_{\Xo}.\label{eq:U_0-characterizing}
\end{align}
\end{prop}
\begin{proof}
Suppose $\To$ is a characterizing operator. Equation \ref{eq:E_0T_0-consistency}
is immediate. Let $\left\{ \h_{\i}\right\} _{\i\in\mathcal{I}}$ be
a (Hamel) basis of $\Xo$. For each $\i\in\mathcal{I}$ we have $\h_{\i}-\Eo\h_{\i}\in\ker\Eo$
so there is some $\fo_{\i}$ (not necessarily unique) that solves
$\To\fo_{\i}=\h_{\i}-\Eo\h_{\i}$. The operator $\Uo$ can then be
defined by $\sum_{\i\in\mathcal{I}}a_{\i}\h_{\i}\mapsto\sum_{\i\in\mathcal{I}}a_{\i}\fo_{\i}$,
satisfying \ref{eq:U_0-characterizing}.

Conversely, suppose \ref{eq:E_0T_0-consistency} holds and $\Uo$
exists satisfying \ref{eq:U_0-characterizing}. For $\h\in\ker\Eo$
we have $\To\left(\Uo\h\right)=\h$ and hence $\h\in\im\To$, so $\ker\Eo\subseteq\im\To$.
Equation \ref{eq:E_0T_0-consistency} immediately says that $\im\To\subseteq\ker\Eo$,
so $\To$ is a characterizing operator.\end{proof}
\begin{rem}
In full generality (and in accordance with \cite{Ste86}), we do not
require any topological structure on $\Fo$ and $\Xo$. The proof
of \ref{prop:U_0-characterizing} uses the \textbf{a}xiom of \textbf{c}hoice
for the existence of a basis, and does not ensure that the Stein transform
$\Uo$ is particularly well-behaved. In practice, we will usually
require that $\Uo$ is well-behaved when we apply Stein's method (in
fact, we will usually have an explicit formula for $\Uo$).
\end{rem}
\begin{comment}

I managed to prove a necessary and sufficient condition for $\Uo$
to be bounded if $\To$ is a Banach space operator (namely, $\ker\To$
is complementable). I think it's probably a little off-topic to include
this though.

\end{comment}

\begin{comment}
\begin{rem}
but an equivalent condition for existence of a bounded Stein transform
is that the kernel of $\To$ is complementable. That is, there is
a closed subspace $S\subseteq\Fo$ with $S\oplus\ker\To=\Fo$. For
example, this condition will always be satisfied if $\Fo$ is (isomorphic
to a) Hilbert space, by the existence of orthogonal complements. In
fact, Hilbert spaces are the only spaces where every closed subspace
is complementable \cite{LT71}.\end{rem}
\begin{proof}
[Proof of claim]First assume $\ker\To$ is complementable, with complement
$S$. Now, $\ker\Eo=\im\To$ is a closed (Banach) subspace of $\Xo$,
so $\To$ restricts to a linear operator $\T':S\to\im\To$ between
Banach spaces. By construction, $\T'$ is bijective so by the bounded
inverse theorem, $\T'$ has a bounded inverse $\U':\im\To\to S$.
This gives a bounded Stein transform
\[
\Uo:\Xo\to\Fo:\h\mapsto\U'\left(\h-\Eo\h\right).
\]
Conversely, if $\Uo$ is a bounded Stein transform for $\To$, let
$J=\id_{\Fo}-\Uo\To$. If $\fo\in\ker J\cap\ker\To$ then $\To\fo=0$
so $J\fo=\fo-\Uo0=\fo=0$. That is, $\ker J\cap\ker\To=\left\{ 0\right\} $.
For any $\fo\in\Fo$, we have $\fo=J\fo+\Uo\To\fo$. Now, 
\[
J\Uo\To=\Uo\To-\Uo\left(\id_{\Xo}-\Eo\right)\To=\Uo\To-\Uo\To+\Uo\Eo\To=0
\]
and 
\[
\To J=\To-\left(\id_{\Xo}-\Eo\right)\To=\To-\To+\Eo\To=0,
\]
so $J\fo\in\ker\To$ and $\Uo\To\fo\in\ker J$. That is, $\ker\To\oplus\ker J=\Fo$
and we conclude that $\ker\To$ is complementable.
\end{proof}
\end{comment}

Now, note that for any random variable $\X$ and any $\h\in\Xo$,
Equation \ref{eq:U_0-characterizing} allows us to to make the transformation
\begin{equation}
\E_{\X}\h=\Eo\h+\E_{\X}\To\Uo\h.\label{eq:stein-transformation}
\end{equation}
The original purpose of Stein's method was to estimate some particular
$\E_{\X}\h$. If $\Lo$ is a well-understood distribution then the
term $\Eo\h$ should be easy to compute or estimate, and if the distribution
of $\X$ was ``close'' to $\Lo$, then it should be possible to
show that the remainder $\E_{\X}\To\Uo\h$ is small.

For our purposes, the main use of \ref{eq:stein-transformation} is
to bound $d_{\cH}\left(\X,\Lo\right)$ for some $\cH\subseteq\Xo$.
For any $\cY\supseteq\Uo\cH$, we have 
\begin{equation}
d_{\cH}\left(\X,\Lo\right)=\sup_{\h\in\cH}\left|\E_{\X}\To\Uo\h\right|\le\sup_{\fo\in\cY}\left|\E_{\X}\To\fo\right|.\label{eq:stein-distance-transform}
\end{equation}
We have reduced the problem of bounding $d_{\cH}\left(\X,\Lo\right)$
to that of bounding $\left|\E_{\X}\To\fo\right|$ (uniformly over
$\fo\in\cY$). Especially in the cases where $\Lo$ is normal or Poisson
and $\cH$ is one of the standard choices in \ref{def:special-metrics},
there are a number of known convenient choices of $\cY$.

In the next two sections, we will present characterizing operators
for the normal and Poisson distributions, and some associated convenient
choices for $\cY$.


\section{The Normal Case\label{sec:normal}}

We'll use \ref{prop:U_0-characterizing} to give our first example
of a characterizing operator.
\begin{thm}
\label{thm:normal-characterizing}Let $\X_{\N}\in\N\left(0,1\right)$
and let 
\[
\cX_{\N}=\left\{ \h\mbox{ absolutely continuous}:\E\left[\left|\X_{\N}\right|^{k}\left|\h\left(\X_{\N}\right)\right|\right]<\infty\mbox{ for all }k\ge0\right\} .
\]
Let $\cF_{\N}$ be the set of absolutely continuous $\fo$ with $\fo'\in\cX_{\N}$.
(For a definition and characterization of absolute continuity, see
\cite[Definition~7.17 and Theorem~7.18]{Rud66}).

The linear operator $\T_{\N}:\cF_{\N}\to\cX_{\N}$ given by $\T_{\N}\fo\left(x\right)=\fo'\left(x\right)-x\fo\left(x\right)$
is a characterizing operator for $\N\left(0,1\right)$.\end{thm}
\begin{proof}
(Adapted from \cite[Lecture~II]{Ste86}). 

First, for each $k\ge0$ we have $e^{x^{2}/4}>x^{k}$ for sufficiently
large $x$, so 
\[
\int_{C}^{\infty}x^{k}e^{-x^{2}/2}\d x<\int_{C}^{\infty}e^{-x^{2}/4}\d x<\infty
\]
for some large $C$. It follows that $\E\left|\X_{\N}\right|^{k+1}<\infty$,
and $\cX_{\N}$ contains the constant functions.

Now, we prove that $\T_{\N}$ is well-defined as an operator with
codomain $\cX_{\N}$. Fix $\fo\in\cF_{\N}$ and $k\ge0$. We have
\begin{alignat}{1}
 & \E\left[\left|\X_{\N}\right|^{k}\left|\T_{\N}\fo\left(\X_{\N}\right)\right|\right]\nonumber \\
 & \quad\le\E\left[\left|\X_{\N}\right|^{k}\left|\fo'\left(\X_{\N}\right)\right|\right]+\E\left[\left|\X_{\N}\right|^{k+1}\left|\fo\left(\X_{\N}\right)-\fo\left(0\right)\right|\right]+\left|\fo\left(0\right)\right|\E\left[\left|\X_{\N}\right|^{k+1}\right].\label{eq:normal-well-defined-bound}
\end{alignat}
\begin{comment}Now, $\E\left|\X\right|^{a}\le\left(\E\left|\X\right|^{b}\right)^{a/b}$
for $a\le b$ by H\"older's inequality (see \cite[Theorem~3.6~(1)]{Rud66})
\begin{alignat*}{1}
\E\left|\X_{\N}\right|^{k+1} & \prec\int_{0}^{\infty}x^{k+1}e^{-x^{2}/2}\d x\\
 & \prec\int_{0}^{\infty}x^{k}\frac{\d}{\d x}e^{-x^{2}/2}\d x\\
 & \prec\left.x^{k}e^{-x^{2}/2}\right|_{0}^{\infty}-\int_{0}^{\infty}x^{k-1}e^{-x^{2}/2}\d x\\
 & \prec\int_{0}^{\infty}x^{k-1}e^{-x^{2}/2}\d x
\end{alignat*}
Proceeding inductively, we have $\E\left|\X_{\N}\right|^{k+1}\prec\int_{0}^{\infty}xe^{-x^{2}/2}\d x\prec\left.e^{-x^{2}/2}\right|_{0}^{\infty}<\infty$
if $k$ is even, or $\E\left|\X_{\N}\right|^{k+1}\prec\int_{0}^{\infty}e^{-x^{2}/2}\d x<\infty$
if $k$ is odd

Note that the function $x\mapsto x^{k+3}e^{-x^{2}/2}$ is decreasing
for sufficiently large $x$ (say for $x\ge C_{2}$), so 
\begin{alignat*}{1}
\int_{t}^{\infty}x^{k+1}e^{-x^{2}/2}\d x & \le t^{k+3}e^{-t^{2}/2}\int_{C_{2}}^{\infty}\frac{1}{x^{2}}\d x
\end{alignat*}
for $t\ge C_{2}$, and
\[
\int_{t}^{\infty}x^{k+1}e^{-x^{2}/2}\d x\le\E\left|\X_{\N}\right|^{k+1}e^{C_{2}^{2}/2}e^{-t^{2}/2}
\]
for $0\le t<C_{2}$. That is, there is $C_{3}$ with 
\[
\int_{t}^{\infty}x^{k+1}e^{-x^{2}/2}\d x\le C_{3}\left(1+t^{k+3}\right)
\]
for $t\ge0$.

\end{comment}

\begin{comment}

Stein had $\int_{t}^{\infty}x^{k}e^{-x^{2}/2}\d x\le C_{3}\left(1+t^{k}\right)$,
but I couldn't immediately see how to prove that, and this works too.

\end{comment}

The first term in \ref{eq:normal-well-defined-bound} is finite by
the definition of $\cF_{\N}$, and the third term is finite, as above.
We will now bound the second term. Note that 
\begin{alignat*}{1}
\int_{t}^{\infty}x^{k+1}e^{-x^{2}/2}\d x & \le e^{-t^{2}/2}\int_{0}^{\infty}\left(y+t\right)^{k+1}e^{-y^{2}/2}\d y\\
 & =O\left(e^{-t^{2}/2}\left(1+t+\dots t^{k+1}\right)\right)\\
 & =O\left(e^{-t^{2}/2}\left(1+t^{k+1}\right)\right),
\end{alignat*}
with the substitution $y=x-t$. With Fubini's theorem (see \cite[Theorem~8.8]{Rud66}),
\begin{alignat*}{1}
\E\left[\left|\X_{\N}\right|^{k+1}\left|\fo\left(\X_{\N}\right)\right|\right] & =O\left(\,\int_{0}^{\infty}x^{k+1}\left|\int_{0}^{x}\fo'\left(t\right)\d t\right|e^{-x^{2}/2}\d x\right.\\
 & \qquad\qquad\left.-\int_{-\infty}^{0}x^{k+1}\left|\int_{0}^{x}\fo'\left(t\right)\d t\right|e^{-x^{2}/2}\d x\right)\\
 & =O\left(\int_{0}^{\infty}\int_{t}^{\infty}x^{k+1}\left|\fo'\left(t\right)\right|e^{-x^{2}/2}\d x\d t\right.\\
 & \qquad\qquad\left.-\int_{-\infty}^{0}\int_{-\infty}^{t}x^{k+1}\left|\fo'\left(t\right)\right|e^{-x^{2}/2}\d x\d t\right)\\
 & =O\left(\int_{0}^{\infty}e^{-t^{2}/2}\left(1+t^{k+1}\right)\left|\fo'\left(t\right)\right|\d t\right.\\
 & \qquad\qquad\left.-\int_{-\infty}^{0}e^{-t^{2}/2}\left(1+t^{k+1}\right)\left|\fo'\left(t\right)\right|\d t.\right)\\
 & =O\left(\E\left[\left|\X_{\N}\right|^{0}\left|\fo'\left(\X_{\N}\right)\right|\right]+\E\left[\left|\X_{\N}\right|^{k+1}\left|\fo'\left(\X_{\N}\right)\right|\right]\right)\\
 & <\infty.
\end{alignat*}
It follows that $\E\left[\left|\X_{\N}\right|^{k}\left|\T_{\N}\fo\left(\X_{\N}\right)\right|\right]<\infty$
and $\T_{\N}\fo\in\cX_{\N}$.

Now, for any $\fo\in\cF_{\N}$, integration by parts gives
\begin{equation}
\E_{\N}\T_{\N}\fo=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-t^{2}/2}\fo'\left(t\right)\d t-\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}te^{-t^{2}/2}\fo\left(t\right)\d t=0\label{eq:normal-stein-identity}
\end{equation}
so $\E_{\N}\T_{\N}=0$ and \ref{eq:E_0T_0-consistency} holds.

Next, consider the differential equation
\begin{equation}
\T_{\N}\fo\left(x\right)=\fo'\left(x\right)-x\fo\left(x\right)=\h\left(x\right)-\E_{\N}\h\label{eq:diff-eqn}
\end{equation}
for $\fo$. We solve this equation with the method of integrating
factors:
\begin{alignat*}{1}
e^{-x^{2}/2}\fo'\left(x\right)-xe^{-x^{2}/2}\fo\left(x\right) & =e^{-x^{2}/2}\left(\h\left(x\right)-\E_{\N}\h\right),\\
\frac{\d}{\d x}e^{-x^{2}/2}\fo\left(x\right) & =e^{-x^{2}/2}\left(\h\left(x\right)-\E_{\N}\h\right).
\end{alignat*}
The general solution is 
\begin{equation}
\fo\left(x\right)=e^{x^{2}/2}\int_{-\infty}^{x}\left(\h\left(t\right)-\E_{\N}\h\right)e^{-t^{2}/2}\d t+ce^{x^{2}/2}.\label{eq:diff-eqn-soln}
\end{equation}


We can therefore define $\U_{\N}:\cX_{\N}\to\cF_{\N}$ by 
\begin{equation}
\U_{\N}\h\left(x\right)=e^{x^{2}/2}\int_{-\infty}^{x}\left(\h\left(t\right)-\E_{\N}\h\right)e^{-t^{2}/2}\d t=-e^{x^{2}/2}\int_{x}^{\infty}\left(\h\left(t\right)-\E_{\N}\h\right)e^{-t^{2}/2}\d t.\label{eq:U_N}
\end{equation}
(These two definitions are the same because their difference is $e^{x^{2}/2}\left(\E_{\N}\h-\E_{\N}\h\right)=0$).
By construction,
\begin{align*}
\T_{\N}\U_{\N}\h\left(x\right) & =\h\left(x\right)-\E_{\N}\h.
\end{align*}
So, \ref{eq:U_0-characterizing} holds. To apply \ref{prop:U_0-characterizing},
it remains to prove that $\U_{\N}$ is indeed well-defined as an operator
with codomain $\cF_{\N}$. Since $\U_{\N}\h$ satisfies \ref{eq:diff-eqn},
we have
\begin{alignat}{1}
\E\left[\left|\X_{\N}\right|^{k}\left|\left(\U_{\N}\h\right)'\left(\X_{\N}\right)\right|\right] & =\E\left[\left|\X_{\N}\right|^{k}\left|\X_{\N}\U_{\N}\h\left(\X_{\N}\right)+\h\left(\X_{\N}\right)-\E_{\N}\h\right|\right]\nonumber \\
 & \le\E\left[\left|\X_{\N}\right|^{k+1}\left|\U_{\N}\h\left(\X_{\N}\right)\right|\right]+\E\left[\left|\X_{\N}\right|^{k}\left|\h\left(\X_{\N}\right)\right|\right]+\E\left|\X_{\N}\right|^{k}\E_{\N}\h.\label{eq:normal-transform-well-defined-bound}
\end{alignat}
for $\h\in\cX_{\N}$. The first and third terms of \ref{eq:normal-transform-well-defined-bound}
are finite by the definition of $\cX_{\N}$ and the fact that it contains
the constant function $1$. Finally, for the second term, Fubini's
theorem gives
\begin{alignat*}{1}
\E\left[\left|\X_{\N}\right|^{k+1}\left|\U_{\N}\h\left(\X_{\N}\right)\right|\right] & =O\left(\int_{0}^{\infty}x^{k+1}\int_{x}^{\infty}\left|\h\left(t\right)-\E_{\N}\h\right|e^{-t^{2}/2}\d t\d x\right.\\
 & \qquad\qquad\left.-\int_{-\infty}^{0}x^{k+1}\int_{\infty}^{x}\left|\h\left(t\right)-\E_{\N}\h\right|e^{-t^{2}/2}\d t\d x\right)\\
 & =O\left(\int_{0}^{\infty}\left|\h\left(t\right)-\E_{\N}\h\right|e^{-t^{2}/2}\int_{0}^{t}x^{k+1}\d x\d t\right.\\
 & \qquad\qquad\left.-\int_{-\infty}^{0}\left|\h\left(t\right)-\E_{\N}\h\right|e^{-t^{2}/2}\int_{t}^{0}x^{k+1}\d x\d t\right)\\
 & =O\left(\int_{-\infty}^{\infty}\left|t\right|^{k+2}\left|\h\left(t\right)-\E_{\N}\h\right|e^{-t^{2}/2}\d x\d t\right)\\
 & =O\left(\E\left[\left|\X_{\N}\right|^{k+2}\left|\h\left(\X_{\N}\right)\right|\right]+\E\left[\left|\X_{\N}\right|^{k+2}\right]\right)<\infty.
\end{alignat*}
That is, $\E\left[\left|\X_{\N}\right|^{k}\left|\left(\U_{\N}\h\right)'\left(\X_{\N}\right)\right|\right]<\infty$,
so $\U_{\N}\h\in\cF_{\N}$.\end{proof}
\begin{rem}
Note that $\cH_{\W}\subseteq\cX_{\N}$, where $\cH_{\W}$ is as defined
in \ref{def:special-metrics}. Since $\cH_{\W}$ is a determining
class, $\T_{\N}$ is a characterizing operator in the sense of \ref{prop:characterizing}.
\end{rem}
Useful application of Stein's method depends on a good understanding
of the functions in $\U_{\N}\cH$. This will give us a set of functions
$\cY$ to maximize over, as in \ref{eq:stein-distance-transform}.
In the case of the Wasserstein and bounded Lipschitz metric, we can
show that the first three derivatives are all bounded, so we can choose
$\cY$ to be the set of functions that satisfies these bounds. This
makes these metrics good choices for normal approximation.
\begin{thm}
\label{thm:wasserstein-normal-bounds}Suppose that $\cH\in\left\{ \cH_{\W},\cH_{\BL}\right\} $
and $\Lo=\N\left(0,1\right)$. Using the characterizing operator in
\ref{thm:normal-characterizing}, 
\[
\left\Vert \U_{\N}\h\right\Vert _{\infty}\le2,\quad\left\Vert \left(\U_{\N}\h\right)'\right\Vert _{\infty}\le3\sqrt{\pi/2},\quad\left\Vert \left(\U_{\N}\h\right)''\right\Vert _{\infty}\le6.
\]
\end{thm}
\begin{proof}
(Adapted from \cite[Section~3.1]{Ste12}).

First, note that Lipschitz functions are trivially absolutely continuous,
and 
\[
\E\left[\left|\X_{\N}\right|^{k}\left|\h\left(\X_{\N}\right)\right|\right]<\left|\h\left(0\right)\right|\E\left|\X_{\N}\right|^{k}+\E\left|\X_{\N}\right|^{k+1}
\]
for $\h\in\cH$. So, $\cH\subseteq\cX_{\N}$ and it makes sense to
consider $\U_{\N}\cH$.

Since $\U_{\N}\h=\U_{\N}\left(\h-\h\left(0\right)\right)$ for all
$\h\in\cH_{\N}$, it suffices to consider $\h\in\cH$ with $\h\left(0\right)=0$.
So, fix such a $\h\in\cH$. We have $\h\left(x\right)\le\left|x\right|$
for all $x\in\RR$, and it follows that 
\[
\left|\E_{\N}\h\right|\le\E\left|\X_{\N}\right|=\frac{2}{\sqrt{2\pi}}\int_{0}^{\infty}xe^{-x^{2}/2}\d x=\sqrt{2/\pi}.
\]
Then, for $x\ge0$,
\begin{alignat*}{1}
\left|\U_{\N}\h\left(x\right)\right| & \le e^{x^{2}/2}\int_{x}^{\infty}\left|\h\left(t\right)-\E_{\N}\h\right|e^{-t^{2}/2}\d t\\
 & \le e^{x^{2}/2}\int_{x}^{\infty}\left(t+\sqrt{\frac{2}{\pi}}\right)e^{-t^{2}/2}\d t\\
 & \le e^{x^{2}/2}\int_{x}^{\infty}te^{-t^{2}/2}\d t+\sqrt{\frac{2}{\pi}}\int_{0}^{\infty}e^{-s^{2}/2}\d s\\
 & =2.
\end{alignat*}
(We have used the substitution $s=t-x$, and the Gaussian integral).
With the alternative form for $\U_{\N}$ in \ref{eq:U_N}, an identical
argument gives $\left|\U_{\N}\h\left(x\right)\right|\le2$ for $x\le0$,
so $\left\Vert \U_{\N}\h\right\Vert _{\infty}\le2$.

\begin{comment}Now, define the differentiable function $\fo_{h}$
by 
\[
\fo_{\h}\left(x\right)=-\int_{0}^{1}\frac{1}{2\sqrt{t\left(1-t\right)}}\E\left[\X_{\N}\h\left(\sqrt{t}x+\sqrt{1-t}\X_{\N}\right)\right]\d t.
\]
We have 
\[
\fo_{\h}'\left(x\right)=-\int_{0}^{1}\frac{1}{2\sqrt{1-t}}\E\left[\X_{\N}\h'\left(\sqrt{t}x+\sqrt{1-t}\X_{\N}\right)\right]\d t.
\]
Now, integration by parts as in \ref{eq:normal-stein-identity} gives
\[
\E\left[\X_{\N}\h\left(\sqrt{t}x+\sqrt{1-t}\X_{\N}\right)\right]=\E\left[\sqrt{1-t}\h'\left(\sqrt{t}x+\sqrt{1-t}\X_{\N}\right)\right].
\]
Using Fubini's theorem (see \cite[Theorem~8.8]{Rud66}), 
\begin{alignat*}{1}
\fo_{\h}'\left(x\right)-x\fo_{\h}\left(x\right) & =\int_{0}^{1}\E\left[\left(-\frac{\X_{\N}}{2\sqrt{1-t}}+\frac{x}{2\sqrt{t}}\right)\h'\left(\sqrt{t}x+\sqrt{1-t}\X_{\N}\right)\right]\d t\\
 & =\int_{0}^{1}\E\left[\frac{\d}{\d t}\h\left(\sqrt{t}x+\sqrt{1-t}\X_{\N}\right)\right]\d t\\
 & =\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-y^{2}/2}\int_{0}^{1}\frac{\d}{\d t}\h\left(\sqrt{t}x+\sqrt{1-t}y\right)\d t\d y\\
 & =\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-y^{2}/2}\left(\h\left(x\right)-\h\left(y\right)\right)\d t\d y\\
 & =\h\left(x\right)-\E\h.
\end{alignat*}
So, $\fo_{\h}$ satisfies \ref{eq:diff-eqn} and we must have 
\[
\fo_{\h}=e^{x^{2}/2}\int_{-\infty}^{x}\left(\h\left(t\right)-\E_{\N}\h\right)e^{-t^{2}/2}\d t+ce^{x^{2}/2}
\]
for some $c$, by the general form \ref{eq:diff-eqn-soln} of the
solution to \ref{eq:diff-eqn}. By the Lipschitz condition on $\h$,
\[
\left|\fo_{\h}\left(x\right)\right|=\left|\int_{0}^{1}\frac{1}{2\sqrt{t}}\E\left[\h'\left(\sqrt{t}x+\sqrt{1-t}\X_{\N}\right)\right]\d t\right|\le\int_{0}^{1}\frac{1}{2\sqrt{t}}\d t,
\]
so $e^{-x^{2}/2}\fo_{\h}\left(x\right)$ as $x\to-\infty$. That is,
$c=0$ and 
\[
\left\Vert \left(\U_{\N}\h\right)'\right\Vert _{\infty}=\left\Vert \fo_{\h}'\right\Vert _{\infty}\le\E\left|\X_{\N}\right|\int_{0}^{1}\frac{1}{2\sqrt{1-t}}\d t\le\sqrt{\frac{2}{\pi}}.
\]


\begin{note}The preceding argument is very unmotivated. If I have
time I'll try to frame it differently, as a different way of solving
the differential equation \ref{eq:diff-eqn}. There's also a different
(longer) argument in the proof of \cite[Lemma~2.4]{Ste12}.\end{note}

\end{comment}

Now, note that
\[
\left(\T_{\N}\U_{\N}\h\right)'\left(x\right)=\left(\U_{\N}\h\right)''\left(x\right)-x\left(\U_{\N}\h\right)'\left(x\right)-\U_{\N}\h\left(x\right)=\h'\left(x\right).
\]
Set $g=\h'+\U_{\N}\h$. Note $\left\Vert g\right\Vert _{\infty}\le\left\Vert \h'\right\Vert _{\infty}+\left\Vert \U_{\N}\h\right\Vert _{\infty}\le3$,
so $g\in\cX_{\N}$, and 
\[
\E_{\N}g=\E\left[\left(\U_{\N}\h\right)''\left(\X_{\N}\right)-\X_{\N}\left(\U_{\N}\h\right)'\left(\X_{\N}\right)\right]=0
\]
as in \ref{eq:normal-stein-identity}. Hence
\[
\T_{\N}\left(\left(\U_{\N}\h\right)'\right)=g+\E_{\N}g,
\]
and $\left(\U_{\N}\h\right)'$ satisfies \ref{eq:diff-eqn} (with
$g$ taking the role of $\h$). By the general form \ref{eq:diff-eqn-soln}
of the solution to \ref{eq:diff-eqn}, we must have 
\[
\left(\U_{\N}\h\right)'=e^{x^{2}/2}\int_{-\infty}^{x}\left(\h\left(t\right)-\E_{\N}\h\right)e^{-t^{2}/2}\d t+ce^{x^{2}/2}
\]
for some $c$. Since $\left(\U_{\N}\h\right)'\in\cX_{\N}$, we must
have $e^{-x^{2}/2}\left(\U_{\N}\h\right)'\left(x\right)\to0$. That
is, $c=0$. For $x\ge0$, we have

\[
\left|\left(\U_{\N}\h\right)'\left(x\right)\right|\le e^{x^{2}/2}\int_{x}^{\infty}\left|g\left(t\right)\right|e^{-t^{2}/2}\d t\le3\int_{0}^{\infty}e^{-s^{2}/2}\d s\le3\sqrt{\frac{\pi}{2}}.
\]
(As before, $s=t-x$). A similar calculation shows that $\left|\left(\U_{\N}\h\right)'\left(x\right)\right|\le3\sqrt{\frac{\pi}{2}}$
for $x<0$, so 
\[
\left\Vert \left(\U_{\N}\h\right)'\right\Vert _{\infty}\le3\sqrt{\frac{\pi}{2}}.
\]
Also, for $x\ge0$,
\begin{alignat*}{1}
\left|\left(\U_{\N}g\right)'\left(x\right)\right| & =\left|g\left(x\right)+x\U_{\N}g\left(x\right)\right|\\
 & \le3+3xe^{x^{2}/2}\int_{x}^{\infty}e^{-t^{2}/2}\d t\\
 & \le3+3xe^{x^{2}/2}\int_{x}^{\infty}\frac{t}{x}e^{-t^{2}/2}\d t\\
 & =6.
\end{alignat*}
Similarly, $\left|\left(\U_{\N}g\right)'\left(x\right)\right|\le6$
for $x<0$, so 
\[
\left\Vert \left(\U_{\N}\h\right)''\right\Vert _{\infty}=\left\Vert \left(\U_{\N}g\right)'\right\Vert _{\infty}\le6.\qedhere
\]
\end{proof}
\begin{rem}
\label{rem:better-wasserstein-bounds}In the interests of a simple
proof, the bounds in \ref{thm:wasserstein-normal-bounds} are very
crude. The stronger bounds $\left\Vert \fo'\right\Vert _{\infty}\le\sqrt{\frac{2}{\pi}}$
and $\left\Vert \fo''\right\Vert _{\infty}\le2$ can be shown to hold,
with similarly elementary but much more fiddly proofs. See \cite[Lemma~2.4]{CGS10}.
\end{rem}

\subsection{The Berry-Esseen Theorem\label{sub:berry-esseen}}

The Berry-Esseen \textbf{t}heorem is a quantitative version of the
\textbf{c}entral \textbf{l}imit \textbf{t}heorem. The original version
was independently proved in \cite{Ber41} and \cite{Ess42} with some
relatively involved Fourier analysis. However, the machinery of Stein's
method allows for a very simple proof. This section should help make
the theory from \ref{sec:stein-general} a bit more concrete by giving
our first example of Stein's method in action.
\begin{thm}
\label{thm:berry-esseen}Let $Q_{1},\dots,Q_{n}\in\L Q$ be independent,
with $\E Q=0$, $\E Q^{2}=1$ and $\E\left|Q\right|^{3}<\infty$.
Define 
\[
\X=\frac{1}{\sqrt{n}}\sum_{\i=1}^{n}Q_{i}.
\]
Then 
\[
d_{\W}\left(\X,\N\left(0,1\right)\right)\le\frac{C}{\sqrt{n}}\,\E\left|Q\right|^{3}
\]
for some constant $C$.\end{thm}
\begin{rem}
In the following proof, we will prove the theoren with $C=9$. The
sharper bounds in \ref{rem:better-wasserstein-bounds} yield $C=3$
with the same argument.
\end{rem}

\begin{rem}
\ref{prop:transfer} allows us to transfer the result of \ref{thm:berry-esseen}
from the Wasserstein metric to the Kolmogorov metric. The Berry-Esseen
\textbf{t}heorem is usually stated for the Kolmogorov metric, because
it has a direct interpretation as the supremum difference between
probabilities. However in this case the transfer makes the bound much
worse. It is possible to use Stein's method directly in the Kolmogorov
metric (see \cite[Section~6]{Ste12}), but this is a little more involved.\end{rem}
\begin{proof}
[Proof of \ref{thm:berry-esseen}](Adapted from \cite[Section~5]{Ste12}).
Let $f$ be a function satisfying the bounds in \ref{thm:wasserstein-normal-bounds}

We need to compare 
\begin{equation}
\E\left[\X f\left(\X\right)\right]=\E\left[\frac{1}{\sqrt{n}}\sum_{\i=1}^{n}Q_{\i}f\left(\X\right)\right]\label{eq:berry-esseen-EXfX}
\end{equation}
with $\E f'\left(\X\right)$. Define $W_{\i}=\X-\frac{1}{\sqrt{n}}Q_{\i}$,
so each $W_{\i}$ and $Q_{\i}$ are independent. For each $\i$ we
then have $\E Q_{\i}f\left(W_{\i}\right)=\E Q_{\i}\E f\left(W_{\i}\right)=0$,
so 
\begin{equation}
\E\left[Q_{\i}f\left(\X\right)\right]=\E\left[Q_{\i}\left(f\left(\X\right)-f\left(W_{\i}\right)\right)\right].\label{eq:berry-esseen-EQfX}
\end{equation}
Now, we use a Taylor expansion with the Lagrange form of the remainder.
Noting that $\X-W_{\i}=\frac{1}{\sqrt{n}}Q_{\i}$, we have
\[
f\left(\X\right)=f\left(W_{\i}\right)+\frac{1}{\sqrt{n}}Q_{\i}f'\left(W_{\i}\right)+\frac{1}{2n}Q_{\i}^{2}f''\left(E\right)
\]
for some (random) $E$. Combining this with \ref{eq:berry-esseen-EQfX}
gives
\begin{equation}
\E\left[Q_{\i}f\left(\X\right)\right]=\frac{1}{\sqrt{n}}\E\left[Q_{\i}^{2}f'\left(W_{\i}\right)\right]+\frac{1}{2n}\E\left[Q_{\i}^{3}f''\left(E\right)\right].\label{eq:berry-esseen-EQfX-2}
\end{equation}
Next, note that
\begin{equation}
\E\left[Q_{\i}^{2}f'\left(W_{\i}\right)\right]=\E Q_{\i}^{2}\,\E f'\left(W_{\i}\right)=\E f'\left(W_{\i}\right)=\E f'\left(\X\right)+\frac{1}{\sqrt{n}}\,\E\left[Q_{\i}f''\left(E'\right)\right]\label{eq:berry-esseen-EQ2fW}
\end{equation}
for some $E'$, again using the Lagrange form of the remainder. Since
$\left\Vert f''\right\Vert _{\infty}\le6$, we have 
\begin{equation}
\frac{1}{2}\E\left|Q_{\i}^{3}f''\left(E\right)\right|\le3\,\E\left|Q^{3}\right|,\quad\E\left|Q_{\i}f''\left(E'\right)\right|\le6\,\E\left|Q\right|.\label{eq:berry-esseen-error-bounds}
\end{equation}
H\"older's inequality (see \cite[Theorem~3.6~(1)]{Rud66}) for the
$L^{3/2}$ norm gives 
\[
1=\E\left|Q^{2}\right|^{3/2}\le\left(\E\left|\left(Q^{2}\right)^{3/2}\right|^{2/3}\right)^{3/2}=\E\left|Q^{3}\right|,
\]
so an application of H\"older's inequality for the $L^{3}$ norm
gives
\begin{equation}
\E\left|Q\right|\le\E\left|Q^{3}\right|^{1/3}\le\E\left|Q^{3}\right|.\label{eq:berry-esseen-holder}
\end{equation}
Combining \ref{eq:berry-esseen-EXfX,eq:berry-esseen-EQfX-2,eq:berry-esseen-EQ2fW,eq:berry-esseen-error-bounds,eq:berry-esseen-holder}
gives
\begin{alignat*}{1}
\left|\E\left[\X f\left(\X\right)\right]-\E f'\left(\X\right)\right| & \le\left|\frac{1}{\sqrt{n}}\sum_{\i=1}^{n}\left(\E\left[Q_{\i}f\left(\X\right)\right]-\frac{1}{\sqrt{n}}\E\left[Q_{\i}^{2}f'\left(W_{\i}\right)\right]\right)\right|\\
 & \qquad+\left|\frac{1}{n}\sum_{\i=1}^{n}\left(\E\left[Q_{\i}^{2}f'\left(W_{\i}\right)\right]-\E f'\left(\X\right)\right)\right|\\
 & \le\frac{3}{\sqrt{n}}\,\E\left|Q^{3}\right|+\frac{6}{\sqrt{n}}\,\E\left|Q^{3}\right|\\
 & =\frac{9}{\sqrt{n}}\,\E\left|Q^{3}\right|.
\end{alignat*}
This completes the proof.
\end{proof}

\section{The Poisson Case\label{sec:poisson}}
\begin{thm}
\label{thm:poisson-characterizing}Let $\X_{\Po\left(\lambda\right)}\in\Po\left(\lambda\right)$
and let 
\[
\cX_{\Po\left(\lambda\right)}=\cF_{\Po\left(\lambda\right)}=\left\{ \h:\NN\to\RR:\E\left[\X_{\Po\left(\lambda\right)}^{k}\left|\h\left(\X_{\Po\left(\lambda\right)}\right)\right|\right]<\infty\mbox{ for all }k\in\NN\right\} .
\]
The operator $\T_{\Po\left(\lambda\right)}:\cF_{\Po\left(\lambda\right)}\to\cX_{\Po\left(\lambda\right)}$
defined by $\T_{\Po\left(\lambda\right)}\fo\left(k\right)=\lambda\fo\left(k+1\right)-k\fo\left(k\right)$
is a characterizing operator for $\Po\left(\lambda\right)$.\end{thm}
\begin{proof}
The components of the proof are very similar to that of \ref{thm:normal-characterizing}.
First, fix $k$ and note that 
\[
\left.\left(\i+1\right)^{k}\frac{\lambda^{\left(\i+1\right)}}{\left(\i+1\right)!}\middle/\i^{k}\frac{\lambda^{\i}}{\i!}\right.\to0
\]
as $\i\to\infty$. So by the ratio test $\E\X_{\Po\left(\lambda\right)}^{k}=O\left(\sum_{\i=0}^{\infty}\i^{k}\frac{\lambda^{\i}}{\i!}\right)<\infty$,
and $\cX_{\Po\left(\lambda\right)}$ contains the constant functions.

Now, we check that $\T_{\N}\fo$ is well-defined as a linear operator
with codomain $\cX_{\Po\left(\lambda\right)}$. For $\fo\in\cF_{\Po\left(\lambda\right)}$
and $k\in\NN$,
\[
\E\left[\X_{\Po\left(\lambda\right)}^{k}\left|\T_{\Po\left(\lambda\right)}\fo\left(\X_{\Po\left(\lambda\right)}\right)\right|\right]\le\lambda\E\left[\X_{\Po\left(\lambda\right)}^{k}\left|\fo\left(\X_{\Po\left(\lambda\right)}+1\right)\right|\right]+\E\left[\X_{\Po\left(\lambda\right)}^{k+1}\left|\fo\left(\X_{\Po\left(\lambda\right)}\right)\right|\right].
\]
We have
\begin{alignat*}{1}
\E\left[\X_{\Po\left(\lambda\right)}^{k}\left|\fo\left(\X_{\Po\left(\lambda\right)}+1\right)\right|\right] & =O\left(\sum_{\i=0}^{\infty}\i^{k}\frac{\lambda^{\i}}{\i!}\left|\fo\left(\i+1\right)\right|\right)\\
 & =O\left(\sum_{r=0}^{k+1}\sum_{\i=0}^{\infty}\left(\i+1\right)^{r}\frac{\lambda^{\i+1}}{\left(\i+1\right)!}\left|\fo\left(\i+1\right)\right|\right)\\
 & =O\left(\sum_{r=0}^{k+1}\E\left[\X_{\Po\left(\lambda\right)}^{k}\left|\fo\left(\X_{\Po\left(\lambda\right)}\right)\right|\right]\right)\\
 & <\infty.
\end{alignat*}
It follows that $\E\left[\X_{\Po\left(\lambda\right)}^{k}\left|\T_{\Po\left(\lambda\right)}\fo\left(\X_{\Po\left(\lambda\right)}\right)\right|\right]<\infty$
and $\T_{\Po\left(\lambda\right)}\fo\in\cX_{\Po\left(\lambda\right)}$.

For any $\fo\in\cF_{\Po\left(\lambda\right)}$, we have
\[
\E_{\Po\left(\lambda\right)}\T_{\Po\left(\lambda\right)}\fo=e^{-\lambda}\sum_{\i=0}^{\infty}\frac{\lambda^{\i+1}}{\i!}\fo\left(\i+1\right)-e^{-\lambda}\sum_{\i=1}^{\infty}\frac{\lambda^{\i}}{\left(\i-1\right)!}\fo\left(\i\right)=0
\]
so $\E_{\Po\left(\lambda\right)}\T_{\Po\left(\lambda\right)}=0$ and
\ref{eq:E_0T_0-consistency} holds. Then, define $\U_{\Po\left(\lambda\right)}:\cX_{\Po\left(\lambda\right)}\to\cF_{\Po\left(\lambda\right)}$
by 
\[
\U_{\Po\left(\lambda\right)}\h\left(\i\right)=\frac{\left(\i-1\right)!}{\lambda^{\i}}\sum_{\ii=0}^{\i-1}\frac{\lambda^{\ii}}{\ii!}\left(\h\left(\ii\right)-\E_{\Po\left(\lambda\right)}\h\right)
\]
for $\i\in\ZZ^{+}$ (and $\U_{\Po\left(\lambda\right)}\h\left(0\right)=0$).

For $\h\in\cX_{\Po\left(\lambda\right)}$ and $k\in\NN$,
\begin{alignat*}{1}
\E\left[\X_{\Po\left(\lambda\right)}^{k}\left|\U_{\Po\left(\lambda\right)}\h\left(\X_{\Po\left(\lambda\right)}\right)\right|\right] & =O\left(\sum_{\i=1}^{\infty}\i^{k-1}\left|\sum_{\ii=0}^{\i-1}\frac{\lambda^{\ii}}{\ii!}\left(\h\left(\ii\right)-\E_{\Po\left(\lambda\right)}\h\right)\right|\right)\\
 & =O\left(\sum_{\i=1}^{\infty}\i^{k-1}\left|\sum_{\ii=\i}^{\infty}\frac{\lambda^{\ii}}{\ii!}\left(\h\left(\ii\right)-\E_{\Po\left(\lambda\right)}\h\right)\right|\right)\\
 & =O\left(\sum_{\ii=1}^{\infty}\frac{\lambda^{\ii}}{\ii!}\left|\h\left(\ii\right)-\E_{\Po\left(\lambda\right)}\h\right|\sum_{\i=1}^{\ii}\i^{k-1}\right)\\
 & =O\left(\sum_{\ii=1}^{\infty}\ii^{k}\frac{\lambda^{\ii}}{\ii!}\left|\h\left(\ii\right)-\E_{\Po\left(\lambda\right)}\h\right|\right)<\infty.
\end{alignat*}
So, $\U_{\Po\left(\lambda\right)}\h\in\cF_{\Po\left(\lambda\right)}$,
and $\U_{\Po\left(\lambda\right)}$ is well-defined with codomain
$\cF_{\Po\left(\lambda\right)}$. Now, substituting and simplifying
gives
\[
\T_{\Po\left(\lambda\right)}\U_{\Po\left(\lambda\right)}\h\left(\i\right)=\h\left(\i\right)-\E_{\Po\left(\lambda\right)}\h,
\]
so \ref{eq:U_0-characterizing} holds and \ref{prop:U_0-characterizing}
completes the proof.\end{proof}
\begin{rem}
As for the normal case, $\T_{\Po\left(\lambda\right)}$ is a characterizing
operator in the sense of \ref{prop:characterizing}, because $\cH_{\TV}\subseteq\cX_{\Po\left(\lambda\right)}$.
\end{rem}
The natural metric to use in Poisson approximation is the total variation
metric. We can bound the functions in $\U_{\Po\left(\lambda\right)}\cH_{\TV}$
as well as their first differences (define $\Delta\fo\left(\i\right)=\fo\left(\i+1\right)-\fo\left(\i\right)$).
\begin{thm}
\label{thm:poisson-bound}Using the characterizing operator in \ref{thm:poisson-characterizing},
for all $\h\in\cH_{\TV}$, 
\[
\left\Vert \U_{\Po\left(\lambda\right)}\h\right\Vert _{\infty}\le\min\left\{ 2,3\lambda^{-1/2}\right\} ,\quad\left\Vert \Delta\U_{\Po\left(\lambda\right)}\h\right\Vert _{\infty}\le\lambda^{-1}\left(1-e^{-\lambda}\right).
\]
\end{thm}
\begin{proof}
(Adapted from \cite[Lemma~1.1.1]{BHJ92}). For any Borel $\A$ and
any $k\in\NN$,
\begin{align}
\U_{\Po\left(\lambda\right)}\one_{\A}\left(\i\right) & =\frac{e^{\lambda}\,\left(\i-1\right)!}{\lambda^{\i}}\left(\Po_{\lambda}\left(\A\cap\left[0,\i-1\right]\right)-\Po_{\lambda}\left(\A\right)\Po_{\lambda}\left(\left[0,\i-1\right]\right)\right)\nonumber \\
 & =\frac{e^{\lambda}\,\left(\i-1\right)!}{\lambda^{\i}}\left(\vphantom{\int}\Po_{\lambda}\left(\A\cap\left[0,\i-1\right]\right)\left(\vphantom{\sum}1-\Po_{\lambda}\left(\left[0,\i-1\right]\right)\right)\right.\nonumber \\
 & \qquad\qquad\qquad\quad-\left.\vphantom{\int}\left(\vphantom{\sum}\Po_{\lambda}\left(\A\right)-\Po_{\lambda}\left(\A\cap\left[0,\i-1\right]\right)\right)\Po_{\lambda}\left(\left[0,\i-1\right]\right)\right)\nonumber \\
 & =\frac{e^{\lambda}\,\left(\i-1\right)!}{\lambda^{\i}}\left(\vphantom{\sum}\Po_{\lambda}\left(\A\cap\left[0,\i-1\right]\right)\Po_{\lambda}\left(\left[\i,\infty\right]\right)-\Po_{\lambda}\left(\A\backslash\left[0,\i-1\right]\right)\Po_{\lambda}\left(\left[0,\i-1\right]\right)\right).\label{eq:U_po-bound-in-progress}
\end{align}
Note that $\Po_{\lambda}\left(\A\cap\left[0,\i-1\right]\right)$ is
bounded above by $\Po_{\lambda}\left(\left[0,\i-1\right]\right)$
and $\Po_{\lambda}\left(\A\backslash\left[0,\i-1\right]\right)$ is
bounded above by $\Po_{\lambda}\left(\left[\i,\infty\right]\right)$,
so
\[
\left|\U_{\Po\left(\lambda\right)}\one_{\A}\left(\i\right)\right|\le\frac{e^{\lambda}\,\left(\i-1\right)!}{\lambda^{\i}}\Po_{\lambda}\left(\left[0,\i-1\right]\right)\Po_{\lambda}\left(\left[\i,\infty\right]\right).
\]
If $1\le\i\le\lambda$, then
\begin{alignat*}{1}
\left|\U_{\Po\left(\lambda\right)}\one_{\A}\left(\i\right)\right| & \le\frac{e^{\lambda}\,\left(\i-1\right)!}{\lambda^{\i}}\Po_{\lambda}\left(\left[0,\i-1\right]\right)\\
 & =\lambda^{-1}\sum_{\ii=0}^{\i-1}\frac{\left(\i-1\right)!}{\ii!\lambda^{\i-1-\ii}}\\
 & =\lambda^{-1}\sum_{\iii=0}^{\i-1}\frac{\left(\i-1\right)!}{\left(\i-1-\iii\right)!\lambda^{\iii}}.
\end{alignat*}
Now, for $\iii=0,\dots,\i-1$ define 
\[
a_{\iii}=\frac{\left(\i-1\right)!}{\left(\i-1-\iii\right)!\lambda^{\iii}}.
\]
Note $a_{\iii}\le\left(\i-1\right)^{\iii}/\lambda^{\iii}\le1$ for
all $\iii$, so immediately $\left|\U_{\Po\left(\lambda\right)}\one_{\A}\left(\i\right)\right|\le\i/\lambda\le1$.
Also, 
\[
a_{\iii}\le\frac{\i-\iii}{\lambda}\le1-\lambda^{-1/2},\quad a_{\iii+1}/a_{\iii}=\frac{\i-\iii}{\lambda}\le1-\lambda^{-1/2}
\]
for $\iii>\lambda^{1/2}$. So,
\begin{alignat*}{1}
\left|\U_{\Po\left(\lambda\right)}\one_{\A}\left(\i\right)\right| & \le\lambda^{-1}\sum_{\iii=0}^{\floor{\lambda^{1/2}}}a_{\iii}+\lambda^{-1}\sum_{\iii=\ceil{\lambda^{1/2}}}^{\i-1}a_{\iii}\\
 & \le\lambda^{-1}\left(1+\lambda^{1/2}\right)+\lambda^{-1}\sum_{\iiii=1}^{\infty}\left(1-\lambda^{-1/2}\right)^{\iiii}\\
 & =2\lambda^{-1/2}
\end{alignat*}
(note $\lambda\ge\i\ge1$, so $0\le1-\lambda^{-1/2}<1$). Now, suppose
that $\i>\lambda$. Then
\begin{alignat*}{1}
\left|\U_{\Po\left(\lambda\right)}\one_{\A}\left(\i\right)\right| & \le\frac{e^{\lambda}\,\left(\i-1\right)!}{\lambda^{\i}}\Po_{\lambda}\left(\left[\i,\infty\right]\right)\\
 & =\i^{-1}\sum_{\ii=\i}^{\infty}\frac{\i!}{\ii!\lambda^{\i-\ii}}\\
 & =\i^{-1}\sum_{\iii=0}^{\infty}\frac{\lambda^{\iii}\i!}{\left(\i+\iii\right)!}\\
 & \le\i^{-1}\sum_{\iii=0}^{\infty}\frac{\lambda^{\iii}}{\left(\i+1\right)^{\iii}}\\
 & =\frac{\i+1}{\i\left(\i+1-\lambda\right)}\\
 & <2.
\end{alignat*}
This also shows that $\left|\U_{\Po\left(\lambda\right)}\one_{\A}\left(\i\right)\right|\le2\lambda^{-1/2}$
if $\lambda<1$. Now, let $b_{\iii}=\lambda^{\iii}\i!/\left(\i+\iii\right)!$,
so 
\[
b_{\iii}\le\frac{\lambda^{\iii}}{\left(\i+1\right)^{\iii}}\le1.
\]
Also, for $\iii>\lambda^{1/2}$,
\[
b_{\iii}\le\frac{\lambda}{\i+\iii}<\frac{\lambda}{\lambda+\lambda^{1/2}}\mbox{ and }\frac{b_{\iii+1}}{b_{\iii}}\le\frac{\lambda}{\i+\iii}<\frac{\lambda}{\lambda+\lambda^{1/2}}.
\]
So, for $\lambda\ge1$,
\begin{alignat*}{1}
\left|\U_{\Po\left(\lambda\right)}\one_{\A}\left(\i\right)\right| & \le\i^{-1}\sum_{\iii=0}^{\floor{\lambda^{1/2}}}b_{\iii}+\i^{-1}\sum_{\iii=\ceil{\lambda^{1/2}}}^{\infty}b_{\iii}\\
 & =\lambda^{-1}\left(1+\lambda^{1/2}\right)+\lambda^{-1}\sum_{\iiii=1}^{\infty}\left(\frac{\lambda}{\lambda+\lambda^{1/2}}\right)^{\iiii}\\
 & =\lambda^{-1}+2\lambda^{-1/2}\\
 & \le3\lambda^{-1/2}.
\end{alignat*}
We have proved that $\left\Vert \U_{\Po\left(\lambda\right)}\h\right\Vert _{\infty}\le\min\left\{ 2,3\lambda^{-1/2}\right\} $
for $\h\in\cH$.

\begin{comment}

\cite[Lemma~1.1.1]{BHJ92} gives this argument (but with no details),
and claims it yields $\left|\U_{\Po\left(\lambda\right)}\one_{\A}\left(\i\right)\right|\le2\lambda^{-1/2}$.
I'm not seeing it though... I might revisit this.

\end{comment}

Now, we prove the bound on $\left\Vert \Delta\U_{\Po\left(\lambda\right)}\h\right\Vert _{\infty}$.
Let $\i,k\in\NN$ and suppose $\i\le k$. By \ref{eq:U_po-bound-in-progress},
\begin{alignat*}{1}
\U_{\Po\left(\lambda\right)}\one_{\left\{ k\right\} }\left(\i-1\right) & =-\frac{\lambda^{k}}{e^{\lambda}k!}\sum_{\ii=0}^{\i-2}\lambda^{\ii-\i}\frac{\left(\i-2\right)!}{\ii!}\\
 & =-\frac{\lambda^{k}}{e^{\lambda}k!}\sum_{\iii=1}^{\i-1}\lambda^{\iii-\i-1}\frac{\left(\i-2\right)!}{\left(\iii-1\right)!}.
\end{alignat*}
If $\iii\le\i-1$ then $\frac{\i-1}{\iii}\ge1$ so
\begin{alignat*}{1}
\U_{\Po\left(\lambda\right)}\one_{\left\{ k\right\} }\left(\i-1\right) & \ge-\frac{\lambda^{k}}{e^{\lambda}k!}\sum_{\iii=0}^{\i-1}\lambda^{\iii-\i-1}\frac{\left(\i-1\right)!}{\iii!}\\
 & =\U_{\Po\left(\lambda\right)}\one_{\left\{ k\right\} }\left(\i\right).
\end{alignat*}
Similarly, if $\i\ge k$ then 
\begin{alignat*}{1}
\U_{\Po\left(\lambda\right)}\one_{\left\{ k\right\} }\left(\i\right) & =\frac{\lambda^{k}}{e^{\lambda}k!}\sum_{\ii=\i}^{\infty}\lambda^{\ii-\i}\frac{\left(\i-1\right)!}{\ii!}\\
 & =\frac{\lambda^{k}}{e^{\lambda}k!}\sum_{\iii=\i+1}^{\infty}\lambda^{\iii-\i-1}\frac{\left(\i-1\right)!}{\left(\iii-1\right)!}\\
 & \ge\frac{\lambda^{k}}{e^{\lambda}k!}\sum_{\iii=\i+1}^{\infty}\lambda^{\iii-\i-1}\frac{\i!}{\iii!}\\
 & =\U_{\Po\left(\lambda\right)}\one_{\left\{ k\right\} }\left(\i+1\right).
\end{alignat*}
Therefore, 
\[
\U_{\Po\left(\lambda\right)}\one_{\left\{ k\right\} }\left(\i+1\right)-\U_{\Po\left(\lambda\right)}\one_{\left\{ k\right\} }\left(\i\right)
\]
is positive only when $\i=k$, and
\begin{alignat*}{1}
\U_{\Po\left(\lambda\right)}\one_{\left\{ k\right\} }\left(k+1\right)-\U_{\Po\left(\lambda\right)}\one_{\left\{ k\right\} }\left(k\right) & =e^{-\lambda}\lambda^{-1}\sum_{\ii=k+1}^{\infty}\frac{\lambda^{\ii}}{\ii!}+e^{-\lambda}\lambda^{-1}\sum_{\iiii=1}^{k}\frac{\lambda^{\iiii}}{\iiii!}\frac{\iiii}{k}\\
 & \le e^{-\lambda}\lambda^{-1}\left(e^{\lambda}-1\right)=\lambda^{-1}\left(1-e^{-\lambda}\right).
\end{alignat*}
Now, for any $\A\subseteq\NN$ we have $\one_{\A}=\sum_{k\in\A}\one_{\left\{ k\right\} }$,
so linearity of $\U_{\Po\left(\lambda\right)}$ gives 
\[
\Delta\U_{\Po\left(\lambda\right)}\one_{\A}\left(k\right)\le\lambda^{-1}\left(1-e^{-\lambda}\right)
\]
for all $k$. Finally, note that $\U_{\Po\left(\lambda\right)}\one_{\NN\backslash\A}+\U_{\Po\left(\lambda\right)}\one_{\A}=\U_{\Po\left(\lambda\right)}\one_{\NN}=0$,
so 
\[
-\Delta\U_{\Po\left(\lambda\right)}\one_{\A}\left(k\right)=\Delta\U_{\Po\left(\lambda\right)}\one_{\NN\backslash\A}\left(k\right)\ge\lambda^{-1}\left(1-e^{-\lambda}\right).
\]
We conclude that $\left\Vert \U_{\Po\left(\lambda\right)}\h\right\Vert _{\infty}\le\lambda^{-1}\left(1-e^{-\lambda}\right)$
for $\h\in\cH_{\TV}$.\end{proof}
\begin{rem}
\label{rem:poisson-better-bounds}A similarly elementary but more
fiddly argument can be used to show that the bound $\left\Vert \fo\right\Vert _{\infty}\le\min\left\{ 1,1.4\lambda^{-1/2}\right\} $
is also valid (see \cite[Lemma~4]{Bar83}). A much more sophisticated
argument in \cite[Remark~10.2.4]{BHJ92} gives the sharper bound $\left\Vert \fo\right\Vert _{\infty}\le\min\left\{ 1,\sqrt{2/\left(e\lambda\right)}\right\} $.
\end{rem}

\subsection{A Quantitative Poisson Limit Theorem\label{sub:quantitative-poisson-limit}}

Just as we quantified the \textbf{c}entral \textbf{l}imit \textbf{t}heorem
in \ref{sub:berry-esseen}, we can quantify the Poisson \textbf{l}imit
\textbf{t}heorem to give our first example of Poisson approximation
by Stein's method.
\begin{thm}
\label{thm:quantitative-poisson-limit}Let $Q_{1},\dots,Q_{n}\in\L Q$
be independent zero-one random variables. Define
\[
\X=\sum_{\i=1}^{n}Q_{\i},
\]
and let $\lambda=\E\X=n\E Q$. Then 
\[
d_{\TV}\left(\X,\Po\left(\lambda\right)\right)\le\frac{\lambda\left(1-e^{-\lambda}\right)}{n}.
\]
\end{thm}
\begin{proof}
(Adapted from \cite[Theorem~4.6]{Ros11}). Let $f$ be a function
satisfying the bounds in \ref{thm:poisson-bound}. The idea of the
proof is fairly similar to that of \ref{thm:berry-esseen}: we are
trying to compare $\E\left[\X f\left(\X\right)\right]$ to $\E\left[\lambda f\left(\X+1\right)\right]$.
Define $W_{\i}=X-Q_{\i}$, so each $W_{\i}$ and $Q_{\i}$ are independent.
We have 
\begin{alignat}{1}
\E\left[\X f\left(\X\right)\right] & =\sum_{\i=1}^{n}\E\left[Q_{\i}f\left(X\right)\right]\nonumber \\
 & =\sum_{\i=1}^{n}\Pr\left(Q_{\i}=1\right)\E\left[f\left(X\right)|Q_{\i}=1\right]\nonumber \\
 & =\sum_{\i=1}^{n}\frac{\lambda}{n}\E\left[f\left(W_{\i}+1\right)\right].\label{eq:poisson-limit-EXfX}
\end{alignat}
Since $X-W_{\i}=Q_{\i}$, we then have 
\begin{equation}
\E\left[f\left(X+1\right)-f\left(W_{\i}+1\right)\right]=\E\left[\sum_{\i=W_{\i}+1}^{\X}\Delta f\left(\i\right)\right]\le\E Q_{\i}\left\Vert \Delta f\left(\i\right)\right\Vert _{\infty}=\frac{\lambda}{n}\left\Vert \Delta f\left(\i\right)\right\Vert _{\infty}.\label{eq:poisson-limit-E-difference}
\end{equation}
Combining \ref{eq:poisson-limit-EXfX} and \ref{eq:poisson-limit-E-difference}
gives
\[
\left|\E\left[\X f\left(\X\right)\right]-\E\left[\lambda f\left(\X+1\right)\right]\right|\le\sum_{\i=1}^{n}\left(\frac{\lambda}{n}\right)^{2}\left\Vert \Delta f\left(\i\right)\right\Vert _{\infty}\le\frac{\lambda\left(1-e^{-\lambda}\right)}{n},
\]
as required.
\end{proof}

\section{Size-Bias Coupling\label{sec:size-bias-coupling}}

\global\long\def\Ls#1{\mathcal{L}_{#1}^{\left(s\right)}}


\global\long\def\s#1{#1^{\left(s\right)}}


In both the Normal and Poisson cases, estimation of the necessary
expectation $\E_{\X}\To\fo$ involves some control over expectations
of the form $\E\left[\X f\left(\X\right)\right]$. In \ref{sub:berry-esseen,sub:quantitative-poisson-limit},
we were able to achieve this with ad-hoc arguments exploiting independence
in our random variable of interest. However, in order to apply Stein's
method in less ideal circumstances, we will need some more machinery.
There are a number of different techiques that can be used in different
cases. In the case of positive random variables such as those found
in combinatorial applications, one particularly useful technique is
\emph{size-bias coupling}, which we will discuss in this section.
\begin{defn}
The \emph{size-bias distribution} $\Ls{\X}$ with respect to a nonnegative
integrable random variable $\X$ is given by $\Ls{\X}\left(A\right)=\E\left[\X\one_{\X\in\A}\right]/\E\X$.
In this section, $\s{\X}$ will always denote a random variable with
the distribution $\Ls{\X}$.\end{defn}
\begin{rem}
\label{rem:size-bias-discrete}The reason this is called size-biasing
is because the probability of each value of $\X$ is biased proportionally
by that value. To be precise, in the discrete case the definition
of $\Ls{\X}$ can be stated as 
\begin{equation}
\Ls{\X}\left(k\right)=\frac{k}{\E\X}\L{\X}\left(k\right).\label{eq:discrete-size-bias-definition}
\end{equation}
for each $k$. This often has a combinatorial interpretation. Intuitively
speaking, if $\L{\X}$ is the number of balls in a random bin, then
$\Ls{\X}$ is the distribution of the number of balls in the bin of
a random ball.
\end{rem}
We can immediately see from \ref{rem:size-bias-discrete} that in
the discrete case, 
\begin{equation}
\E\fo(\s{\X})=\E\left[\X\fo\left(\X\right)\right]/\E\X\label{eq:size-bias-transformation}
\end{equation}
for all $\Ls{\X}$-integrable functions $\fo$. This is in fact true
in general, which can be straightforwardly proved with the measure-theoretic
definition of the integral.

Size-biasing therefore gives us a way to work with expectations of
the form $\E\left[\X\fo\left(\X\right)\right]$. In order to use this
for Stein's method, we need to couple $\X$ with $\s{\X}$ in an appropriate
way. There is a general construction that can be used for both normal
and Poisson approximation in combinatorial applications.
\begin{prop}
Suppose $\left(Q_{\i}\right)_{\i=1}^{n}$ is a sequence of zero-one
random variables, such that $\X=\sum_{\i=1}^{n}Q_{\i}$. Also suppose
that \textup{$\left(\X^{\left(\i\right)}\right)_{\i=1}^{n}$, $\left(Q_{\i}\right)_{\i=1}^{n}$
and $\X$ are coupled in such a way that }$\X^{\left(\i\right)}\in\Lb{\X|Q_{i}=1}$
for each $\i$. Let $I$ be a random variable independent of all the
random variables defined so far, with distribution satisfying $\L I\left(i\right)=\Pr\left(Q_{i}=1\right)/\E\X$.
Then $\X^{\left(I\right)}\in\Ls{\X}$.\end{prop}
\begin{proof}
We have 
\begin{alignat*}{1}
\Pr(\X^{\left(I\right)}=k) & =\sum_{\i=1}^{n}\frac{\Pr\left(Q_{\i}=1\right)}{\E\X}\Pr\left(\X=k|Q_{\i}=1\right)\\
 & =\frac{1}{\E\X}\sum_{\i=1}^{n}\Pr\left(\X=k\mbox{ and }Q_{\i}=1\right).
\end{alignat*}
Now, when $\X=k$, exactly $k$ of the events $\left\{ Q_{\i}=1\right\} $
will hold, so
\[
\sum_{\i=1}^{n}\Pr\left(\X=k\mbox{ and }Q_{\i}=1\right)=\L{\X}\left(k\right).
\]
Then, the discrete definition \ref{eq:discrete-size-bias-definition}
for the size-bias distribution proves $\X^{\left(I\right)}\in\Ls{\X}$.\end{proof}
\begin{example}
[adapted from {\cite[Example~4.21]{Ros11}}]\label{example:isolated-vertices}We
say a vertex in a graph is \emph{isolated }if it has no neighbours.
Fix $n$ and $p$, and let $G\in\rG np$ (see \ref{sub:random-structures}).
Let 
\[
Q_{\i}=\one\left\{ \i\mbox{ is an isolated vertex in }G\right\} ,
\]
so that $\X=\sum_{\i=1}^{n}Q_{\i}$ is the number of isolated vertices
in $G$. Now, the event $Q_{\i}=1$ means that $\i$ has no incident
edges. By the independence in the definition of $\rG np$, the distribution
of $\Lb{\X|Q_{i}=1}$ can be realized as the number of isolated vertices
in a graph obtained by deleting from $G$ all edges incident to $\i$.
By symmetry, each $\Pr\left(Q_{i}=1\right)$ is equal, so we can couple
$\s{\X}$ with $\X$ by letting $\s{\X}$ be the number of isolated
vertices in the graph obtained by picking a vertex in $G$ at random
and deleting all its adjacent edges.

Now, the distribution of $\X$ depends on the relationship between
$n$ and $p$. First, note that each $Q_{\i}$ is ``almost'' independent.
Next, the probability that a vertex is isolated is $\left(1-p\right)^{n-1}$
because the $n-1$ potential edges incident to that vertex must independently
not be present. So if $p$ is not vanishingly small compared to $n$
then isolated points are a ``rare event'' and we might expect $\X$
to have an approximate Poisson distribution. If $p$ is chosen to
be a function of $n$ which vanishes at just the right rate, then
we might expect the central limit theorem to approximately hold, so
that $\X$ has an approximate normal distribution. We will therefore
return to this example to illustrate the application of size-bias
coupling for both normal and Poisson approximation.

In both cases, we will need to compute $\E\X$ and $\Var\X$, so we
do this now. For all expectation calculations in this section, we
will use the idea of \ref{rem:indicator-expectation}. We can immediately
deduce that 
\begin{equation}
\E\X=n\left(1-p\right)^{n-1}.\label{eq:isolated-vertices-expectation}
\end{equation}
For a pair of distinct vertices, the probability that both are isolated
is $\left(1-p\right)^{2n-3}$, so
\begin{alignat*}{1}
\E{\X \choose 2} & =\E\left[\X^{2}/2-\X/2\right]={n \choose 2}\left(1-p\right)^{2n-3}
\end{alignat*}
and
\begin{alignat}{1}
\Var\X & =n\left(n-1\right)\left(1-p\right)^{2n-3}+n\left(1-p\right)^{n-1}-n^{2}\left(1-p\right)^{2n-2}\nonumber \\
 & =n\left(1-p\right)^{n-1}\left(1+\left(np-1\right)\left(1-p\right)^{n-2}\right).\label{eq:isolated-vertices-variance}
\end{alignat}

\end{example}

\subsection{Normal Approximation}

For a nonnegative random variable $\X$, let $\mu=\E\X$ and $\sigma^{2}=\Var\X$.
Suppose we believe $W=\left(\X-\mu\right)/\sigma$ is approximately
distributed as $\N\left(0,1\right)$. In order to prove this, our
objective is to compare $\E\left[f\left(W\right)\right]$ to $\E\left[f'\left(W\right)\right]$,
as in the proof of \ref{thm:berry-esseen}.

Now, for any function $f:\RR\to\RR$, by considering \ref{eq:size-bias-transformation}
with the function $x\mapsto f\left(\left(\X-\mu\right)/\sigma\right)$,
we have
\[
\E\left[Wf\left(W\right)\right]=\E\left[\frac{\X-\mu}{\sigma}f\left(\frac{\X-\mu}{\sigma}\right)\right]=\frac{\mu}{\sigma}\E\left[f\left(\frac{\s{\X}-\mu}{\sigma}\right)-f\left(\frac{\X-\mu}{\sigma}\right)\right].
\]
A Taylor expansion (assuming $f$ is twice-differentiable) then gives
\[
\E\left[Wf\left(W\right)\right]=\frac{\mu}{\sigma}\E\left[\frac{\s{\X}-\X}{\sigma}f'\left(\frac{\X-\mu}{\sigma}\right)+\frac{1}{2}\left(\frac{\s{\X}-\X}{\sigma}\right)^{2}f''\left(E\right)\right],
\]
for some (random) $E$. So, for $\T_{\N}$ as defined in \ref{thm:normal-characterizing},
we have 
\[
\left|\E_{W}\T_{\N}f\right|\le\frac{3\mu}{\sigma^{2}}\sqrt{\frac{\pi}{2}}\E\left|\frac{\sigma^{2}}{\mu}-\E^{\X}\left[\s{\X}-\X\right]\right|+\frac{3\mu}{\sigma^{3}}\E\left(\s{\X}-\X\right)^{2}.
\]
for $f$ satisfying the bounds in \ref{thm:wasserstein-normal-bounds}.
(We have used the tower law of expectation, see \ref{prop:tower-contractivity}).
Now, by considering the function $x\mapsto x$ in \ref{eq:size-bias-transformation},
observe that 
\[
\E\s{\X}=\frac{\E\X^{2}}{\E\X}=\frac{\sigma^{2}+\mu^{2}}{\mu},
\]
so $\E\left[\E^{\X}\left[\s{\X}-\X\right]\right]=\E\left[\s{\X}-\X\right]=\sigma^{2}/\mu$.
H\"older's inequality (\cite[Theorem~3.6~(1)]{Rud66}) for the $L^{2}$
norm then gives
\[
\E\left|\frac{\sigma^{2}}{\mu}-\E^{\X}\left[\s{\X}-\X\right]\right|\le\sqrt{\Var\left(\E^{\X}\left[\s{\X}-\X\right]\right)},
\]
which allows us to conclude
\begin{equation}
\left|\E_{W}\T_{\N}f\right|\le\frac{3\mu}{\sigma^{2}}\sqrt{\frac{\pi}{2}}\sqrt{\Var\left(\E^{\X}\left[\s{\X}-\X\right]\right)}+\frac{3\mu}{\sigma^{3}}\E\left(\s{\X}-\X\right)^{2}.\label{eq:size-bias-normal-bound}
\end{equation}
We will be able to show that $d_{\W}\left(\X,\N\left(0,1\right)\right)$
is small when $\s{\X}$ is coupled in such a way that it is a slight
perturbation of $\X$. We will show how $d_{\W}\left(\X,\N\left(0,1\right)\right)$
can be bounded in practice with the continuation of \ref{example:isolated-vertices}.
\begin{example}
\label{example:isolated-points-normal-example}As in \ref{example:isolated-vertices},
let $\X=\sum_{\i=1}^{n}Q_{\i}$ be the number of isolated vertices
in a random $G\in\rG np$. We should expect $\L{\X}$ to be approximately
normal when $\E Q_{\i}=\L{Q_{\i}}\left(1\right)=\left(1-p\right)^{n-1}$
is approximately constant. Note that $1-p\approx e^{-p}$ for small
$p$, so $\left(1-p\right)^{n-1}\approx e^{-np}$. For the duration
of this example, we will assume $np=\Theta\left(1\right)$. From \ref{eq:isolated-vertices-expectation}
and \ref{eq:isolated-vertices-variance}, it can be shown that $\mu=\Theta\left(n\right)$
and $\sigma=\Theta\left(\sqrt{n}\right)$.

let $I$ be a random vertex, and let $\s{\X}$ be the number of isolated
vertices with the edges incident to a random vertex deleted. For each
vertex $\i$ in $G$, let $D_{\i}$ be the number of vertices adjacent
to $\i$ which have degree $1$, and let $R_{\i}=1-Q_{\i}$ be the
indicator random variable for the event that $\i$ is \emph{not} isolated.
Then, we have 
\[
\s{\X}-\X=D_{I}+R_{I}.
\]
First, note that
\begin{equation}
\E\left(\s{\X}-\X\right)^{2}=\sum_{\i=1}^{n}\frac{1}{n}\E\left(D_{\i}+R_{\i}\right)^{2}\le\sum_{\i=1}^{n}\frac{1}{n}\E\left(D_{\i}+1\right)^{2}=\E\left(D_{\i}+1\right)^{2}.\label{eq:isolated-vertices-E(X^s-X)^2-bound-progress}
\end{equation}
The probability that a particular vertex is a degree-1 neighbour of
the vertex $\i$ is $p\left(1-p\right)^{n-2}$ because 1 particular
edge must be present and $n-2$ particular edges must be absent. So,
\[
\E D_{\i}=\left(n-1\right)p\left(1-p\right)^{n-2}=\Theta\left(1\right).
\]
The probability that two particular vertices are both degree-1 neighbours
of the vertex $\i$ is $p^{2}\left(1-p\right)^{2n-5}$, so 
\begin{alignat*}{1}
\E{D_{\i} \choose 2} & ={n-1 \choose 2}p\left(1-p\right)^{2n-5}.
\end{alignat*}
It follows from \ref{eq:isolated-vertices-E(X^s-X)^2-bound-progress}
that
\begin{equation}
\E\left(\s{\X}-\X\right)^{2}\le\left(n-1\right)p\left(1-p\right)^{n-2}\left(\left(n-2\right)p\left(1-p\right)^{n-3}+3\right)+1=\Theta\left(1\right).\label{eq:isolated-vertices-E(X^s-X)^2-bound}
\end{equation}
Next, by contractivity and the tower law (see \ref{prop:tower-contractivity}),
we have
\begin{equation}
\Var\left(\E^{\X}\left[\s{\X}-\X\right]\right)=\Var\left(\E^{\X}\left[\E^{G}\left[\s{\X}-\X\right]\right]\right)\le\Var\left(\E^{G}\left[\s{\X}-\X\right]\right).\label{eq:isolated-vertices-var-contractive-bound}
\end{equation}
The interpretation of $\E^{G}\left[\s{\X}-\X\right]$ is that $\s{\X}-\X$
is averaged over all choices of the random vertex $I$, for each $G$.
That is, $\E^{G}\left[\s{\X}-\X\right]$ has the distribution of $\sum_{\i=1}^{n}\frac{1}{n}\left(D_{\i}+R_{\i}\right)$.

Now, let $D=\sum_{\i=1}^{n}D_{\i}$ be the number of vertices with
degree 1 in $G$, so that $\E^{G}\left[\s{\X}-\X\right]$ has the
distribution of $\frac{1}{n}\left(D+n-\X\right)$. Let $\hat{D}=D-\E D$
and $\hat{\X}=\X-\E\X$, so that
\[
\Var\left(\E^{G}\left[\s{\X}-\X\right]\right)=\frac{1}{n^{2}}\E(\hat{D}-\hat{\X})^{2}=\frac{1}{n^{2}}\E\left[2\hat{D}^{2}+2\hat{\X}^{2}-(\hat{D}+\hat{\X})^{2}\right]\le\frac{2}{n^{2}}\left(\Var D+\sigma^{2}\right).
\]
The probability that a particular vertex $\i$ has $\ii$ as its only
neighbor is $p\left(1-p\right)^{n-2}$ because 1 particular edge must
be present and $n-2$ particular edges must be absent. So, the probability
that a particular vertex has degree 1 is $\left(n-1\right)p\left(1-p\right)^{n-2}$
and 
\[
\E D=n\left(n-1\right)p\left(1-p\right)^{n-2}.
\]
Consider a particular unordered pair of vertices $\left\{ \i,\ii\right\} $.
There are two possibilities for both $\i$ and $\ii$ to have degree
1. The first possibility is $\i$ and $\ii$ are adjacent, and have
no other neighbours. This event has probability $p\left(1-p\right)^{2n-4}$.
The second possibility is that $\i$ has a single non-$\ii$ neighbour
and $\ii$ has a single non-$\i$ neighbour. This event has probability
$\left(n-1\right)^{2}p^{2}\left(1-p\right)^{2n-5}$. We conclude that
\begin{alignat*}{1}
\E\left[D\left(D-1\right)/2\right] & ={n \choose 2}\left(p\left(1-p\right)^{2n-4}+\left(n-1\right)^{2}p^{2}\left(1-p\right)^{2n-5}\right),\\
\Var D & =n\left(n-1\right)p\left(1-p\right)^{n-2}\left(1+\left(n-1\right)p\left(1-p\right)^{n-3}\left(np-1\right)+\left(1-p\right)^{n-2}\right)\\
 & =O\left(n\right).
\end{alignat*}
It follows that
\begin{alignat}{1}
\Var\left(\E^{G}\left[\s{\X}-\X\right]\right) & \le\Theta\left(\frac{1}{n}\right)\label{eq:isolated-vertices-var-full-bound}
\end{alignat}
Combining Equations \ref{eq:size-bias-normal-bound}, \ref{eq:isolated-vertices-E(X^s-X)^2-bound},
\ref{eq:isolated-vertices-var-contractive-bound} and \ref{eq:isolated-vertices-var-full-bound},
we conclude 
\[
d_{\W}\left(\frac{\X-\mu}{\sigma},\N\left(0,1\right)\right)=O\left(n^{-3/2}\right).
\]

\end{example}

\subsection{Poisson Approximation}

Let $\lambda=\E\X$, and suppose we believe $\L{\X}\approx\Po\left(\lambda\right)$.
Let $\T_{\Po\left(\lambda\right)}$ be the Poisson characterizing
operator defined in \ref{thm:poisson-characterizing}, and suppose
$f$ satisfies the bounds in \ref{thm:poisson-bound}. We have 
\[
\left|\E_{\X}\T_{\Po\left(\lambda\right)}f\right|=\left|\E\left[\X f\left(\X\right)-\lambda f\left(\X+1\right)\right]\right|\le\lambda\E\left|f(\s{\X})-f\left(\X+1\right)\right|.
\]
By the tower law (\ref{prop:tower-contractivity}) and similar reasoning
to the proof of \ref{thm:quantitative-poisson-limit}, it follows
that
\begin{equation}
\left|\E_{\X}\T_{\Po\left(\lambda\right)}f\right|\le\lambda\left\Vert \Delta f\left(\i\right)\right\Vert _{\infty}\E\left|\X+1-\s{\X}\right|\le(1-e^{-\lambda})\E\left|\X+1-\s{\X}\right|.\label{eq:size-bias-poisson-bound}
\end{equation}
We conclude that $d_{\TV}\left(\X,\Po\left(\lambda\right)\right)$
will be small when $\s{\X}$ is coupled such that it effectively increments
$\X$. This is almost exactly what the coupling in \ref{example:isolated-vertices}
accomplishes. We turn now to the completion of that example.
\begin{example}
Recall the definitions of $\X$ and $\s{\X}$ in \ref{example:isolated-vertices},
and the definition and expectation of $D_{\i}$ in \ref{example:isolated-points-normal-example}.
Note that
\[
\E\left|\X+1-\s{\X}\right|=\sum_{\i=1}^{n}\frac{1}{n}\E\left[|\X+1-\s{\X}|\,\middle\vert I=\i\right]
\]
and
\[
\E\left[|\X+1-\s{\X}|\,\middle\vert I=\i\right]=\E\left|Q_{\i}-D_{\i}\right|\le\E Q_{\i}+\E D_{\i}=\left(1-p\right)^{n-1}+\left(n-1\right)p\left(1-p\right)^{n-2}.
\]
Recall that $\lambda=\E\X=n\left(1-p\right)^{n-1}$ so with \ref{eq:size-bias-poisson-bound}
we have 
\begin{alignat*}{1}
d_{\TV}\left(\X,\Po\left(\lambda\right)\right) & \le(1-e^{-\lambda})\left(\left(1-p\right)^{n-1}+\left(n-1\right)p\left(1-p\right)^{n-2}\right)\\
 & \le\lambda^{2}(1-e^{-\lambda})\left(\frac{1}{n}+\frac{p}{1-p}\right).
\end{alignat*}
As we guessed in \ref{example:isolated-vertices}, this means $\X$
is approximately Poisson when $n$ is large and $p$ is not too small,
so that $\lambda$ is small and $\frac{p}{1-p}$ is not too big.
\end{example}

\section{The Method of Exchangeable Pairs\label{sec:exchangeable-pairs}}

\global\long\def\fX{f}


\global\long\def\hX{\h}


\global\long\def\XX{\cX_{\X}}


\global\long\def\eX{\mathbf{X}}


\global\long\def\FX{\cF_{\X}}


\global\long\def\TX{\T_{\eX}}


\global\long\def\OmX{\Om_{\X}}


\global\long\def\OmeX{\Om_{\eX}^{\left(2\right)}}


\global\long\def\conn{\alpha}


\global\long\def\x#1{x^{\left(#1\right)}}


In \cite{Ste86}, Stein introduced a method for the construction of
a characterizing operator using an object called an \emph{exchangeable}
\emph{pair}. Up until this point, we have only been interested in
characterizing operators for our ``special'' distributions $\Lo$
to which we compare our distribution of interest $\L{\X}$. The exchangeable
pairs construction can be used for this purpose, but in this section
we will be more interested in a technique for applying Stein's method
that involves the construction of a characterizing operator for $\L{\X}$.

The general idea is that we define an exchangeable pair $\eX$, and
by a general construction this gives a characterizing operator $\TX$
for our distribution of interest $\L{\X}$. We then use an operator
$\conn$ to connect the domains of $\TX$ and $\To$ in such a way
that $\TX\conn$ approximates $\To$. By the definition of a characterizing
operator, we have $\E_{\X}\To=\E_{\X}\left(\To-\TX\conn\right)$,
so we can use the fact that $\To-\TX\conn$ is small to bound $\E_{\X}\To\fo$.

This method has been applied in some generality, but for simplicity,
we assume throughout that the random variable of interest $\X$ has
finite support $\OmX$.
\begin{example}
[adapted from {\cite[Example~4.21]{Ros11}}]\label{example:fixed-points}Throughout
this section, we will refer to an example problem to illustrate the
principles of Stein's method for exchangeable pairs.

We will say a \emph{fixed point} of a permutation $\sigma\in S_{n}$
is an index $k\in\range n$ that satisfies $\sigma\left(k\right)=k$.
Let $\X:S_{n}\to\left\{ 0\right\} \cup\range n$ give the number of
fixed points in each permutation from $S_{n}$. We interpret $\X$
as a random variable on the underlying uniform space $\rS n$.

Now, if $n$ is large then fixed points are largely independent of
each other, and each of $n$ indices has a probability of $1/n$ to
be a fixed point. So (recalling \ref{rem:indicator-expectation}),
we might expect $\L{\X}$ to be ``close'' to $\Po\left(1\right)$.
We will attempt to bound $d_{\TV}\left(\X,\Po\left(1\right)\right)$
to validate and quantify this intuition.\end{example}
\begin{defn}
A 2-dimensional random pair $\eX=\left(\X_{1},\X_{2}\right)$ is an
\emph{exchangeable pair} if $\Lb{\X_{1},\X_{2}}=\Lb{\X_{2},\X_{1}}$.
We will denote the support of $\eX$ by $\OmeX$ (this is a subset
of $\OmX^{2}$).
\end{defn}
That is, a pair $\eX$ is exchangeable if exchanging the components
of the pair does not change their joint distribution. In particular,
the marginal distributions of $\X_{1}$ and $\X_{2}$ must be the
same.
\begin{prop}
There is a natural equivalence between time-homogeneous reversible
Markov \textbf{c}hains with steady-state distribution $\L{\X}$, and
exchangeable pairs with margins $\L{\X}$.\end{prop}
\begin{proof}
Given an exchangeable pair $\eX$ with margins $\L{\X}$, we can define
a time-homogenous Markov chain $M$ with transition probabilities
$p\left(x_{1},x_{2}\right)=\Pr\left(\X_{2}=x_{2}|\X_{1}=x_{1}\right)$.
As in \ref{sub:markov-chains}, let $\pi\left(x\right)=\L{\X}\left(x\right)$.
We have
\[
\pi\left(x_{1}\right)p\left(x_{1},x_{2}\right)=\L{\bX}\left(x_{1},x_{2}\right)=\L{\bX}\left(x_{2},x_{1}\right)=\pi\left(x_{2}\right)p\left(x_{2},x_{1}\right)
\]
for any $x_{1},x_{2}\in\OmX$. So, $M$ is reversible with steady-state
distribution $\L{\X}$. Of course, given a reversible Markov \textbf{c}hain
with steady-state distribution $\L{\X}$, we can define the distribution
$\L{\bX}$ in exactly the same way.\end{proof}
\begin{defn}
We say an exchangeable pair $\eX$ is \emph{connected} if the corresponding
Markov \textbf{c}hain is irreducible.\end{defn}
\begin{rem}
\label{rem:underlying-pair}We are particularly interested in exchangeable
pairs $\eX$ with marginal distributions $\L{\X_{1}}=\L{\X_{2}}=\L{\X}$.
If $\X$ is defined on an underlying combinatorial probability space
$\left(\Om,2^{\Om},\Pr\right)$, it is often convenient to first construct
an exchangeable pair $\mathbf{W}=\left(W_{1},W_{2}\right)$ with margins
$\Pr$, so that the vector $\eX_{\mathbf{W}}=\left(\X\left(W_{1}\right),\X\left(W_{2}\right)\right)$
is an exchangeable pair with margins $\L{\X}$. If $\left(W_{1},W_{2}\right)$
is connected, then $\eX_{\mathbf{W}}$ is connected also.\end{rem}
\begin{example}
\label{example:fixed-points-exchangeable-pair}We continue \ref{example:fixed-points}.
Define a specific exchangeable pair $\mathbf{W}=\left(W_{1},W_{2}\right)$
with margins $\rS n$ by 
\[
\Pr\left(\left(W_{1},W_{2}\right)=\left(\sigma_{1},\sigma_{2}\right)\right)=\begin{cases}
\left(n!{n \choose 2}\right)^{-1} & \mbox{if }\sigma_{1}=\sigma_{2}\left(\i\,\ii\right)\mbox{ for some transposition \ensuremath{\left(\i\,\ii\right)}},\\
0 & \mbox{otherwise}.
\end{cases}
\]
The relation of differing by a transposition is symmetric, so $\mathbf{W}$
is indeed an exchangeable pair. The Markov \textbf{c}hain associated
with $\mathbf{W}$ has a simple interpretation. Given a random permutation
$\sigma$, to make a transition in the Markov \textbf{c}hain we just
randomly choose one of the ${n \choose 2}$ possible transpositions
and compose it with $\sigma$. Because the transpositions generate
$S_{n}$, the pair $\mathbf{W}$ is connected, so we can use the construction
from \ref{rem:underlying-pair} to produce a connected exchangeable
pair $\eX$ with margins $X$.
\end{example}
The Markov \textbf{c}hain underlying a connected exchangeable pair
can be naturally viewed as a connected one-dimensional simplicial
complex. The zeroth reduced homology group $\ker\partial_{0}/\im\partial_{1}$
of a connected simplicial complex is trivial, and this motivates the
construction of a characterizing operator in a natural way. (The following
theorem is self-contained and requires no knowledge of homology).
\begin{thm}
\label{thm:characterizing-from-exchangeable-pair}Suppose\textup{
$\eX$ }is a connected exchangeable pair with margins $\L{\X}$. Let
$\FX$ be the set of functions $\fX:\OmX^{2}\to\RR$ which are antisymmetric
in the sense that $\fX\left(x_{1},x_{2}\right)=-\fX\left(x_{2},x_{1}\right)$.
Let $\XX$ be the set of functions $\hX:\OmX\to\RR$.

Define $\TX:\FX\to\XX$ by $\TX\fX\left(x\right)=\sum_{x_{2}\in\OmX}\fX\left(x,x_{2}\right)p\left(x,x_{2}\right)=\E\left[\fX\left(\eX\right)|\X_{1}=x\right]$,
so that $\TX\fX\left(\X\right)$ is distributed as $\E^{\X_{1}}\fX\left(\eX\right)$.
Then $\TX$ is a characterizing operator for $\X$.\end{thm}
\begin{rem}
This theorem appears in \cite[Section~4]{Ste92}, but the proof there
appears to be incorrect. I have adjusted it fairly significantly.\end{rem}
\begin{proof}
To see that $\im\TX\subseteq\ker\E_{\X}$, fix $\fX\in\FX$ and note
that by the tower law of expectation (\ref{prop:tower-contractivity}),
\[
\E_{\X}\TX\fX=\E\E^{\X_{1}}\fX\left(\eX\right)=\E\fX\left(\eX\right).
\]
By exchangeability and antisymmetry, $\E\fX\left(\eX\right)=\E\fX\left(\X_{2},\X_{1}\right)=-\E\fX\left(\eX\right)$,
so we can deduce $\E\fX\left(\X_{1},\X_{2}\right)=\E_{\X}\TX\fX=0$.
This did not require the connectedness condition. We can similarly
prove that $\TX$ is well-defined as an operator from $\FX$ to $\XX$:
note that for $\fX\in\FX$ we have $\E_{\X}\left|\TX\fX\right|\le\E\left|\fX\left(\eX\right)\right|<\infty$
by contractivity, so $\TX\fX\in\XX$.

We will next prove $\ker\E_{\X}\subseteq\im\TX$, but first we make
some definitions. For each $x\in\OmX$, let $\hX_{x}$ be the function
that takes the value $\pi\left(x\right)^{-1}$ on $x$ and is zero
elsewhere, so that 
\[
\hX=\sum_{x\in\OmX}\hX\left(x\right)\pi\left(x\right)\hX_{x}
\]
 for each $\hX\in\XX$. For each $\left(x_{1},x_{2}\right)\in\OmeX$,
define $\fX_{x_{1},x_{2}}\in\FX$ as the function that takes the value
${\displaystyle \left(\pi\left(x_{1}\right)p\left(x_{1},x_{2}\right)\right)^{-1}}$
on $\left(x_{1},x_{2}\right)$, takes the value $-\left(\pi\left(x_{2}\right)p\left(x_{2},x_{1}\right)\right)^{-1}$
on $\left(x_{2},x_{1}\right)$, and takes the value zero elsewhere.
Note that this function is antisymmetric by the reversibility of the
Markov \textbf{c}hain of $\eX$. We have $\TX\fX_{x_{1},x_{2}}=\hX_{x_{2}}-\hX_{x_{1}}$.

Let $\hX\in\ker\E_{\X}$, and fix an arbitrary $x^{*}\in\OmX$. By
the connectedness assumption, for each $x\in\OmX$ there is a sequence
\[
x=\x 0,\x 1,\dots,\x{k-1},\x k=x^{*}
\]
 with $\left(\x{\i-1},\x{\i}\right)\in\OmeX$ for $\i\in\range k$.
Define
\[
\fX_{x}^{*}=\sum_{\i=1}^{k}\fX_{\x{\i-1},\x{\i}},
\]
so that $\hX_{x^{*}}-\hX_{x}=\TX\fX_{x}^{*}$ by linearity. It follows
that
\[
\hX=\sum_{x\in\OmX}\hX\left(x\right)\pi\left(x\right)\left(\hX_{x^{*}}-\TX\fX_{x}^{*}\right)=\sum_{x\in\OmX}\left(\E_{\X}\hX\right)\hX_{x^{*}}-\TX\sum_{x\in\OmX}\hX\left(x\right)\pi\left(x\right)\fX_{x}^{*}\in\im\TX.
\]
Once we have defined an exchangeable pair and used it to construct
$\TX$, the final step is to choose an operator $\conn:\Fo\to\FX$
in such a way that $\To$ can be easily compared with $\TX\conn$.
\end{proof}
\begin{comment}

It's not clear if characterizing operators are in any sense unique
so that for two characterizing operators $\T_{1},\T_{2}$ there is
\emph{always} a connection $\conn$ that makes $\T_{1}=\T_{2}\conn$.

\end{comment}
\begin{example}
\label{example:poisson-connecting-operator}For the Poisson case in
\ref{thm:poisson-characterizing}, we need to compare $\lambda\fo\left(\X+1\right)$
with $\X\fo\left(\X\right)$. It is often fruitful to define $\conn$
by 
\[
\conn\fo\left(x_{1},x_{2}\right)=c\fo\left(x_{2}\right)\one\left\{ x_{2}=x_{1}+1\right\} -c\fo\left(x_{1}\right)\one\left\{ x_{1}=x_{2}+1\right\} 
\]
for some $c\in\RR$. We will then have
\[
\left(\TX\conn f\right)\left(x\right)=c\fo\left(x+1\right)\Pr\left(\X_{2}=\X_{1}+1|\X_{1}=x\right)-c\fo\left(x\right)\Pr\left(\X_{1}=\X_{2}+1|\X_{1}=x\right)
\]
so $\E_{\X}\T_{\Po\left(\lambda\right)}\fo=\E_{\X}\left(\T_{\Po\left(\lambda\right)}\fo-\TX\conn\fX\right)$
equals 
\[
\E\left[\fo\left(\X_{1}+1\right)\left(\lambda-c\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\right)-\fo\left(\X_{1}\right)\left(\X_{1}-c\Pr\left(\X_{1}=\X_{2}+1|\X_{1}\right)\right)\right].
\]
Using the triangle inequality, we have 
\begin{equation}
\left|\E_{\X}\T_{\Po\left(\lambda\right)}\fo\right|\le\min(2,3\lambda^{-1/2})\left(\vphantom{\sum}\E\left|\lambda-c\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\right|+\E\left|\X_{1}-c\Pr\left(\X_{1}=\X_{2}+1|\X_{1}\right)\right|\right)\label{eq:EXT0-immigration-death-bound}
\end{equation}
for all $\fo$ satisfying the bound in \ref{thm:poisson-bound}. This
bound is effective when 
\begin{equation}
\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\approx\lambda/c,\quad\Pr\left(\X_{2}=\X_{1}-1|\X_{1}\right)\approx\X_{1}/c.\label{eq:immigration-death}
\end{equation}
The interpretation of these approximate equalities is that the Markov
\textbf{c}hain associated with $\eX$ is approximately an immigration-death
process. This is likely to happen when $\X\left(\om\right)$ is in
some sense a statistic of the amount of local structure over the object
$\om$, and $\eX$ is defined by a Markov \textbf{c}hain on $\Om$
(as in \ref{rem:underlying-pair}) that (uniformly) randomly disturbs
local structure. The conclusion to \ref{example:fixed-points}, presented
in \ref{example:fixed-point-conclusion} below, should make this clear.
\end{example}
\begin{comment}

I should include a sub-$\sigma$-algebra note as in \cite{Joh11}
to simplify the conditional expectation stuff (I'd rather condition
on the permutation than on its number of fixed points).

\end{comment}


\begin{example}
\label{example:fixed-point-conclusion}We continue with \ref{example:fixed-points},
recalling the exchangeable pairs $\mathbf{W}$ and $\eX$ from \ref{example:fixed-points-exchangeable-pair}.
The interpretation of 
\[
\Pr\left(\X_{2}=\X_{1}-1|\X_{1}\right)
\]
is the probability of a transposition destroying exactly one out of
the existing $\X_{1}$ fixed points. In order to destroy exactly one
fixed point, we have to choose a fixed point to destroy, and swap
it with a non-fixed-point. There are $\X_{1}\left(n-\X_{1}\right)$
out of ${n \choose 2}$ transpositions that do this, so
\[
\Pr\left(\X_{2}=\X_{1}-1|\X_{1}\right)=\frac{\X_{1}\left(n-\X_{1}\right)}{{n \choose 2}}.
\]
Next, we will find a formula for $\Pr\left(\X\left(W_{2}\right)=\X\left(W_{1}\right)+1|W_{1}\right)$,
noting that 
\[
\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)=\E\left[\Pr\left(\X\left(W_{2}\right)=\X\left(W_{1}\right)+1|W_{1}\right)|\X_{1}\right].
\]
In order to create exactly one new fixed point, we have to choose
an index $k$ that is not fixed in $W_{1}$ (there are $n-\X_{1}$
such) and compose $W_{1}$ with the transposition $\left(k\;\sigma\left(k\right)\right)$.
This creates exactly one fixed point unless $\sigma^{-1}\left(k\right)=k$,
in which case it creates two. We have counted this second case twice
for every transposition in the cycle decomposition of $W_{1}$. Let
$Y$ be the number of transpositions in the cycle decomposition of
$W_{1}$. We have 
\[
\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)=\frac{n-\X_{1}-2\E\left[Y|\X_{1}\right]}{{n \choose 2}}.
\]
In order to satisfy \ref{eq:immigration-death} as closely as possible,
we choose $c={n \choose 2}/n$. Recalling that $\E\X_{1}=1$, we then
have
\begin{align}
\E\left|1-c\Pr\left(X_{2}=X_{1}+1|X_{1}\right)\right| & =\E\left[1-\frac{n-\X_{1}-2\E\left[Y|\X_{1}\right]}{n}\right]\nonumber \\
 & =1/n+2\E Y/n,\label{eq:fixed-point-progress-1}\\
\E\left|X_{1}-c\Pr\left(X_{2}=X_{1}-1|X_{1}\right)\right| & =\E\left[\X_{1}-\frac{\X_{1}\left(n-\X_{1}\right)}{n}\right]\nonumber \\
 & =\E\X_{1}^{2}/n.\label{eq:fixed-point-progress-2}
\end{align}
Now, the probability that a transposition $\left(\i\,\ii\right)$
is in the cycle decomposition of $W_{1}$ is $\left(n\left(n-1\right)\right)^{-1}$
because $\i$ must map to $\ii$ out of the $n$ possible options
in $\range n$, then $\ii$ must map to $\i$ out of the $n-1$ possible
options in $\range n\backslash\left\{ \ii\right\} $. There are ${n \choose 2}=n\left(n-1\right)/2$
possible transpositions so, by \ref{rem:indicator-expectation}, it
follows that $\E Y=1/2$.

Next, $\E{\X_{1} \choose 2}$ is the expected number of unordered
pairs of distinct fixed points in a permutation $\sigma\in\rS n$.
For any unordered pair of distinct indices $\left\{ \i,\ii\right\} $,
the probability that both are fixed is $\left(n\left(n-1\right)\right)^{-1}$
because $\i$ must map to $\i$ out of the $n$ possible options in
$\range n$, then $\ii$ must map to $\ii$ out of the $n-1$ possible
options in $\range n\backslash\left\{ \i\right\} $. Again applying
\ref{rem:indicator-expectation}, we have $\E{\X_{1} \choose 2}=\E\left[\X_{1}^{2}/2-\X_{1}/2\right]={n \choose 2}\left(n\left(n-1\right)\right)^{-1}=1/2$
and $\E\left[\X_{1}^{2}\right]=2$.

Combining \ref{eq:stein-distance-transform}, \ref{eq:EXT0-immigration-death-bound},
\ref{eq:fixed-point-progress-1} and \ref{eq:fixed-point-progress-2},
we can conclude that $d_{\TV}\left(\X,\Po\left(1\right)\right)\le8/n$.
\end{example}
We stress that this example was included only for illustrative purposes.
Since $d_{\TV}$ has a relatively direct probabilistic interpretation,
and since permutations are very simple, well-understood combinatorial
objects, it is actually possible to analyze $d_{\TV}\left(\X,\Po\left(1\right)\right)$
with elementary (but complicated) combinatorics. One can obtain super-exponential
bounds which are much better than the bound we obtained by Stein's
method (see \cite[Section~1.1]{ABT03}). However, such elementary
analysis is sometimes not possible, as we will now see.

\begin{comment}

The ``generator method'' \cite{BC05} says that the Poisson characterizing
operator can be obtained with the generator of an immigration-death
process and the Normal characterizing operator can be obtained with
the generator of an O-U process. Investigate the link here?

\end{comment}


\subsection{Switchings and Short Cycles in Random Regular Graphs}

A standard way to enumerate combinatorial objects is by way of a recurrence.
That is, we try to show that every object can be built up from independent
substructures, so the number of objects of a given size is some function
of the number of objects of smaller sizes. The most difficult combinatorial
structures to enumerate and study, then, are those that are constrained
in some way so that they exhibit self-dependence, meaning that they
cannot be built up from substructures. For example, regular graphs
are graphs where each vertex has the same degree. There is no simple,
uniform way to construct a regular graph from regular graphs of smaller
degree or with fewer vertices.

Combinatorial\emph{ switchings} are a powerful tool for the analysis
of these kinds of combinatorial structures. In particular, they have
been applied with great success to the enumeration of regular graphs
and Latin squares.

A particularly interesting application of Stein's method for exchangeable
pairs is to piggyback onto results proved using switchings. This idea
was very recently introduced by Johnson \cite{Joh11}. In this subsection,
I'll describe how Johnson improved a certain Poisson \textbf{l}imit
\textbf{t}heorem concerning short cycles in random regular graphs,
in a quite natural, and generally applicable way. First, we give some
background on short cycles in random regular graphs, and switchings.


\subsubsection{Short Cycles in Random Regular Graphs}

Let $\X_{\ell,n}^{\left(d\right)}$ be the number of cycles of length
$\ell$ in a random $d$-regular graph on $n$ vertices. For all asymptotics
we will assume $n\to\infty$ in such a way that $nd$ is always even
(otherwise there are no $d$-regular graphs on $n$ vertices). The
following is a critical theorem describing the structure of random
regular graphs.
\begin{thm}
\label{thm:short-cycles-fixed}Fix $\ell\in\NN$ and $d\in\NN$ with
$d\ge3$. Then
\[
\Lb{\X_{\ell,n}^{\left(d\right)}}\to\Po\left(\frac{\left(d-1\right)^{\ell}}{2\ell}\right)
\]
as $n\to\infty$.
\end{thm}
This theorem is most easily proved with the method of moments; see
\cite[Theorem~2.5]{Wor99}.
\begin{rem}
The full theorem is that for any $r\in\NN$, the vector of short cycle
counts $\left(\X_{1,n}^{\left(d\right)},\dots\X_{r,n}^{\left(d\right)}\right)$
is asymptotically distributed as an \emph{independent} vector of Poisson
random variables. That is, not only do we understand the distributions
of the short cycle counts, we also know that these cycle counts do
not interact with each other, asymptotically speaking. Since we have
not discussed the weak topology on random vectors, all the results
and discussion in this subsection are simplified to the single-variable
case.
\end{rem}

\begin{rem}
\label{rem:short-cycles-are-rare}One reason \ref{thm:short-cycles-fixed}
is interesting is because it tells us the number of ``short cycles''
in a random regular graph does not grow (to infinity) with the size
of the graph. That is, random regular graphs are likely to ``locally''
``look like forests''.
\end{rem}
\ref{thm:short-cycles-fixed} can be extended to the case where $d$
is allowed to grow modestly with $n$. In \cite{MWW04}, a natural
variant of \ref{thm:short-cycles-fixed} was proved for $\left(d-1\right)^{2\ell-1}=o\left(n\right)$,
using the idea of \emph{switchings}. The condition $\left(d-1\right)^{2\ell-1}=o\left(n\right)$
appeared to be a natural threshold; it was conjectured that short
cycle counts are no longer asymptotically Poisson if $d$ grows any
faster.

It was recently proved that in fact cycle counts remain asymptotically
Poisson past this boundary:
\begin{thm}
[\cite{Joh11}]\label{thm:short-cycles-variable}Let $\ell$ and $d$
depend on $n$, in such a way that $d\ge3$ and 
\[
\sqrt{\ell}\left(d-1\right)^{3\ell/2-1}=o\left(n\right).
\]
Let $n\to\infty$ in such a way that $nd$ is always even. Then,
\[
d_{\TV}\left(\X_{\ell,n}^{\left(d\right)},\Po\left(\frac{\left(d-1\right)^{\ell}}{2\ell}\right)\right)\to0.
\]
as $n\to\infty$.
\end{thm}
The proof of \ref{thm:short-cycles-variable} combines switchings
with Stein's method.


\subsubsection{Switchings\label{sub:switchings}}

Formally speaking, a switching is a binary relation $\rightsquigarrow$
on a set of combinatorial objects $\Om$ (in our case, $\Om$ is the
set of $d$-regular graphs on $n$ vertices). Usually, a switching
is understood as an ``action'' that changes one combinatorial object
to another. The switching used in \cite{MWW04} is defined as follows:
\begin{defn}
\label{def:cycle-switching}Let $C=v_{1}\dots v_{\ell}v_{1}$ be a
cycle of length $\ell$ in a $d$-regular graph $G$. For each $\i\in\ZZ/\ell\ZZ$,
suppose $e_{\i}=u_{\i}w_{\i+1}\in E\left(G\right)$ satisfies $u_{\i}v_{\i}\notin E\left(G\right)$
and $v_{\i}w_{\i}\notin E\left(G\right)$. (Each of the $3\ell$ vertices
in $v_{\i},u_{\i},w_{\i}$ must be distinct). Let $G'$ be the $d$-regular
graph obtained from $G$ by deleting each $e_{\i}$ and all the edges
in $C$ and adding each of the edges $u_{\i}v_{\i}$ and $v_{\i}w_{\i}$.
Suppose that no cycles of length less than or equal to $\ell$ other
than $C$ are created or destroyed by this process. Then, we say $G\rightsquigarrow G'$.
\ref{fig:cycle-switching} below illustrates the idea.
\end{defn}
\begin{figure}[h]
\begin{center}
\hspace{0.5cm}
\foreach \j in {1,2}
{
\begin{tikzpicture}

\def \iN {5}
\def \oN {10}
{
 \foreach \s in {1,...,\iN}
 {
  \node[vertex] (i\s) at ({90+360/\iN * (\s - 1)}:1cm){};% {$\s$};
  \node at ({90+360/\iN * (\s - 1)}:0.67cm){$v_\s$};
  \node at ({90+360/\iN * (\s - 0.75)}:2.42cm){$u_\s$};
  \node at ({90+360/\iN * (\s - 1.25)}:2.42cm){$w_\s$};
 }
 \foreach \s in {1,...,\oN}
 {
  \node[vertex] (o\s) at ({360/\oN * (\s - 1)}:2cm){};% {$\s$};
 }
}

\ifthenelse{\equal{\j}{1}}{
\path
(i1) edge (i2)
(i2) edge (i3)
(i3) edge (i4)
(i4) edge (i5)
(i5) edge (i1)
(o2) edge (o3)
(o4) edge (o5)
(o6) edge (o7)
(o8) edge (o9)
(o10) edge (o1);
\node[font=\Huge] at (3.8cm,0) {$\rightsquigarrow$};
}
{
\path
(i1) edge (o3) edge (o4)
(i2) edge (o5) edge (o6)
(i3) edge (o7) edge (o8)
(i4) edge (o9) edge (o10)
(i5) edge (o1) edge (o2);
}
\end{tikzpicture}
\hspace{0.5cm}
}
\end{center}

\caption{\label{fig:cycle-switching}A switching with $\ell=5$ (unaffected
edges and vertices are not pictured).}


\end{figure}


The typical application of a switching is to estimate the relative
sizes of some subsets of $\Om$, by analysing the ``flow'' of switchings
between the subsets. The switching is generally designed in such a
way that this ``flow'' is maximized in a certain ``direction''.
See \cite{HM10} for a discussion of switchings in generality.

For our case, define $S\left(k\right)$ to be the set of $d$-regular
graphs with exactly $k$ cycles of length $\ell$. The switching in
\ref{def:cycle-switching} has been chosen so that it attempts to
remove exactly one short cycle in a graph, so that for each $k$ the
``flow'' between $S\left(k\right)$ and $S\left(k-1\right)$ is
large.

In \cite[Section~3]{MWW04}, an estimate was obtained for the average
number of ways $F\left(k\right)$ to ``switch into $S\left(k-1\right)$''
from $S\left(k\right)$ (that is, the average number of $G'\in S\left(k-1\right)$
which satisfy $G\rightsquigarrow G'$, where $G$ ranges over $S\left(k\right)$).
Similarly, an estimate was obtained for the average number of ways
$B\left(k-1\right)$ to ``switch from $S\left(k\right)$'' into
$S\left(k-1\right)$ (that is, the average number of $G\in S\left(k\right)$
satisfying $G\rightsquigarrow G'$, where $G'$ ranges over $S\left(k-1\right)$).
Then, $S\left(k\right)F\left(k\right)=S\left(k-1\right)B\left(k-1\right)$
is the number of pairs $\left(G,G'\right)\in S\left(k\right)\times S\left(k-1\right)$
with $G\rightsquigarrow G'$.

The estimates on $F\left(k\right)$ and $B\left(k\right)$ can be
combined for an estimate of 
\[
\frac{\Pr\left(\X_{\ell,n}^{\left(d\right)}=k\right)}{\Pr\left(\X_{\ell,n}^{\left(d\right)}=0\right)}=\frac{S\left(k\right)}{S\left(0\right)}=\frac{B\left(0\right)\dots B\left(k-1\right)}{F\left(1\right)\dots F\left(k\right)},
\]
which can then be used to estimate the distribution of $\X_{\ell,n}^{\left(d\right)}$.


\subsubsection{Stein's Method and Switchings}

\global\long\def\fG{\mathfrak{G}}


Obvious parallels can be drawn between the application of switchings
in \cite{MWW04} and the application of Stein's method in \ref{example:poisson-connecting-operator}.
$F\left(k\right)$ can be interpreted as the ``probability'' that
the (``forwards'') switching ``destroys a cycle'', and $B\left(k\right)$
can be interpreted as the ``probability'' that the ``backwards''
switching ``creates a cycle'', both given that there are $k$ existing
cycles. These are analogous to the probabilities $\Pr\left(\X_{2}=\X_{1}-1|\X_{1}\right)$
and $\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)$ that were estimated
in \ref{example:poisson-connecting-operator}.

The idea, then, is to use a switching to define a suitable exchangeable
pair, and to adapt the switching estimates for use with Stein's method.
There are two main advantages of this approach over a bare-hands switching
argument. First, a regular switching argument requires individual
estimates of each $F\left(k\right)$ and $B\left(k\right)$, but Stein's
method allows us to estimate the relevant probabilities only in expectation
(effectively, we are only estimating $\E\left[\lambda-cB\left(\X\right)\right]$
and $\E\left[\X-cF\left(\X\right)\right]$, where $\lambda$ is the
parameter of the approximating Poisson distribution). Second, our
final estimate is likely to be sharper when using Stein's method,
because we can take advantage of the sophisticated estimates developed
for Stein's method rather than using elementary ad-hoc estimates.
In particular, the improvement in \ref{thm:short-cycles-variable}
over the natural bound in \cite{MWW04} is due to the term $\lambda^{-1/2}$
in \ref{eq:EXT0-immigration-death-bound}.

The idea is that a switching $\rightsquigarrow$ defines a graph $\fG$
with an edge for every pair $\left(G,G'\right)$ with $G\rightsquigarrow G'$,
whose edges we can weight if desired. Loops are then added as necessary
to so that each vertex has the same weight $\Delta$. By \ref{prop:graph-to-markov},
$\fG$ induces a reversible Markov \textbf{c}hain with uniform stationary
distribution (in our case $\rG nd)$, which corresponds to an exchangeable
pair $\mathbf{G}=\left(G_{1},G_{2}\right)$ with margins $\rG nd$.
With \ref{rem:underlying-pair}, this provides us an exchangeable
pair $\bX=\left(\X_{1},\X_{2}\right)$ with margins $\L{\X}$, which
we use to apply Stein's method.

We give a very simplified sketch of the proof of \ref{thm:short-cycles-variable}.
We use the switching in \ref{def:cycle-switching}. Assign all edges
in $\fG$ a weight of 1, and let $\X=\X_{\ell,n}^{\left(d\right)}$.
Let $F_{C}\left(G\right)$ be the number of ways to switch out of
a graph $G$ by destroying an $\ell$-cycle $C\subseteq K_{n}$. Now,
there are $\falling n{\ell}/\left(2\ell\right)$ cycles in the complete
graph $K_{n}$ on $n$ vertices, so 
\begin{alignat*}{1}
\E\left|\X_{1}-c\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\right| & \le\E\left|\sum_{C\in K_{n}}\one_{C\subseteq G_{1}}-\frac{c}{\Delta}\sum_{C\in K_{n}}F_{C}\left(G_{1}\right)\right|\\
 & \le\frac{\falling n{\ell}}{2\ell}\E\left|\one_{C\subseteq G_{1}}-\frac{c}{\Delta}F_{C}\left(G_{1}\right)\right|.
\end{alignat*}
Similarly, let $B_{C}\left(G\right)$ be the number of ways to switch
into $G$ by destroying $C$. We have 
\[
\E\left|\frac{\left(d-1\right)^{\ell}}{2\ell}-c\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\right|\le\frac{\falling n{\ell}}{2\ell}\,\E\left|\frac{\left(d-1\right)^{\ell}}{\falling n{\ell}}-\frac{c}{\Delta}B_{C}\left(G_{1}\right)\right|.
\]
Now, if $C\subseteq G$, then 
\begin{equation}
F_{C}\left(G\right)\approx\falling n{\ell}\, d^{\ell}/2^{\ell}.\label{eq:FC-approximate}
\end{equation}
To see this, note that a forwards switching is determined by the $\ell$
vertices $\left\{ w_{\i}\right\} _{\i}$ and a neighbour $u_{\i}$
for each. For large $n$, there are about $\falling n{\ell}$ ways
to choose the $\left\{ w_{\i}\right\} _{\i}$, and each $w_{\i}$
has $d$ neighbors (for large $n$ these are unlikely to interfere
with each other). Because each $u_{\i}$ and $w_{\i}$ can be interchanged,
resulting in the same switching, we then need to divide by $2^{\ell}$.
Also, we have
\begin{equation}
B_{C}\left(G\right)\approx{d \choose 2}^{\ell},\label{eq:BC-approximate}
\end{equation}
because a switching that destroyed $C$ to create $G$ can be identified
with a path $u_{\i}v_{\i}w_{\i}$ for each $v_{\i}\in C$, and there
are about ${d \choose 2}^{\ell}$ ways to choose these paths. With
these estimates in mind, we choose $c=\Delta2^{\ell}/\left(\falling n{\ell}d^{\ell}\right)$,
so that both of 
\begin{equation}
\E\left|\one_{C\subseteq G_{1}}-\frac{c}{\Delta}F_{C}\left(G_{1}\right)\right|,\quad\E\left|\frac{\left(d-1\right)^{\ell}}{\falling n{\ell}}-\frac{c}{\Delta}B_{C}\left(G_{1}\right)\right|\label{eq:switchings-required-expectation-bound}
\end{equation}
are small. This will give us a bound on 
\[
\E\left|\X_{1}-c\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\right|+\E\left|\frac{\left(d-1\right)^{\ell}}{2\ell}-c\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\right|,
\]
as is required to prove that 
\[
d_{\TV}\left(\X,\Po\left(\frac{\left(d-1\right)^{\ell}}{\falling n{\ell}}\right)\right)\approx0.
\]


The arguments needed to actually obtain bounds on the expectations
in \ref{eq:switchings-required-expectation-bound} can be mostly adapted
from corresponding arguments in the bare-hands application of switchings
in \cite{MWW04}, which we will now sketch.

By the argument used to establish \ref{eq:FC-approximate}, we know
that a switching that destroys a cycle $C$ is determined by a choice
of a set of edges $\left\{ w_{\i}u_{\i}\right\} _{\i=1}^{\ell}$.
To estimate $\E\left[F_{C}\left(G\right)\right]$, we need a lower
bound (in expectation) of the number of such choices that are ``legal''
as per \ref{def:cycle-switching} (that is, the relevant vertices
are distinct and no extraoneous cycles are created or destroyed).
A choice will certainly be legal if we require that each of the edges
$w_{\i}u_{\i}$ are a certain minimum distance (say $\ell$) away
from each other and of the cycle $C$, and we also require that none
of the edges $w_{\i}u_{\i}$ are part of a ``short cycle'' (a cycle
with length less than or equal to $\ell$). The number of vertices
with distance less than or equal to $\ell$ from a particular vertex
is at most $d^{\ell}$, which is a vanishingly small proportion of
the vertices of a large graph. Due to this and the fact that short
cycles are ``rare'' in expectation (\ref{rem:short-cycles-are-rare}),
it can be shown combinatorially that
\[
\E\left|\one_{C\subseteq G_{1}}-\frac{c}{\Delta}F_{C}\left(G_{1}\right)\right|=O\left(\frac{\ell\left(d-1\right)^{2\ell-1}}{n^{\ell+1}}\right).
\]
We similarly need to investigate the sharpness of \ref{eq:BC-approximate}
by bounding the number of ``legal'' choices of sets of paths $\left\{ u_{\i}v_{\i}w_{\i}\right\} _{\i=1}^{\ell}$.
For this, we restrict our attention to such choices where no edge
$v_{\i}u_{\i}$ or $v_{\i}w_{\i}$ is contained in a short cycle,
and the distance between $u_{\i}v_{\i}w_{\i}$ and $u_{\ii}v_{\ii}w_{\ii}$
is exactly the distance between $v_{\i}$ and $v_{\ii}$. We can then
obtain a similar bound
\[
\E\left|\frac{\left(d-1\right)^{\ell}}{\falling n{\ell}}-\frac{c}{\Delta}B_{C}\left(G_{1}\right)\right|=O\left(\frac{\ell\left(d-1\right)^{2\ell-1}}{n^{\ell+1}}\right).
\]
\ref{thm:short-cycles-variable} can be shown to follow.


\section{Further Reading}

There are many aspects of Stein's method that we were unable to cover
in this thesis. We have discussed approximation only with the Poisson
and normal distributions, but Stein's method has been applied in much
more generality. The method has been applied for other ``special''
distributions such as the exponential, geometric and semicircle distributions
(\cite{Ros11,FL14}). More interestingly, Stein's method has been
used to approximate with application-specific distributions (see,
for example, \cite{BCX07}). This is done using general characterizing
operator constructions such as the exchangeable pairs approach (\ref{thm:characterizing-from-exchangeable-pair}),
and the ``generator approach'' (\cite{BC05}).

Also, while we were able to discuss the method of size-bias coupling
(\ref{sec:size-bias-coupling}), there are several other distributional
transformations that can be used to control the expectations produced
by the Stein transformation. The most notable of these is zero-biasing
(see, for example, \cite[Section~2.3.3]{CGS10}).

We have also only discussed Stein's method as a tool for bounding
the distance between distributions, but the method can be applied
to prove other types of results, including, for example, concentration
inequalities (see, for example, \cite[Section~7]{Ros11}). The method
has even been used for totally non-probabilistic results, such as
the asymptotic enumeration of Latin rectangles (\cite[Section~XI]{Ste86}).

Finally, it is worth remarking that Stein's method is currently an
active area of research, particularly in the fields of stochastic
processes and random matrix theory. In fact, several interesting papers
appeared on the arXiv too recently for me to be able to discuss them
in the body of this thesis. The reader may be interested in a very
recent survey paper \cite{Cha14} discussing the history of Stein's
method and some current open problems.

\let\textbf\oldtextbf

\bibliographystyle{amsalpha.mod}
\bibliography{readings}

\end{document}
