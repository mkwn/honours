%% LyX 2.0.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[11pt,english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm,headheight=3cm,headsep=3cm,footskip=1.5cm}
\setlength{\parskip}{\bigskipamount}
\setlength{\parindent}{0pt}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{esint}
\setstretch{1.5}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}[section]
  \theoremstyle{plain}
  \newtheorem{cor}[thm]{\protect\corollaryname}
  \theoremstyle{plain}
  \newtheorem{lem}[thm]{\protect\lemmaname}
  \theoremstyle{plain}
  \newtheorem{prop}[thm]{\protect\propositionname}
  \theoremstyle{plain}
  \newtheorem{conjecture}[thm]{\protect\conjecturename}
  \theoremstyle{definition}
  \newtheorem{defn}[thm]{\protect\definitionname}
  \theoremstyle{definition}
  \newtheorem{example}[thm]{\protect\examplename}
  \theoremstyle{remark}
  \newtheorem{rem}[thm]{\protect\remarkname}
  \theoremstyle{remark}
  \newtheorem{claim}[thm]{\protect\claimname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% clickable links
\usepackage[bookmarks,hidelinks]{hyperref} 

% list hackery
\usepackage{enumitem}
\setlist[enumerate]{label=\textup{(\roman*)},topsep=-5pt}
\setlist[itemize]{topsep=-5pt}

% cleveref allows \ref{thm:asdf} instead of Theorem~\ref{thm:asdf}
\usepackage[nameinlink,capitalise]{cleveref}
\AtBeginDocument{\renewcommand{\ref}[1]{\cref{#1}}}

% LyX won't let me include cleveref before theorem declarations so I need to redefine everything as a hack
\theoremstyle{plain}
\newtheorem{mythm}{\protect\theoremname}[section]
\renewenvironment{thm}{\begin{mythm}}{\end{mythm}}
\theoremstyle{definition}
\newtheorem{mydefn}[mythm]{\protect\definitionname}
\renewenvironment{defn}{\begin{mydefn}}{\end{mydefn}}
\theoremstyle{definition}
\newtheorem{myexample}[mythm]{\protect\examplename}
\renewenvironment{example}{\begin{myexample}}{\end{myexample}}
\theoremstyle{plain}
\newtheorem{myprop}[mythm]{\protect\propositionname}
\renewenvironment{prop}{\begin{myprop}}{\end{myprop}}
\theoremstyle{plain}
\newtheorem{mycor}[mythm]{\protect\corollaryname}
\renewenvironment{cor}{\begin{mycor}}{\end{mycor}}
\theoremstyle{plain}
\newtheorem{mylem}[mythm]{\protect\lemmaname}
\renewenvironment{lem}{\begin{mylem}}{\end{mylem}}
\theoremstyle{plain}
\newtheorem{myconjecture}[mythm]{\protect\conjecturename}
\renewenvironment{conjecture}{\begin{myconjecture}}{\end{myconjecture}}
\theoremstyle{remark}
\newtheorem{myrem}[mythm]{\protect\remarkname}
\renewenvironment{rem}{\begin{myrem}}{\end{myrem}}
\theoremstyle{remark}
\newtheorem{myclaim}[mythm]{\protect\claimname}
\renewenvironment{claim}{\begin{myclaim}}{\end{myclaim}}

% equation cref format
\crefformat{equation}{#2(#1)#3}

% make cref play nicely with different kinds of lists
\newlist{conditions}{enumerate}{3}
\setlist[conditions]{label=\textup{(\roman*)},topsep=-5pt}
\if@cref@capitalise
  \crefname{conditions}{Condition}{Conditions}
\else
  \crefname{conditions}{Condition}{Conditions}
\fi
\crefalias{conditionsi}{conditions}
\crefalias{conditionsii}{conditions}
\crefalias{conditionsiii}{conditions}

% ordered lists should use parens instead of a point
%\renewcommand\theenumi{\arabic{enumi}}
%\renewcommand\labelenumi{(\theenumi)}

% \left(\right) should behave the same as ()
\let\originalleft\left
\let\originalright\right
\renewcommand{\left}{\mathopen{}\mathclose\bgroup\originalleft}
\renewcommand{\right}{\aftergroup\egroup\originalright}

% table of contents spacing tweaks
\usepackage{tocloft}
\setlength\cftparskip{-2pt}
\setlength\cftbeforesecskip{1pt}
\setlength\cftaftertoctitleskip{2pt}

% comment environment
\usepackage{verbatim}

% mainly for light colours color!percent
\usepackage{xcolor}

% shaded WIP notes
\theoremstyle{definition}
\newtheorem{commentthm}{Comment}[section]
\newtheorem{todothm}[commentthm]{Issue}
\usepackage{framed}
\usepackage{lipsum}
\newenvironment{note}
{\colorlet{shadecolor}{blue!5}\begin{shaded}\begin{commentthm}}{\end{commentthm}\end{shaded}}
\newenvironment{todo}
{\colorlet{shadecolor}{red!5}\begin{shaded}\begin{todothm}}{\end{todothm}\end{shaded}}

\makeatother

\usepackage{babel}
  \providecommand{\claimname}{Claim}
  \providecommand{\conjecturename}{Conjecture}
  \providecommand{\corollaryname}{Corollary}
  \providecommand{\definitionname}{Definition}
  \providecommand{\examplename}{Example}
  \providecommand{\lemmaname}{Lemma}
  \providecommand{\propositionname}{Proposition}
  \providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

\begin{document}

\title{Stein's Method}


\author{Matthew Kwan}

\maketitle
\begin{comment}
\begin{thm}
t\end{thm}
\begin{cor}
c\end{cor}
\begin{lem}
l\end{lem}
\begin{prop}
p\end{prop}
\begin{conjecture}
c\end{conjecture}
\begin{defn}
d\end{defn}
\begin{example}
e\end{example}
\begin{rem}
r\end{rem}
\begin{claim}
c
\end{claim}
\end{comment}

\global\long\def\floor#1{\left\lfloor #1\right\rfloor }


\global\long\def\ceil#1{\left\lceil #1\right\rceil }


\global\long\def\i{i}


\global\long\def\ii{j}


\global\long\def\NN{\mathbb{N}}


\global\long\def\ZZ{\mathbb{Z}}


\global\long\def\RR{\mathbb{R}}


\global\long\def\CC{\mathbb{C}}


\global\long\def\F{\mathcal{F}}


\global\long\def\range#1{\left[#1\right]}


\global\long\def\d{\operatorname d}


\global\long\def\id{\operatorname{id}}


\global\long\def\one{\operatorname{1}}


\begin{comment}\begin{note}Commentary in blue\end{note}\begin{todo}Issues
in red\end{todo}\end{comment}

\tableofcontents{}


\section{Introduction}

\begin{note}

The plan is to introduce with limit theorems: Central Limit theorem,
Poisson Limit theorem. The failure of limit theorems is that they
provide no understanding of speed of convergence, in particular convergence
cannot be assumed to be uniform as parameters vary.

Stein's method is a technique for bounding the distance between distributions,
with a variety of different distance metrics. Quantitative bounds
can be useful in their own right, or can be further applied to prove
asymptotic results.

\end{note}


\subsection{Notation}

For this thesis, the set of natural numbers $\NN$ includes zero.
We write $\one_{A}$ for the characteristic function of a set $A$:
$\one_{A}\left(x\right)=1$ if $x\in A$, otherwise $\one_{A}\left(x\right)=0$.
Also, $\range k$ denotes the set $\left\{ 1,\dots,k\right\} $.

Unless otherwise specified, all asympotics are as $n\to\infty$. Apart
from standard asymptotic notation, we use two notions of asymptotic
equivalence: $f\sim g$ means $f=g\left(1+o\left(1\right)\right)$
and $f\asymp g$ means $f=\Theta\left(g\right)$.

In this thesis, unless stated otherwise, graphs are labelled. That
is, they are distinguished even within isomorphism classes. A graph
may not have loops or multiple edges; an object which is allowed to
have loops and/or multiple edges will be called a multigraph.

The phrase ``randomly choose'' is taken to mean a uniformly random
choice. That means, each possible option is chosen with equal probability.


\part{Theory}


\section{General Probability Theory}

\global\long\def\L#1{\mathcal{L}_{#1}}


\DeclareRobustCommand{\L}[1]{\ifmmode{\mathcal{L}_{#1}}\else\polishL\fi}

\global\long\def\Lb#1{\mathcal{L}\left(#1\right)}


\global\long\def\Om{\Omega}


\global\long\def\om{\omega}


\global\long\def\cA{\mathcal{A}}


\global\long\def\B{\mathcal{B}}


\global\long\def\E{\mathbb{E}}


\global\long\def\F{F}


\global\long\def\N{\mathcal{N}}


\global\long\def\Po{\operatorname{Po}}


\global\long\def\Var{\operatorname{Var}}


\global\long\def\im{\operatorname{im}}


\global\long\def\sgn{\operatorname{sign}}


\global\long\def\supp{\operatorname{supp}}


\global\long\def\Pr{\mathbb{P}}


\global\long\def\m{\mu}


\global\long\def\X{X}


\global\long\def\bX{\mathbf{X}}


\global\long\def\A{A}


\global\long\def\P{\mathcal{P}}


\begin{comment}

For many combinatorial applications, an informal understanding of
probability theory will suffice, because probability spaces of combinatorial
objects are usually finite. However, we will need a more rigorous
foundation in probability theory. The following is only intended as
a brief review (cite textbook).


\subsection{Measure theory}

Let $\A$ be a collection of subsets of some set $\Om$. We say $\cA$
is a \emph{$\sigma$-algebra} if it is closed under countable unions
and complements, and contains the empty set. A \emph{measure} on $\cA$
is a map $\m:\cA\to\left[0,\infty\right)$ such that for any collection
of pairwise disjoint sets $\left\{ \A_{\i}\right\} _{\i\in\NN}\subseteq\cA$,
we have $\sum_{\i\in\NN}\m\left(A_{\i}\right)=\m\left(\bigcup_{i\in\NN}\A_{\i}\right)$.

A \emph{measure space} consists of a set $\Om$, a $\sigma$-algebra
$\cA$ of subsets of $\Om$ and a measure $\m$ on $\cA$. We represent
a measure space by the triple $\left(\Om,\cA,\m\right)$.
\begin{itemize}
\item define $\sigma$-algebra generated by a set
\item define Borel $\sigma$-algebra $\B$
\item if $\Om$ is countable we can choose $\cA=\Om$
\item define measurable functions $X:\Om_{1}\to\Om_{2}$
\item define integration
\end{itemize}
\end{comment}


\subsection{Review of basic concepts}

\begin{note}

I'm a little bit uncertain how much depth to go into for this. At
the moment, it's written so that someone who's seen measure theory
but no probability theory (an analyst) can understand. Where possible,
I've tried to translate things into the discrete case, because it's
often more intuitive (and since I plan for applications to be combinatorial).

\end{note}

For many combinatorial applications, an informal understanding of
probability theory will suffice. However, in this thesis a rigorous
foundation in probability theory will be useful. The following is
intended only as a brief review.
\begin{defn}
A \emph{probability space} is a measure space $\left(\Om,\cA,\Pr\right)$
with $\Pr\left(\Om\right)=1$. In this case we say $\Pr$ is a \emph{probability
measure}, and denote the set of all probability measures on $\left(\Om,\cA\right)$
by $\P\left(\Om,\cA\right)$ or $\P\left(\Om\right)$ if there is
no ambiguity. An \emph{event} is a measurable set $\A\in\cA$. When
we have only defined one measure on the set $\Om$, we will sometimes
abuse notation and write $\Pr$ to mean$\left(\Om,\cA,\Pr\right)$,
and vice versa.
\end{defn}
\begin{todo}

abuse?

\end{todo}

For our purposes $\Om$ will often be a finite set of combinatorial
objects, with $\cA$ as the power set of $\Om$. In this case $\Pr$
is defined by $\Pr\left(\om\right):=\Pr\left(\left\{ \om\right\} \right)$,
for each $\om\in\Om$. We will discuss specific probability spaces
on combinatorial objects in \ref{sec:random-structures}.

For an event $\A$, $\Pr\left(\A\right)$ is interpreted as the ``probability
that $\A$ occurs''. For combinatorial spaces, events are usually
of the form $\A=\left\{ \om\in\Om:\, P\left(\om\right)\mbox{ holds}\right\} $,
where $P\left(\om\right)$ is some property of an object $\om$. For
clarity, we often abuse notation slightly and write $\Pr\left(P\left(\om\right)\mbox{ holds}\right)$
instead of $\Pr\left(\A\right)$.
\begin{defn}
A \emph{random element} is a measurable function $\X:\left(\Om_{1},\cA_{1}\right)\to\left(\Om_{2},\cA_{2}\right)$.
If $\Om_{2}=\RR^{n}$ and $\cA_{2}$ is the Borel $\sigma$-algebra
on $\RR^{n}$, then we say $\X$ is a \emph{random vector}; if $n=1$
then $\X$ is a \emph{random variable}. If $\Om_{2}$ is countable
then we say $\X$ is \emph{discrete}.
\end{defn}
Note that if $\Om_{1}$ is countable, then any function is measurable.

If $\left(\Om_{1},\cA_{1}\right)$ is equipped with a probability
measure $\Pr$, we will use slightly abusive (but standard) notation
like $\Pr\left(\X\in\A\right)$ to denote $\Pr\left(\left\{ \om\in\Om_{1}:\X\left(\om\right)\in\A\right\} \right)$,
and interpret this as ``the probability that $\X$ falls in the set
$\A$''.

Note that such probabilities depend on an the underlying probability
space $\left(\Om_{1},\cA_{1},\Pr\right)$. We will often be interested
in the probability that a random element takes certain values, without
regard to any particular underlying probability space.
\begin{defn}
Suppose $\X$ is a random element which takes values in the measurable
space $\left(\Om,\cA\right)$. The \emph{distribution} (or \emph{law})
$\L{\X}$ of $\X$ with respect to an underlying probability $\Pr$
is the pushforward measure with respect to $\X$. That is, it is a
probability measure defined by $\L{\X}\left(\A\right)=\Pr\left(\X^{-1}\left(\A\right)\right)$
for $\A\subseteq\cA$. Usually, $\X$ will have been implicitly defined
with respect to a particular underlying probability measure, so we
will omit mention of $\Pr$ and just write $\L{\X}$. Also, we occasionally
use the notation $\Lb{\X}:=\L{\X}$ for ease of reading.
\end{defn}
It is worth noting that in fact any probability measure is the distribution
of some random element. To see this, note that given a probability
measure $\Pr\in\P\left(\Om\right)$, we can choose $\X=\id_{\Om}$
to have $\L{\X}=\Pr$ with respect to the underlying probability measure
$\Pr$. So, it is often convenient to define a probability distribution
before specifying a random variable with that distribution.
\begin{defn}
The notation $\X\in\L{}$ means $\L{\X}=\L{}$. We say $\X$ \emph{is
distributed as }$\L{}$.\end{defn}
\begin{example}
The normal distribution with parameters $\mu$ and $\sigma$ is denoted
$\N\left(\mu,\sigma\right)$ or $\N_{\mu,\sigma}$ and is defined
by $\N_{\mu,\sigma}\left(B\right)=\frac{1}{\sigma\sqrt{2\pi}}\int_{B}e^{-\frac{\left(x-\mu\right)^{2}}{2\sigma^{2}}}\d x$
for any Borel set $B$.
\end{example}
For a discrete random variable, we can naturally define the set of
values on which it is supported.
\begin{defn}
The \emph{support} of a discrete random element $\X$ is the set 
\[
\supp\left(\X\right)=\left\{ k\in\Om:\Pr\left(\X=k\right)>0\right\} .
\]

\end{defn}
Then, $\L{\X}$ is just an assignment of a probability to each value
in $\supp\left(\X\right)$.
\begin{example}
The Poisson distribution with parameter $\lambda$ is denoted $\Po\left(\lambda\right)=\Po_{\lambda}$;
this is defined by $\Po_{\lambda}\left(k\right)=\frac{\lambda^{k}e^{-\lambda}}{k!}$
for all $k\in\NN$.\end{example}
\begin{defn}
The \emph{expected value }of a random variable $\X$ is $\E\X=\int x\d\L{\X}\left(x\right)$.
\end{defn}
For a random variable $\X$ that takes integer values, this definition
is equivalent to the well-known formula $\E\X=\sum_{x\in\ZZ}x\,\Pr\left(\X=x\right)$.
\begin{rem}
\label{rem:indicator-expectation}If $\X$ is a random variable that
can be interpreted as counting the number of objects that satisfy
some property, then we can express $\X$ as a sum of indicator variables
$\sum_{\i}\one_{\A_{\i}}$, where $\A_{\i}$ is the event that the
$\i$th object satisfies our property. Noting that $\E$ is linear,
we have $\E\X=\sum_{\i}\E\one_{\A_{\i}}=\sum_{\i}\Pr\A_{\i}$. So,
in order to compute the expectation of $\X$ we just need to compute
the probability that each object satisfies our required property.
\end{rem}
If we fix a particular underlying probability space $\left(\Om,\cA,\Pr\right)$,
we can also equivalently view expectation as a linear functional on
the space of integrable functions: $\E f=\int f\left(\om\right)\d\Pr$.
Sometimes we will define a new probability space $\left(\Om,\cA,\Pr'\right)$
on an existing measurable space. In this case we will write $\E_{\Pr'}$
to indicate expecation with respect to the measure $\Pr'$, to avoid
ambiguity.

In fact, the expectation functional defines its underlying probability
measure, because $\E\one_{\A}=\Pr\left(\A\right)$ for all $\A$.
Since the distribution of a random variable is specified by a probability
measure, the distribution $\Lb{\X}$ of a random variable $\X$ also
uniquely defines an expectation functional $\E_{\X}:=\E_{\Lb{\X}}$.

\begin{comment}
\begin{defn}
The \emph{variance }of a random variable $\X$ is $\Var\X=\E\left(\X-\E X\right)^{2}$.
The \emph{$n$th moment} of a random variable $\X$ is $\E\left[\X^{n}\right]$.
\end{defn}
conditional variance?

\end{comment}
\begin{defn}
For two collections $S,S'\subseteq\cA_{1}$ of events, we say that
$S$ and $S'$ are \emph{independent} if $\Pr\left(\A\cap\A'\right)=\Pr\left(\A\right)\Pr\left(\A'\right)$
for each $\A\in S$ and $\A\in S'$.
\end{defn}
If $\left\{ \A\right\} $ is independent of $S'$, we also say that
$\A$ itself is independent of $S'$. If $\left\{ \om\in\Om_{1}:\X\left(\om\right)\in\A_{2}\right\} $
is independent of $S'$ for some random element $\X:\left(\Om_{1},\cA_{1}\right)\to\left(\Om_{2},\cA_{2}\right)$,
then we say $\X$ is independent of $S'$. We can analogously say
that two random variables are independent, or a random variable and
an event are independent, or any similar combination.
\begin{defn}
If two objects are not independent, then we say they are \emph{dependent}.
\end{defn}

\begin{defn}
Suppose $\X:\Om_{1}\to\Om_{2}$ is a random element defined on these
spaces, and $\A_{1}\in\cA_{1}$ is an event with nonzero probability.
Then the \emph{distribution of $\X$ conditioned on $\A_{1}$} is
denoted by $\L{\X|\A_{1}}$ and defined by $\L{\X|\A_{1}}\left(\A_{2}\right)=\Pr\left(\X\in\A_{2}|\A_{1}\right)$
for $\A_{2}\in\cA_{2}$. The expected value of a random variable with
distribution $\L{\X|\A_{1}}$ is called the \emph{conditional expected
value of $\X$ given $\A_{1}$} and is denoted $\E\left[\X|\A_{1}\right]$.
\end{defn}
We can also define conditional expectation with respect to another
random variable. If $\X_{1}$ and $\X_{2}$ are random variables defined
on the same underlying probability space $\left(\Om,\cA,\Pr\right)$,
then the sets $\X_{2}^{-1}\left(B\right)$ for Borel $B$ comprise
a sub-$\sigma$-algebra $\cA'$ of $\cA$. Then, $\m:\A'\mapsto\E\left[\X_{1}\one_{\A'}\right]$
is a signed measure on $\cA'$ that is absolutely continuous with
respect to the restriction of $\Pr$ to $\cA'$. By the Radon-Nikodym
theorem there is an $\cA'$-measurable random variable $\E\left[\X_{1}|\X_{2}\right]$
that satisfies $\E\left[\X_{1}\one_{\A'}\right]=\E\left[\E\left[\X_{1}|\X_{2}\right]\one_{\A'}\right]$
for all $\A'$ in $\cA'$. This random variable is almost uniquely
defined: for any two choices of $\E\left[\X_{1}|\X_{2}\right]$, the
probability that they differ is zero.
\begin{defn}
The random variable $\E\left[\X_{1}|\X_{2}\right]$ as defined above
is called the \emph{conditional expectation of $\X_{1}$ with respect
to $\X_{2}$}. We can also view conditional expectation as a linear
operator between functions: we define $\E^{\X_{2}}$ by $\X_{1}\mapsto\E\left[\X_{1}|\X_{2}\right]$.
\end{defn}
This definition generalizes the previous definition of expectation
conditioned on an event: if $\om\in\A$ and $\Pr\left(\A\right)>0$
then $\E\left[\X|\one_{\A}\right]\left(\om\right)=\E\left[\X|\A\right]$.

Note that if $\X_{2}$ is discrete then we do not need to invoke Radon-Nikodym.
We can define $\E\left[\X_{1}|\X_{2}\right]$ by $\E\left[\X_{1}|\X_{2}\right]\left(\om\right)=\E\left[\X_{1}|\X_{2}=\X_{2}\left(\om\right)\right]$
for all $\om\in\Om$ with $\Pr\left(\X_{2}=\X_{2}\left(\om\right)\right)>0$;
this defines $\E\left[\X_{1}|\X_{2}\right]$ up to a set of probability
zero.

We finally present a simple consequence of the definition of conditional
expectation.
\begin{prop}
[Tower Law of Expectation]\label{prop:tower-law}Suppose $\X_{1}$
and $\X_{2}$ are random variables defined on the same underlying
probability space $\left(\Om,\cA\right)$. Then $\E\left[\E^{\X_{2}}\X_{1}\right]=\E\left[\X_{1}\right]$.\end{prop}
\begin{proof}
$\E\left[\E^{\X_{2}}\X_{1}\right]=\E\left[\E\left[\X_{1}|\X_{2}\right]\one_{\Om}\right]=\E\left[\X_{1}\one_{\Om}\right]=\E\left[\X_{1}\right]$
\end{proof}

\subsection{Coupling}

Given a finite collection of measure spaces $\left(\Om_{1},\cA_{1},\m_{1}\right),\dots,\left(\Om_{n},\cA_{n},\m_{n}\right)$
recall the construction of the product measure space $\left(\Om,\cA,\m\right):=\left(\prod_{i=1}^{n}\Om_{i},\bigotimes_{i=1}^{n}\cA_{i},\prod_{i=1}^{n}\m_{i}\right)$.
If a random element takes values in a product space then each component
is measurable, and conversely if the components of a random tuple
are measurable then that tuple is measurable in the product space.
So, we can make the following definitions:
\begin{defn}
Given random elements $\X_{1},\dots,\X_{n}$ on the same underlying
probability space, $\Lb{\X_{1},\dots,\X_{n}}:=\Lb{\left(\X_{1},\dots,\X_{n}\right)}$
is called the \emph{joint distribution} of $\X_{1},\dots,\X_{n}$.
Conversely, given a random tuple $\left(\X_{1},\dots,\X_{n}\right)$,
each $\Lb{\X_{\i}}$ is called a \emph{marginal distribution}.
\end{defn}
Suppose we have two distributions of random elements $\Lb{\X_{1}}$
and $\Lb{\X_{2}}$. \emph{Coupling} is the technique of constructing
a random ordered pair $\left(\X_{1},\X_{2}\right)$ which realizes
the given distributions as marginal distributions. Usually this is
done by specifying the joint distribution $\Lb{\X_{1},\X_{2}}$.

The idea is that coupling creates a particular kind of dependence
between $\X_{1}$ and $\X_{2}$ that allows us to compare the two
distributions. Often, we are able to make conclusions about the distributions
$\Lb{\X_{\i}}$ which are independent of their specific realizations
as random elements in the coupling.


\subsection{Markov Chains}

\begin{note}

I'll need to define Markov Chains, stationary distributions, irreducibility
and time-reversibility.

Perhaps I should talk more generally about stochastic processes, because
applying exchangeable pairs to Stein's method is has connections with
Ornstein-Uhlenbeck processes and also Stein's method can be applied
to Poisson processes.

\end{note}


\subsection{The Weak Topology on Probability Measures}

\global\long\def\cH{\mathcal{H}}


\global\long\def\h{h}


\global\long\def\TV{\mathrm{TV}}


\global\long\def\K{\mathrm{K}}


\global\long\def\W{\mathrm{W}}


\begin{note}

The main purpose of this section is to motivate the metrics usually
used in Stein's method: they are all legitimate topological metrics
and are consistent with the topology of convergence in distribution.
In particular, if we can show $\d_{\cH}\left(\X_{n},\X\right)\to0$
we have shown that $\X_{n}\xrightarrow{d}\X$, as Toby does \cite{Joh11}.

\end{note}
\begin{defn}
Let $\left(\X_{n}\right)_{n\in\NN}$ be a sequence of random variables.
We say $\X_{n}$ \emph{converges in distribution} to a random variable
$\X$ if $\E f\left(\X_{n}\right)\to\E f\left(\X\right)$ for all
bounded continuous functions $f$. Alternatively, we say $\Lb{\X_{n}}$
converges \emph{weakly} to $\Lb{\X}$, or simply $\Lb{\X_{n}}\to\Lb{\X}$.
The topology on $\P\left(\RR\right)$ associated with this convergence
is called the \emph{weak topology} (we will see that it is indeed
a topology). Convergence in distribution of random vectors is defined
component-wise.
\end{defn}

\begin{defn}
The \emph{distribution function} $\F_{\X}$ of a random variable $\X$
is defined by $\F_{\X}\left(x\right)=\Pr\left(\X\le x\right)$.\end{defn}
\begin{thm}
\label{prop:dist}The following are equivalent.

\begin{conditions}

\item \label{prop:dist-def}$\Lb{\X_{n}}\to\Lb{\X}$

\item \label{prop:dist-F}$\F_{\X_{n}}\left(x\right)\to\F_{\X}\left(x\right)$
for all $x$ where $\F_{\X}$ is continuous

\item (L\'evy's continuity theorem) $\E e^{it\X_{n}}\to\E e^{it\X}$
for all $t\in\RR$.

\end{conditions}
\end{thm}
The equivalence of \ref{prop:dist-def,prop:dist-F} is a well-known
result called the Portmanteau Theorem.

When $\X$ and each $\X_{n}$ are integer random variables, then \ref{prop:dist-F}
reduces to the condition that $\Pr\left(\X_{n}=k\right)\to\Pr\left(\X=k\right)$
for all $k$. This characterization is usually used to prove the Poisson
limit theorem.

Classically, distributional convergence results are often proved by
L\'evy's continuity theorem. For example, this approach is usually
used to prove the central limit theorem. For combinatorial applications,
convergence in distribution can also be proved by the ``method of
moments'': if $\X$ is the only random variable with the moments
$\left(\E\X^{k}\right)_{k\in\NN}$, then $\Lb{\X_{n}}\to\Lb{\X}$
if $\E\X_{n}^{k}\to\E\X^{k}$. Convergence in distribution can also
sometimes be inferred from stronger forms of convergence when $\X$
and all the $\X_{n}$ are coupled to the same underlying space.

A disadvantage of all these approaches is that they provide little
information about the rate of convergence.

In functional analysis terms, note that expectation operators are
bounded linear functionals on the space of real bounded continuous
functions. Then, $\Lb{\X_{n}}\to\Lb{\X}$ just means that $\E_{\X_{n}}\to\E_{\X}$
in the weak-star topology. Although $C_{b}\left(\RR\right)^{*}$ is
not metrizable, the subspace corresponding to $\P\left(\RR\right)$
is in fact metrizable, with a metric called the L\'evy metric. For
Stein's method we will be interested in some slightly stronger metrics,
which we will now define.
\begin{defn}
\label{def:general-metrics}Let $\cH$ be a collection of real measurable
``test'' functions. Define $d_{\cH}:\P\left(\RR\right)^{2}\to\RR^{+}$
by $d_{\cH}\left(\Pr_{1},\Pr_{2}\right)=\sup_{\h\in\cH}\left|\E_{\Pr_{1}}\h-\E_{\Pr_{2}}\h\right|$.
For random variables $\X_{1},\X_{2}$, we write $d_{\cH}\left(\X_{1},\X_{2}\right)$
instead of $d_{\cH}\left(\Lb{\X_{1}},\Lb{\X_{2}}\right)$.
\end{defn}
\begin{todo}

What if there is no supremum or $\E_{\Pr_{1}}\h=\infty$?

\end{todo}
\begin{defn}
A set of real functions $\cH$ is a \emph{determining class} if $\E_{\Pr_{1}}\h=\E_{\Pr_{2}}\h$
for all $\h\in\cH$ implies that $\Pr_{1}=\Pr_{2}$.
\end{defn}
Each $d_{\cH}$ is non-negative, symmetric and satisfies the triangle
inequality. Hence, to check that $d_{\cH}$ is a metric, we only need
to check that $d_{\cH}\left(\Pr_{1},\Pr_{2}\right)=0$ implies that
$\Pr_{1}=\Pr_{2}$. That is, we need to check that $\cH$ is a determining
class.
\begin{defn}
\label{def:special-metrics}We define some special cases of $d_{\cH}$.\begin{itemize}

\item If $\cH_{\K}=\left\{ \one_{\left(-\infty,x\right]}:x\in\RR\right\} $
then $d_{\K}:=d_{\cH_{\K}}$ is called the \emph{Kolmogorov metric}.

\item If $\cH_{\W}$ is the set of real functions $\h$ that satisfy
$\left|\h\left(x_{1}\right)-\h\left(x_{2}\right)\right|\le\left|x_{1}-x_{2}\right|$
for all $x_{1},x_{2}\in\RR$ (that is, the set of functions with Lipschitz
constant 1), then $d_{\W}:=d_{\cH_{\W}}$ is called the \emph{Wasserstein
metric}.

\item If $\cH_{\TV}$ is the set of functions $\one_{B}$ for Borel
$B$, then $d_{\TV}:=d_{\cH_{\TV}}$ is called the \emph{total variation
metric}.\end{itemize}\end{defn}
\begin{prop}
\label{prop:metric}The Kolmogorov, Wasserstein and total variation
``metrics'' are actually metrics.\end{prop}
\begin{proof}
We check that $\cH_{\K}$, $\cH_{\W}$ and $\cH_{\TV}$ are determining
classes. Let $\cH\in\left\{ \cH_{\K},\cH_{\W},\cH_{\TV}\right\} $,
and suppose that $\E_{\Pr_{1}}\h=\E_{\Pr_{2}}\h$ for all $\h\in\cH$.
It suffices to prove that $\Pr_{1}\left(\left(-\infty,x\right]\right)=\Pr_{2}\left(\left(-\infty,x\right]\right)$
for all $x\in\RR$, since the sets $\left(-\infty,x\right]$ generate
the Borel $\sigma$-algebra. For $\cH\in\left\{ \cH_{\K},\cH_{\TV}\right\} $
this is immediate, because $\Pr_{\i}\left(\left(-\infty,x\right]\right)=\E_{\Pr_{\i}}\one_{\left(-\infty,x\right]}$.
So, consider, $\cH=\cH_{\W}$.

For $\varepsilon>0$ and $x\in\RR$, let $\h_{x,\varepsilon}$ be
the continuous function which takes the value 1 on the set $\left(-\infty,x\right]$,
takes the value 0 on the set $\left[x+\varepsilon,\infty\right)$,
and is linearly interpolated in the range $\left[x,x+\varepsilon\right]$.
Since $\varepsilon\h_{x,\varepsilon}\in\cH_{\W}$, we have $\E_{\Pr_{1}}\h_{x,\varepsilon}=\E_{\Pr_{2}}\h_{x,\varepsilon}$
for each $\varepsilon>0$. For each $x\in\RR$, $\h_{x,1/n}\to\one_{\left(-\infty,x\right]}$
pointwise and each $\h_{x,1/n}\le1$ so by the dominated convergence
theorem, $\E_{\Pr_{\i}}\h_{1/n}\to\E_{\Pr_{\i}}\one_{\left(-\infty,x\right]}$
for each $\i\in\left\{ 1,2\right\} $. We have proved that $\Pr_{1}\left(\left(-\infty,x\right]\right)=\Pr_{2}\left(\left(-\infty,x\right]\right)$
for all $x\in\RR$.
\end{proof}

\begin{prop}
\label{prop:stronger-than-weak}The topologies induced by the Kolmogorov,
Wasserstein and total variation metrics are each stronger than the
weak topology.\end{prop}
\begin{proof}
If $d_{\K}\left(\X_{n},\X\right)\to0$ or $d_{\TV}\left(\X_{n},\X\right)\to0$
then $\F_{\X_{n}}\to\F_{\X}$ uniformly, so certainly \ref{prop:dist-F}
of \ref{prop:dist} holds.

Now, suppose $d_{\W}\left(\X_{n},\X\right)\to0$. Let $d_{n}=\sqrt{d_{\W}\left(\X_{n},\X\right)}$
and recall the definition of $\h_{x,\varepsilon}$ from the proof
of \ref{prop:metric}. Since $d_{n}\h_{x,d_{n}}\in\cH_{\W}$ for each
$n\in\NN$, we have 
\[
\E_{\X_{n}}\h_{x,d_{n}}-\E_{\X}\h_{x,d_{n}}\le d_{\W}\left(\X_{n},\X\right)/d_{n}=d_{n}\to0
\]
 uniformly for $x\in\RR$. Now, note that 
\[
\F_{\X}\left(x-\varepsilon\right)\le\E_{\X}h_{x-\varepsilon,\varepsilon}\le\F_{\X}\left(x\right)\le\E_{\X}h_{x,\varepsilon}\le\F_{\X}\left(x+\varepsilon\right)
\]
 for any random variable $\X$. If $\F_{\X}$ is continuous at $x$
then 
\begin{align*}
\F_{\X_{n}}\left(x\right)-\F_{\X}\left(x\right) & \le\left(\E_{\X_{n}}\h_{x,d_{n}}-\E_{\X}\h_{x,d_{n}}\right)+\left(\F_{\X}\left(x+d_{n}\right)-\F_{\X}\left(x\right)\right)\to0\\
\F_{\X_{n}}\left(x\right)-\F_{\X}\left(x\right) & \ge\left(\E_{\X_{n}}\h_{x-d_{n},d_{n}}-\E_{\X}\h_{x-d_{n},d_{n}}\right)+\left(\F_{\X}\left(x-d_{n}\right)-\F_{\X}\left(x\right)\right)\to0
\end{align*}
so \ref{prop:dist-F} of \ref{prop:dist} holds.
\end{proof}
\ref{prop:stronger-than-weak} tells us that we can sensibly use our
metrics to quantify the distance between random variables, in a way
that is consistent with distributional (weak) convergence. All three
metrics are relevant in their own right, but sometimes one may be
easier to work with. It is sometimes possible to transfer results
between metrics, though this usually results in worse constants than
working directly in the desired metric. We now prove some transfer
results.

\begin{todo}It may be worthwhile to actually characterize the Wasserstein,
Kolmogorov and Total Variation topologies. In particular, Wikipedia
says that Wasserstein convergence is just weak convergence plus convergence
of the first moment.\end{todo}
\begin{defn}
If $\F_{\X}\left(x\right)=\int_{-\infty}^{x}f_{\X}\left(x\right)\d x$
for some $f_{\X}$, then $f_{\X}$ is called the \emph{Lebesgue density}
of $\X$, and $\X$ is called a \emph{continuous} random variable.
\end{defn}
If $\X$ is a continuous random variable, then by the Radon-Nikodym
chain rule $\E_{\X}\h=\int_{\RR}\h\left(x\right)f_{\X}\left(x\right)\d x$.
\begin{prop}
Let $\X_{1},\X_{2}$ be random variables.\begin{enumerate}

\item \label{prop:transfer-K/TV}$d_{\K}\left(\X_{1},\X_{2}\right)\le d_{\TV}\left(\X_{1},\X_{2}\right)$

\item \label{prop:transfer-K/W}If $\left|f_{\X_{2}}\left(x\right)\right|\le C$
for all $x$, then $d_{\K}\left(\X_{1},\X_{2}\right)\le\sqrt{2Cd_{\W}\left(\X_{1},\X_{2}\right)}$.

\end{enumerate}\end{prop}
\begin{proof}
(Adapted from \cite[Proposition 1.2]{Ros11}). \ref{prop:transfer-K/TV}
is immediate from the definition. Then, as in the proof of \ref{prop:stronger-than-weak},\vspace{-10pt}
\begin{eqnarray*}
\F_{\X_{n}}\left(x\right)-\F_{\X}\left(x\right) & \le & \left(\E_{\X_{n}}\h_{x,\varepsilon}-\E_{\X}\h_{x,\varepsilon}\right)+\left(\E_{\X}\h_{x,\varepsilon}-\F_{\X}\left(x\right)\right)\\
 & \le & d_{\W}\left(\X_{1},\X_{2}\right)/\varepsilon+\int_{x}^{x+\varepsilon}\h_{x,\varepsilon}\, f_{\X}\left(x\right)\d x\\
 & \le & d_{\W}\left(\X_{1},\X_{2}\right)/\varepsilon+C\varepsilon/2
\end{eqnarray*}
and similarly\vspace{-10pt}
\begin{eqnarray*}
\F_{\X_{n}}\left(x\right)-\F_{\X}\left(x\right) & \ge & -d_{\W}\left(\X_{1},\X_{2}\right)/\varepsilon-C\varepsilon/2,
\end{eqnarray*}
So, we can take $\varepsilon=\sqrt{2d_{\W}\left(\X_{1},\X_{2}\right)/C}$
to prove \ref{prop:transfer-K/W}.\end{proof}
\begin{example}
If $\L{\X_{2}}=\N\left(0,1\right)$ then $f_{\X_{2}}\left(x\right)=\left(2\pi\right)^{-1/2}e^{-x^{2}/2}$
so we can take $C=\left(2\pi\right)^{-1/2}$ to obtain $d_{\K}\le\left(2/\pi\right)^{1/4}\sqrt{d_{\W}\left(\X_{1},\X_{2}\right)}$.
\end{example}
In a combinatorial setting, many of our results are about integer
random variables. The total variation metric is usually exclusively
used in this case.
\begin{prop}
If $\X_{1},\X_{2}$ are integer-valued random variables, then
\[
d_{\TV}\left(\X_{1},\X_{2}\right)=\frac{1}{2}\sum_{k\in\ZZ}\left|\Pr\left(\X_{1}=k\right)-\Pr\left(\X_{2}=k\right)\right|.
\]
\end{prop}
\begin{proof}
For any Borel set $\A$, let $d_{\A}=\Pr\left(\X_{1}\in\A\right)-\Pr\left(\X_{2}\in\A\right)$,
so that $d_{\TV}\left(\X_{1},\X_{2}\right)=\sup\left|d_{\A}\right|$.
Define 
\begin{align*}
\A_{<} & =\left\{ k\in\ZZ:\,\Pr\left(\X_{1}=k\right)<\Pr\left(\X_{2}=k\right)\right\} ,\\
\A_{>} & =\left\{ k\in\ZZ:\,\Pr\left(\X_{1}=k\right)>\Pr\left(\X_{2}=k\right)\right\} .
\end{align*}
For any Borel $\A$, we have 
\begin{align*}
d_{\A} & =\sum_{k\in\ZZ\cap\A}\left(\Pr\left(\X_{1}=k\right)-\Pr\left(\X_{2}=k\right)\right)\\
 & \le\sum_{k\in\A_{>}}\left(\Pr\left(\X_{1}=k\right)-\Pr\left(\X_{2}=k\right)\right)\\
 & =d_{\A_{>}}
\end{align*}
and similarly $\Pr\left(\X_{1}\in\A\right)-\Pr\left(\X_{2}\in\A\right)\ge d_{\A_{<}}$.
Since $d_{\A_{>}}=-d_{\A_{<}}$, we have 
\[
d_{\TV}\left(\X_{1},\X_{2}\right)=\left(d_{\A_{>}}-d_{\A_{<}}\right)/2=\frac{1}{2}\sum_{k\in\ZZ}\left|\Pr\left(\X_{1}=k\right)-\Pr\left(\X_{2}=k\right)\right|.
\]

\end{proof}

\section{Random Combinatorial Structures\label{sec:random-structures}}

\global\long\def\rG#1#2{\mathcal{G}_{#1,#2}}


\global\long\def\rS#1{\mathcal{S}_{#1}}

\begin{defn}
Given a finite space of combinatorial objects $\Om$, a probability
space $\left(\Om,2^{\Om},\Pr\right)$ is often called a \emph{model}
of $\Om$.
\end{defn}

\begin{defn}
In a probability space $\left(\Om,2^{\Om},\Pr\right)$ where $\Om$
is finite, if $\Pr\left(\om\right)=1/\left|\Om\right|$ for each $\om\in\Om$,
then we say the space is \emph{uniform}.
\end{defn}
Uniform models are the simplest examples of random structures. For
example, the uniform space $\rS n$ of permutations on $n$ elements
has $\Pr\left(\sigma\right)=1/n!$ for each $\sigma\in S_{n}$. The
uniform random graph model $\rG nM$ has $\Pr\left(G\right)={{n \choose 2} \choose M}^{-1}$
for each graph $G$ on the vertex set $\range n$ which has $M$ edges.
The uniform random regular graph model $\rG nd$ is uniform on the
set of all $d$-regular graphs on the vertex set $\range n$, though
an explicit formula for the number of such graphs is not known.

As an important example of a (generally) non-uniform model, the (Erd\H{o}s-R\'enyi)
binomial random graph model $\rG np$ has
\[
\Pr\left(G\right)=p^{\left|E\left(G\right)\right|}\left(1-p\right)^{{n \choose 2}-\left|E\left(G\right)\right|}
\]
for each graph $G$ on the vertex set $\range n$. When $p=1/2$,
we obtain the uniform model on all graphs on the vertex set $\range n$.

One way to conceptualize the binomial model is to consider a sequence
of independent coin tosses, where the coin is biased to land heads
with probability $p$. Each coin toss corresponds to a particular
potential edge, and determines whether that edge is present in the
final random graph. When we define more complicated random models,
we will often use this kind of informal description rather than giving
an explicit formula for each $\Pr\left(\om\right)$.

As another example, the uniform model $\rG nM$ can be alternatively
defined recursively: $\rG n0$ is always the trivial graph with no
edges, and for each $M>0$, to obtain $\rG nM$ we choose $G\in\rG n{M-1}$
and add one of the ${n \choose 2}-\left(M-1\right)$ possible edges
at random.

\begin{note}

This section is unfinished, I'll probably want random matrices and
maybe the pairing model on random regular graphs

\end{note}


\section{Stein's Method in the Abstract}

\begin{note}There are a few quite different presentations of Stein's
method. One thing I'm trying to do here is to unify Stein's functional
analysis approach for exchangeable pairs \cite{Ste86} with Ross'
general presentation\cite{Ros11}.

The reason I want to look at Stein's original, more abstract presentation
is that I think it does a better job motivating why things work. Before
I read that, the steps taken to apply Stein's method seemed like blindly
doing things and it turns out they work.\end{note}

\global\long\def\Lo{\L 0}


\global\long\def\Eo{\E_{0}}


\global\long\def\cF{\mathcal{F}}


\global\long\def\Fo{\cF_{0}}


\global\long\def\cY{\mathcal{Y}}


\global\long\def\fo{f}


\global\long\def\T{T}


\global\long\def\To{\T_{0}}


\global\long\def\U{U}


\global\long\def\Uo{\U_{0}}


\global\long\def\cX{\mathcal{X}}


\global\long\def\Xo{\cX_{0}}


Suppose we have a potentially complicated random variable $\X$, and
we believe the distribution of $\X$ is close to a ``standard''
distribution $\Lo$. Then, Stein's method allows us to compare the
operators $\E_{\X}$ and $\Eo:=\E_{\Lo}$. This is sometimes directly
useful for approximating statistics of $\X$ (for example, $\Pr\left(\X\in\A\right)=\E_{\X}\one_{\A}$).
However, particularly for combinatorical applications, Stein's method
is most often used to bound the distance $d_{\cH}\left(\L{\X},\Lo\right)$,
where the metric $d_{\cH}$ from \ref{def:general-metrics} is defined
in terms of $\E_{\X}$ and $\Eo$.

Stein's method is motivated by the idea of a characterizing operator.
\begin{defn}
Let $\Fo$ be a vector space and $\Xo$ be a vector space of measurable
functions. We say a linear operator $\To:\Fo\to\Xo$ is a \emph{characterizing
operator} for the distribution $\Lo$ if $\im\To=\Xo\cap\ker\Eo$.
For convenience, where there is no ambiguity we will often implicitly
restrict $\Eo$ to $\Xo$, so we can write $\im\To=\ker\Eo$.
\end{defn}
The following proposition shows why $\To$ is called a characterizing
operator.
\begin{prop}
\label{prop:characterizing}If $\To:\Fo\to\Xo$ is a characterizing
operator and $\Xo$ is a determining class then $\im\To\subseteq\ker\E_{\X}$
implies $\L{\X}=\Lo$.\end{prop}
\begin{proof}
If $\h\in\Xo$, then $\h-\Eo\h\in\ker\Eo=\im\To$ so $\E_{\X}\left[\h-\Eo\h\right]=0$.
That is, $\E_{\X}\h=\Eo\h$ for all $\h\in\Xo$, which means $\L{\X}=\Lo$
by the definition of a determining class.
\end{proof}
\begin{todo}\label{todo:characterizing}Ross \cite{Ros11} and others
use this weaker condition as the definition of a characterizing operator.
I'll have to look at examples of operators that satisfy the weaker
but not the stronger condition to see if the stronger definition is
warranted (my guess is yes, if Stein decided to originally define
it the way I did).\end{todo}
\begin{prop}
\label{prop:U_0-characterizing}$\To:\Fo\to\Xo$ is characterizing
if and only if there is a linear operator $\Uo:\Xo\to\Fo$ such that
the following two equations hold.
\begin{align}
\Eo\To & =0_{\Fo},\label{eq:E_0T_0-consistency}\\
\To\Uo+\Eo & =\id_{\Xo}.\label{eq:U_0-characterizing}
\end{align}
\end{prop}
\begin{proof}
Suppose $\To$ is a characterizing operator. Equation \ref{eq:E_0T_0-consistency}
is immediate. Let $\left\{ \h_{\i}\right\} _{\i\in\mathcal{I}}$ be
a (Hamel) basis of $\Xo$. For each $\i\in\mathcal{I}$ we have $\h_{\i}-\Eo\h_{\i}\in\ker\Eo$
so there is some $\fo_{\i}$ (not necessarily unique) that solves
$\To\fo_{\i}=\h_{\i}-\Eo\h_{\i}$. The operator $\Uo$ can then be
defined by $\sum_{\i\in\mathcal{I}}a_{\i}\h_{\i}\mapsto\sum_{\i\in\mathcal{I}}a_{\i}\fo_{\i}$,
satisfying \ref{eq:U_0-characterizing}.

\begin{todo}there's probably a cleaner functional analysis way to
prove that. Also, is $\Uo$  bounded?\end{todo}

Conversely, suppose \ref{eq:E_0T_0-consistency} holds and $\Uo$
exists satisfying \ref{eq:U_0-characterizing}. For $\h\in\ker\Eo$
we have $\To\left(\Uo\h\right)=\h$ and hence $\h\in\im\To$, so $\ker\Eo\subseteq\im\To$.
Equation \ref{eq:E_0T_0-consistency} immediately says that $\im\To\subseteq\ker\Eo$,
so $\To$ is a characterizing operator.
\end{proof}
We'll use \ref{prop:U_0-characterizing} to give two important examples
of characterizing operators.
\begin{thm}
\label{thm:normal-characterizing}Define $T_{\N}$ by $T_{\N}\fo\left(x\right)=\fo'\left(x\right)-x\fo\left(x\right)$.
Let $\cX_{\N}=L^{1}\left(\RR,\N\left(0,1\right)\right)$ be the set
of functions $\h:\RR\to\RR$ that satisfy $\E_{\N}\left|\h\right|<\infty$
and let $\cF_{\N}=\T_{\N}^{-1}\cX_{\N}$ be the set of functions $\fo:\RR\to\RR$
such that $\E_{\N}\left|\T_{\N}\fo\right|<\infty$. Then $\T_{\N}:\cF_{\N}\to\cX_{\N}$
is a characterizing operator for $\N\left(0,1\right)$.\end{thm}
\begin{proof}
For any $f\in\cF_{\N}$, integration by parts gives
\[
\E_{\N}\T_{\N}\fo=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-t^{2}/2}\fo'\left(t\right)\d t-\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}te^{-t^{2}/2}\fo\left(t\right)\d t=0
\]
so $\E_{\N}\T_{\N}=0$ and \ref{eq:E_0T_0-consistency} holds. Then,
define $\U_{\N}$ by 
\[
\U_{\N}\h\left(x\right)=e^{x^{2}/2}\int_{-\infty}^{x}\left(\h\left(t\right)-\E_{\N}\h\right)e^{-t^{2}/2}\d t.
\]
By the product rule and the Fundamental Theorem of Calculus, for all
$\h\in\cX_{\N}$ we have
\begin{align*}
\T_{\N}\U_{\N}\h\left(x\right) & =\h\left(x\right)-\E_{\N}\h.
\end{align*}
Hence, \ref{eq:U_0-characterizing} holds and \ref{prop:U_0-characterizing}
completes the proof.
\end{proof}

\begin{thm}
\label{thm:poisson-characterizing}Define $\T_{\Po\left(\lambda\right)}$
by $\T_{\Po\left(\lambda\right)}\fo\left(k\right)=\lambda\fo\left(k+1\right)-k\fo\left(k\right)$.
Let $\cX_{\Po\left(\lambda\right)}$ be the set of integer-valued
functions $\h:\NN\to\ZZ$ that satisfy $\E_{\Po\left(\lambda\right)}\left|\h\right|<\infty$
and let $\cF_{\Po\left(\lambda\right)}=\T_{\Po\left(\lambda\right)}^{-1}\cX_{\Po\left(\lambda\right)}$
be the set of functions $\fo:\NN\to\ZZ$ such that $\E_{\Po\left(\lambda\right)}\left|\T_{\Po\left(\lambda\right)}\fo\right|<\infty$.
Then $\T_{\Po\left(\lambda\right)}:\cF_{\Po\left(\lambda\right)}\to\cX_{\Po\left(\lambda\right)}$
is a characterizing operator for $\Po\left(\lambda\right)$.\end{thm}
\begin{proof}
For any $f\in\cF_{\Po\left(\lambda\right)}$, we have
\[
\E_{\Po\left(\lambda\right)}\T_{\Po\left(\lambda\right)}\fo=e^{-\lambda}\sum_{\i=0}^{\infty}\frac{\lambda^{\i+1}}{\i!}\fo\left(\i+1\right)-e^{-\lambda}\sum_{\i=1}^{\infty}\frac{\lambda^{\i}}{\left(\i-1\right)!}\fo\left(\i\right)=0
\]
so $\E_{\Po\left(\lambda\right)}\T_{\Po\left(\lambda\right)}=0$ and
\ref{eq:E_0T_0-consistency} holds. Then, define $\U_{\Po\left(\lambda\right)}$
by 
\[
\U_{\Po\left(\lambda\right)}\h\left(k\right)=\frac{\left(k-1\right)!}{\lambda^{k}}\sum_{\i=0}^{k-1}\frac{\lambda^{\i}}{\i!}\left(\h\left(\i\right)-\E_{\Po\left(\lambda\right)}\h\right)
\]
for $k\ge1$. Substituting and simplifying gives
\[
\T_{\Po\left(\lambda\right)}\U_{\Po\left(\lambda\right)}\h\left(k\right)=\h\left(k\right)-\E_{\Po\left(\lambda\right)}\h,
\]
so \ref{eq:U_0-characterizing} holds and \ref{prop:U_0-characterizing}
completes the proof.
\end{proof}
Note that $\cH_{\TV}\subseteq\cX_{\N}$, where $\cH_{\TV}$ is as
defined in \ref{def:special-metrics}. Since $\cH_{\TV}$ is a determining
class, $\T_{\N}$ is a characterizing operator in the sense of \ref{prop:characterizing}.
We can say the same about $\T_{\Po\left(\lambda\right)}$ if we restrict
our attention to integer-valued random variables.

\begin{todo} Stein chose $\Xo=\left\{ \h:\E\left[\id_{\RR}^{k}\left|\h\right|\right]<\infty\mbox{ for all }k\right\} $,
for both the Poisson and normal case. I'm not sure why, I'll revisit
this after looking at exchangeable pairs.\end{todo}

The utility of the introduction of a characterizing operator is that
for each $\h\in\Xo$, Equation \ref{eq:U_0-characterizing} allows
us to to make the transformation
\begin{equation}
\E_{\X}\h=\Eo\h+\E_{\X}\To\Uo\h.\label{eq:stein-transformation}
\end{equation}
The original purpose of Stein's method was to estimate some particular
$\E_{\X}\h$. If $\Lo$ was chosen to be a ``simple'', well-understood
distribution then the term $\Eo\h$ should be easy to compute or estimate,
and if the distribution of $\X$ was ``close'' to $\Lo$, then it
should be possible to show that the remainder $\E_{\X}\To\Uo\h$ is
small.

For our purposes, the main use of \ref{eq:stein-transformation} is
to bound $d_{\cH}\left(\X,\Lo\right)$ for some $\cH\subseteq\Xo$.
For any $\cY\supseteq\Uo\cH$, we have 
\[
d_{\cH}\left(\X,\Lo\right)=\sup_{\h\in\cH}\left|\E_{\X}\To\Uo\h\right|\le\sup_{\fo\in\cY}\left|\E_{\X}\To\fo\right|.
\]
We have reduced the problem of bounding $d_{\cH}\left(\X,\Lo\right)$
to that of bounding $\left|\E_{\X}\To\fo\right|$ (uniformly over
$\fo\in\cY$). Especially in the cases where $\Lo$ is normal or Poisson
and $\cH$ is one of the standard choices in \ref{def:special-metrics},
there are a number of known convenient choices of $\cY$, and a number
of methods that are known to be effective to bound $\left|\E_{\X}\To\fo\right|$.
\begin{example}
\label{example:poisson-bound}If $\cH=\cH_{\TV}$ and $\Lo=\Po\left(\lambda\right)$,
using the characterizing operator in \ref{thm:poisson-characterizing},
then we generally choose 
\[
\cY=\left\{ \fo\in\Fo:\left\Vert \fo\right\Vert _{\infty}\le\min\left\{ 1,\lambda^{-1/2}\right\} ,\,\left\Vert \Delta\fo\right\Vert _{\infty}\le\min\left\{ 1,\lambda^{-1}\right\} \right\} ,
\]
where $\Delta\fo\left(k\right)=\fo\left(k+1\right)-\fo\left(k\right)$.
\end{example}
Proving that this choice of $\cY$ satisfies $\cY\supseteq\U_{\Po\left(\lambda\right)}\cH$
is nontrivial. But, we can prove that the constraints are of the ``correct''
order of magnitude.

\begin{todo}\label{todo:poisson-norm-bound}

The proof is in \cite[Remark 10.2.4]{BHJ92}. There's also a simpler
proof in \cite[Lemma 1.1.1]{BHJ92} that $\left\Vert \fo\right\Vert _{\infty}\le2\min\left\{ 1,\lambda^{-1/2}\right\} $
suffices. I'll revisit this later.

\end{todo}
\begin{prop}
With $\U_{\Po\left(\lambda\right)}$ as in \ref{thm:poisson-characterizing},
we have
\begin{align*}
\sup_{\A}\left\Vert \U_{\Po\left(\lambda\right)}\one_{\A}\right\Vert _{\infty}\asymp\lambda^{-1/2}\mbox{ as }\lambda\to\infty,\quad & \sup_{\A}\left\Vert \U_{\Po\left(\lambda\right)}\one_{\A}\right\Vert _{\infty}\asymp1\mbox{ as }\lambda\to0,\\
\sup_{\A}\left\Vert \Delta\U_{\Po\left(\lambda\right)}\one_{\A}\right\Vert _{\infty}\asymp\lambda^{-1}\mbox{ as }\lambda\to\infty,\quad & \sup_{\A}\left\Vert \Delta\U_{\Po\left(\lambda\right)}\one_{\A}\right\Vert _{\infty}\asymp1\mbox{ as }\lambda\to0,
\end{align*}
\end{prop}
\begin{proof}
(Adapted from \cite[Lemma 1.1.1]{BHJ92}) For any Borel $\A$ and
any $k\in\NN$,
\begin{align*}
\U_{\Po\left(\lambda\right)}\one_{\A}\left(k\right) & =\frac{\left(k-1\right)!}{\lambda^{k}}\left(\Po_{\lambda}\left(\A\cap\range{k-1}\right)-\Po_{\lambda}\left(\A\right)\Po_{\lambda}\left(\range{k-1}\right)\right)\\
 & =\frac{\left(k-1\right)!}{\lambda^{k}}\left(\vphantom{\int}\Po_{\lambda}\left(\A\cap\range{k-1}\right)\left(\vphantom{\sum}1-\Po_{\lambda}\left(\range{k-1}\right)\right)\right.\\
 & \qquad\qquad\qquad-\left.\vphantom{\int}\left(\vphantom{\sum}\Po_{\lambda}\left(\A\right)-\Po_{\lambda}\left(\A\cap\range{k-1}\right)\right)\Po_{\lambda}\left(\range{k-1}\right)\right)\\
 & =\frac{\left(k-1\right)!}{\lambda^{k}}\left(\vphantom{\sum}\Po_{\lambda}\left(\A\cap\range{k-1}\right)\Po_{\lambda}\left(\RR\backslash\range{k-1}\right)-\Po_{\lambda}\left(\A\backslash\range{k-1}\right)\Po_{\lambda}\left(\range{k-1}\right)\right).
\end{align*}
Note that $\Po_{\lambda}\left(\A\cap\range{k-1}\right)$ is bounded
above by $\Po_{\lambda}\left(\range{k-1}\right)$ and $\Po_{\lambda}\left(\A\backslash\range{k-1}\right)$
is bounded above by $\Po_{\lambda}\left(\RR\backslash\range{k-1}\right)$,
so
\[
\left|\U_{\Po\left(\lambda\right)}\one_{\A}\left(k\right)\right|\le\frac{\left(k-1\right)!}{\lambda^{k}}\Po_{\lambda}\left(\range{k-1}\right)\Po_{\lambda}\left(\RR\backslash\range{k-1}\right).
\]
Note that we have equality when $\A=\range{k-1}$. If $k\asymp\lambda$
as $\lambda\to\infty$ then Stirling's approximation (unfinished...)
\end{proof}
\begin{todo}

I need a way to show $\Po_{\lambda}\left(\range{k-1}\right)\asymp1$
when $k$ is close to $\lambda$. I imagine the typical approach would
be to show that $\left(\Po\left(\lambda\right)-\lambda\right)/\lambda\to\N\left(0,1\right)$
with L\'evy's continuity theorem, but that would require me to introduce
more stuff in the probability revision section. Maybe I can use Stein's
method itself for this! In any case I'll think about this more when
I've decided on my approach to \ref{todo:poisson-norm-bound}.

\end{todo}

\begin{todo}

I should have a toy example here that is amenable to several methods.

\end{todo}


\subsection{The method of exchangeable pairs}

\global\long\def\fX{f}


\global\long\def\hX{\h}


\global\long\def\XX{\cX_{\X}}


\global\long\def\eX{\mathbf{X}}


\global\long\def\FX{\cF_{\X}}


\global\long\def\TX{\T_{\eX}}


\global\long\def\OmX{\Om_{\X}}


\global\long\def\OmeX{\Om_{\eX}^{\left(2\right)}}


\global\long\def\conn{\alpha}


\global\long\def\x#1{x^{\left(#1\right)}}


This is Stein's original approach, and is effective in wide generality
for discrete random variables. In what follows, we assume $\X$ is
discrete. Let $\OmX$ be the support of $\X$.
\begin{example}
[adapted from {\cite[Example 4.21]{Ros11}}]\label{example:fixed-points}We
will use an example problem to illustrate the principles in this section.
We will say a \emph{fixed point} of a permutation $\sigma\in S_{n}$
is an index $k\in\left[n\right]$ that satisfies $\sigma\left(k\right)=k$.
Let $\X:S_{n}\to\left\{ 0\right\} \cup\range n$ give the number of
fixed points in each permutation from $S_{n}$. We interpret $\X$
as a random variable on the underlying space $\rS n$.

Now, if $n$ is large then fixed points are largely independent of
each other, and each of $n$ indices has a probability of $1/n$ to
be a fixed point. So (recalling \ref{rem:indicator-expectation}),
we might expect $\L{\X}$ to be ``close'' to $\Po\left(1\right)$.
We will attempt to bound $d_{\TV}\left(\X,\Po\left(1\right)\right)$
to quantify this intuition.
\end{example}
The general idea is that we can use an object $\eX$ called an exchangeable
pair to construct a characterizing operator $\TX$ for $\X$. We then
use an operator $\conn$ to connect the domains of $\TX$ and $\To$
in such a way that $\TX\conn$ approximates $\To$. We then have $\E_{\X}\To=\E_{\X}\left(\To-\TX\conn\right)$,
so we can use the fact that $\To-\TX\conn$ is small to bound $\E_{\X}\To\fo$.
\begin{defn}
A 2-dimensional random pair $\eX=\left(\X_{1},\X_{2}\right)$ is an
\emph{exchangeable pair} if $\Lb{\X_{1},\X_{2}}=\Lb{\X_{2},\X_{1}}$.
We will denote the support of $\eX$ by $\OmeX$.
\end{defn}
That is, a pair $\eX$ is exchangeable if exchanging the components
of the pair does not change their joint distribution. In particular,
the marginal distribution of $\X_{1}$ and $\X_{2}$ must be the same.

\begin{note}

All presentations of Stein's method I've seen use the notation $\left(\X,\X'\right)$
but I think that has the potential to be confusing because the $\X$
in that pair is defined on $\Om^{2}$ whereas the original random
variable $\X$ is defined on $\Om$.

\end{note}
\begin{prop}
There is a natural equivalence between time-homogeneous reversible
Markov chains with steady-state distribution $\L{\X}$, and exchangeable
pairs with margins $\L{\X}$.\end{prop}
\begin{proof}
Given an exchangeable pair $\eX$ with margins $\L{\X}$, we can define
a time-homogenous Markov chain $M$ with transition probabilities
$p\left(x_{1},x_{2}\right)=\Pr\left(\X_{2}=x_{2}|\X_{1}=x_{1}\right)$.
With $\pi\left(x\right)=\Pr\left(\X=x\right)$, we then have
\[
\pi\left(x_{1}\right)p\left(x_{1},x_{2}\right)=\Pr\left(\eX=\left(x_{1},x_{2}\right)\right)=\pi\left(x_{2}\right)p\left(x_{2},x_{1}\right)
\]
for any $x_{1},x_{2}\in\OmX$. So, $M$ is reversible with steady-state
distribution $\L{\X}$.

Conversely, suppose we have a time-homogeneous reversible Markov chain
with steady-state distribution $\L{\X}$. Let the transition probability
between $x$ and $x'$ be $p\left(x,x'\right)$ and let the probability
of state $x$ in the steady-state distribution be $\pi\left(x\right)$.
We can then define an exchangeable pair $\eX$ by 
\[
\Pr\left(\eX=\left(x_{1},x_{2}\right)\right)=\pi\left(x_{1}\right)p\left(x_{1},x_{2}\right)=\pi\left(x_{2}\right)p\left(x_{2},x_{1}\right)=\Pr\left(\eX=\left(x_{2},x_{1}\right)\right)
\]
and the proposition is proved.\end{proof}
\begin{defn}
We say an exchangeable pair $\eX$ is \emph{connected} if the corresponding
Markov chain is irreducible.\end{defn}
\begin{rem}
\label{rem:underlying-pair}We are particularly interested in exchangeable
pairs $\eX$ with marginal distributions $\L{\X_{1}}=\L{\X_{2}}=\L{\X}$.
If $\X$ is defined on an underlying combinatorial probability space
$\left(\Om,2^{\Om},\Pr\right)$, it is often convenient to first construct
an exchangeable pair $\mathbf{W}=\left(W_{1},W_{2}\right)$ with margins
$\Pr$, so that the vector $\eX_{\mathbf{W}}=\left(\X\left(W_{1}\right),\X\left(W_{2}\right)\right)$
is an exchangeable pair with margins $\L{\X}$. If $\left(W_{1},W_{2}\right)$
is connected, then $\eX_{\mathbf{W}}$ is connected also.\end{rem}
\begin{example}
\label{example:fixed-points-exchangeable-pair}We continue \ref{example:fixed-points}.
We will define a specific exchangeable pair $\mathbf{W}=\left(W_{1},W_{2}\right)$
with margins $\rS n$ by 
\[
\Pr\left(\left(W_{1},W_{2}\right)=\left(\sigma_{1},\sigma_{2}\right)\right)=\begin{cases}
\left(n!{n \choose 2}\right)^{-1} & \mbox{if }\sigma_{1}=\sigma_{2}\left(\i\,\ii\right)\mbox{ for some transposition \ensuremath{\left(\i\,\ii\right)}}\\
0 & \mbox{otherwise}.
\end{cases}
\]
The relation of differing by a transposition is symmetric, so $\mathbf{W}$
is indeed an exchangeable pair. The Markov chain associated with $\mathbf{W}$
has a simple interpretation. Given a random permutation $\sigma$,
to make a transition in the Markov chain we just randomly choose one
of the ${n \choose 2}$ possible transpositions and compose it with
$\sigma$. Because the transpositions generate $S_{n}$, the pair
$\mathbf{W}$ is connected, so we can use the construction from \ref{rem:underlying-pair}
to produce a connected exchangeable pair $\eX$ with margins $\mathbf{W}$.
\end{example}
The Markov chain underlying a connected exchangeable pair can be naturally
viewed as a connected one-dimensional simplicial complex. The zeroth
reduced homology group $\ker\partial_{0}/\im\partial_{1}$ of a connected
simplicial complex has dimension zero, and this motivates the construction
of a characterizing operator in a natural way. (The following theorem
is self-contained and requires no knowledge of homology theory).
\begin{thm}
Suppose\textup{ $\eX$ }is a connected exchangeable pair with margins
$\L{\X}$. Let $\FX\subseteq L^{1}\left(\OmX^{2},\L{\eX}\right)$
be the set of functions $\fX:\OmX^{2}\to\RR$ which satisfy $\E\left|\fX\left(\eX\right)\right|<\infty$,
and are antisymmetric in the sense that $\fX\left(x_{1},x_{2}\right)=-\fX\left(x_{2},x_{1}\right)$.
Let $\XX=L^{1}\left(\OmX,\L{\X}\right)$ be the set of functions $\hX:\OmX\to\RR$
that satisfy $\E_{\X}\left|\hX\right|<\infty$.

Define $\TX:\FX\to\XX$ by $\TX\fX\left(x\right)=\sum_{x_{2}\in\OmX}\fX\left(x,x_{2}\right)p\left(x,x_{2}\right)=\E\left[\fX\left(\eX\right)|\X_{1}=x\right]$,
so that $\TX\X=\E^{\X_{1}}\fX\left(\eX\right)$. Then $\TX$ is a
characterizing operator for $\X$.\end{thm}
\begin{proof}
To see that $\im\TX\subseteq\ker\E_{\X}$, fix $\fX\in\cF$ and note
that by the tower law of expectation (\ref{prop:tower-law}), 
\[
\E_{\X}\TX\fX=\E\E^{\X_{1}}\fX\left(\eX\right)=\E\fX\left(\eX\right).
\]
By exchangeability and antisymmetry, $\E\fX\left(\eX\right)=\E\fX\left(\X_{2},\X_{1}\right)=-\E\fX\left(\eX\right)$,
so $\E\fX\left(\X_{1},\X_{2}\right)=\E_{\X}\TX\fX=0$. This did not
require the connectedness condition. We can similarly prove that $\TX$
is well-defined as an operator from $\FX$ to $\XX$: note that $\E_{\X}\left|\TX\fX\right|=\E\left|\fX\left(\eX\right)\right|$
so $\TX\FX\subseteq\XX$.

We will next prove $\ker\E_{\X}\subseteq\im\TX$, but first we make
some definitions. For each $x\in\OmX$, let $\hX_{x}$ be the function
that takes the value $\pi\left(x\right)^{-1}$ on $x$ and is zero
elsewhere, so that $\hX=\sum_{x\in\OmX}\hX\left(x\right)\pi\left(x\right)\hX_{x}$
for each $\hX\in\XX$. For each $\left(x_{1},x_{2}\right)\in\OmeX$,
define $\fX_{x_{1},x_{2}}\in\FX$ as the function that takes the value
${\displaystyle \left(\pi\left(x_{1}\right)p\left(x_{1},x_{2}\right)\right)^{-1}}$
on $\left(x_{1},x_{2}\right)$, takes the value $-\left(\pi\left(x_{2}\right)p\left(x_{2},x_{1}\right)\right)^{-1}$
on $\left(x_{2},x_{1}\right)$, and takes the value zero elsewhere.
Note that this function is antisymmetric by the reversibility of the
Markov chain of $\eX$. We have $\TX\fX_{x_{1},x_{2}}=\hX_{x_{2}}-\hX_{x_{1}}$.

Let $\hX\in\ker\E_{\X}$, and fix an arbitrary $x^{*}\in\OmX$. By
the connectedness assumption, for each $x\in\OmX$ there is a sequence
\[
x=\x 0,\x 1,\dots,\x{k-1},\x k=x^{*}
\]
 with $\left(\x{\i-1},\x{\i}\right)\in\OmeX$ for $\i=1,\dots k$.
Note that 
\[
\hX_{x^{*}}-\hX_{x}=\TX\sum_{\i=1}^{k}\fX_{\x{\i-1},\x{\i}}=:\TX\fX_{x}^{*},
\]
and it follows that
\[
\hX=\sum_{x\in\OmX}\hX\left(x\right)\pi\left(x\right)\left(\hX_{x^{*}}-\TX\fX_{x}^{*}\right)=\left(\E_{\X}\hX\right)\hX_{x^{*}}-\sum_{x\in\OmX}\TX\hX\left(x\right)\pi\left(x\right)\fX_{x}^{*}.
\]
By assumption $\left(\E_{\X}\hX\right)=0$. If $\OmX$ is finite,
as it will be in our applications, then it would immediately follow
that $\hX\in\im\TX$. Otherwise we will need some functional analysis.
Note that $\TX$ is actually an isometry between a subspace $\FX$
of $L^{1}\left(\OmX^{2},\L{\eX}\right)$ and $L^{1}\left(\OmX,\L{\X}\right)$
. First we prove that $\FX$ is closed and therefore a Banach space.
For any $\fX\in L^{1}\left(\OmX^{2},\L{\eX}\right)$, let $\bar{\fX}$
be defined by $\left(x_{1},x_{2}\right)\mapsto-\fX\left(x_{2},x_{1}\right)$,
so that $\fX\in\FX$ implies that $\fX=\bar{\fX}$. Suppose $\fX_{n}\to\fX$,
with $\fX_{n}\in\FX$ for all $n\in\NN$. By exchangeability we have
\[
\left\Vert \fX_{n}-\fX\right\Vert =\E\left|\fX_{n}\left(\eX\right)-\fX\left(\eX\right)\right|=\E\left|\fX_{n}\left(\X_{2},\X_{1}\right)-\bar{\fX}\left(\X_{2},\X_{1}\right)\right|=\left\Vert \fX_{n}-\bar{\fX}\right\Vert 
\]
so $\fX=\bar{\fX}$ and $\fX\in\FX$. Finally, it is a simple fact
that an isometry between Banach spaces has a closed range. For, if
$\left(\TX\fX_{n}\right)_{n\in\NN}$ is a Cauchy sequence in $\im\TX$,
then $\left(\fX_{n}\right)_{n\in\NN}$ is a Cauchy sequence in $\FX$
which converges to some $\fX\in\FX$. It follows that $\TX\fX_{n}\to\TX\fX\in\im\TX$.

We have proved that $\hX\in\im\TX$, completing the proof that $\ker\E_{\X}\subseteq\im\TX$.
\end{proof}
The final step is to choose an operator $\conn:\Fo\to\FX$ in such
a way that $\TX$ can be easily compared with $\To\conn$.
\begin{example}
For the Poisson case in \ref{thm:poisson-characterizing}, we need
to compare $\lambda\fo\left(\X+1\right)$ with $\X\fo\left(\X\right)$.
It is often fruitful to define $\conn$ by 
\[
\conn\fo\left(x_{1},x_{2}\right)=c\fo\left(x_{2}\right)\one\left\{ x_{2}=x_{1}+1\right\} -c\fo\left(x_{1}\right)\one\left\{ x_{1}=x_{2}+1\right\} 
\]
for some $c\in\RR$. We will then have
\[
\To\fo-\TX\conn\fX=\fo\left(\X+1\right)\left(\lambda-c\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\right)-\fo\left(\X_{1}\right)\left(\X_{1}-c\Pr\left(\X_{1}=\X_{2}+1|\X_{1}\right)\right).
\]
Using the triangle inequality and the choice of $\cY$ in \ref{example:poisson-bound},
for all $\fo\in\cY$: 
\[
\E_{\X}\To\fo\le\min\left(1,\lambda^{-1/2}\right)\left(\E\left|\lambda-c\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)\right|+\E\left|X_{1}-c\Pr\left(\X_{1}=\X_{2}+1|\X_{1}\right)\right|\right).
\]
This approximation is effective when 
\begin{align}
\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right) & \approx\lambda/c,\label{eq:immigration-death}\\
\Pr\left(\X_{1}=\X_{2}+1|\X_{1}\right) & \approx\X_{1}/c.\nonumber 
\end{align}
The interpretation of these approximate equalities is that the Markov
chain associated with $\eX$ is approximately an immigration-death
process. This is likely to happen when $\X\left(\om\right)$ is in
some sense a statistic of the amount of local structure over the object
$\om$, and $\eX$ is defined by a Markov chain on $\Om$ (as in \ref{rem:underlying-pair})
that (uniformly) randomly disturbs local structure. The conclusion
to \ref{example:fixed-points} should make this clear:
\end{example}

\begin{example}
We continue \ref{example:fixed-points}, recalling the exchangeable
pairs $\mathbf{W}$ and $\eX$ from \ref{example:fixed-points-exchangeable-pair}.
The interpretation of 
\[
\Pr\left(\X_{1}=\X_{2}+1|\X_{1}\right)=\Pr\left(\X_{2}=\X_{1}-1|\X_{1}\right)
\]
 is the probability of a transposition destroying exactly one out
of an existing $\X_{1}$ fixed points. In order to destroy exactly
one fixed point, we have to choose a fixed point to destroy, and swap
it with a non-fixed-point. There are $\X_{1}\left(n-\X_{1}\right)$
out of ${n \choose 2}$ transpositions that do this, so
\[
\Pr\left(\X_{1}=\X_{2}+1|\X_{1}\right)=\frac{\X_{1}\left(n-\X_{1}\right)}{{n \choose 2}}.
\]
Next, we will find a formula for $\Pr\left(\X\left(W_{2}\right)=\X\left(W_{1}\right)+1|W_{1}\right)$,
noting that 
\[
\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)=\E\left[\Pr\left(\X\left(W_{2}\right)=\X\left(W_{1}\right)+1|W_{1}\right)|\X_{1}\right].
\]
In order to create exactly one fixed point, we have to choose an index
$k$ that is not fixed in $W_{1}$ (there are $n-\X_{1}$ such) and
compose $W_{1}$ with $\left(k\,\sigma\left(k\right)\right)$. This
creates exactly one fixed point unless $\sigma^{-1}\left(k\right)=k$,
in which case it creates two. We have counted this second case twice
for every transposition in the cycle decomposition of $W_{1}$. Let
$Y$ be the number of transpositions in the cycle decomposition of
$W_{1}$. We have 
\[
\Pr\left(\X_{2}=\X_{1}+1|\X_{1}\right)=\frac{n-\X_{1}-2\E\left[Y|\X_{1}\right]}{{n \choose 2}}.
\]
In order to satisfy \ref{eq:immigration-death} as closely as possible,
we choose $c={n \choose 2}/n$. Recalling that $\E\X_{1}=1$, we then
have
\begin{align*}
\E\left|1-c\Pr\left(X_{2}=X_{1}+1|X_{1}\right)\right| & =\E\left[1-\frac{n-\X_{1}-2\E\left[Y|\X_{1}\right]}{n}\right]\\
 & =1/n+2\E Y/n,\\
\E\left|X_{1}-c\Pr\left(X_{2}=X_{1}-1|X_{1}\right)\right| & =\E\left[\X_{1}-\frac{\X_{1}\left(n-\X_{1}\right)}{n}\right]\\
 & =\E\left[\X_{1}^{2}\right]/n.
\end{align*}
Now, the probability that a transposition $\left(\i\,\ii\right)$
is in the cycle decomposition of $W_{1}$ is $\left(n\left(n-1\right)\right)^{-1}$
because $\i$ must map to $\ii$ out of the $n$ possible options
in $\range n$, then $\ii$ must map to $\i$ out of the $n-1$ possible
options in $\range n\backslash\left\{ \ii\right\} $. There are ${n \choose 2}=n\left(n-1\right)/2$
possible transpositions so by \ref{rem:indicator-expectation} it
follows that $\E Y=1/2$.

Now, $\E\left[\X_{1}\left(\X_{1}-1\right)/2\right]$ is the expected
number of unordered pairs of distinct fixed points in a permutation
$\sigma\in\rS n$. For any unordered pair of distinct indices $\left\{ \i,\ii\right\} $,
the probability that both are fixed is $\left(n\left(n-1\right)\right)^{-1}$
because $\i$ must map to $\i$ out of the $n$ possible options in
$\range n$, then $\ii$ must map to $\ii$ out of the $n-1$ possible
options in $\range n\backslash\left\{ \i\right\} $. The total number
of unordered pairs is ${n \choose 2}=n\left(n-1\right)/2$, so again
applying \ref{rem:indicator-expectation}, we have $\E\left[\X_{1}\left(\X_{1}-1\right)/2\right]=1/2$
and $\E\left[\X_{1}^{2}\right]=2$.

We conclude that $d_{\TV}\left(\X,\Po\left(1\right)\right)\le4/n$.
\end{example}
\begin{note}

The ``generator method'' \cite{BC05} says that the Poisson characterizing
operator can be obtained with the generator of an immigration-death
process and the Normal characterizing operator can be obtained with
the generator of an O-U process. Investigate the link here?

\end{note}


\subsection{Size-Bias Coupling}


\part{Applications}

\begin{note}

I'd like to go into a number of small examples (perhaps interspersed
in the discussion of Stein's method in Part I), but I'd like to also
go through a number of ``big'' examples. I'd like these examples
to showcase
\begin{itemize}
\item different types of results: most applications give quantitative estimates.
\cite{Joh11} gives a non-quantitative distributional convergence
result that was not previously proved using other methods. There are
also results that have no connection with distribution metrics, such
as the concentration inequalities in \cite{Ros11}. In particular,
the Latin rectangle example in \cite{Ste86} is interesting in that
the final result is not probabilistic.
\item different types of distributions: definitely at least the Poisson
and normal case, perhaps also an example of a more exotic distribution
like the one in \cite{FS12} or perturbations of Poisson/normal distributions
as in \cite{BCX07}.
\item different ways to apply stein's method: definitely exchangeable pairs
and probably size-biasing. Maybe also Zero-bias coupling.
\end{itemize}
\end{note}

\bibliographystyle{amsalpha}
\bibliography{readings}

\end{document}
